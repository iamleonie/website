---
title: "A brief history of text embedding models"
description: "test test test"
toc: true
---


## Count-based methods
counting how many times a word appeared relative to other words and generating econding based on that

### 1950s-1960s: One-Hot Encoding
One-hot encoding represents each word as a sparse vector with a 1 in the position corresponding to the word in the vocabulary and 0s elsewhere. This was one of the earliest formal ways to represent text for computation.
$O(n)$ computing performnce in the worst case complexity

### 1960s-1970s: [Bag-of-Words (BoW)](bag_of_words.ipynb)
Building on one-hot encoding, BoW represents documents as unordered collections of words, disregarding grammar and word order but keeping multiplicity. This approach treats text as a "bag" of its constituent words.


### 1972: [Term Frequency-Inverse Document Frequency (TF-IDF)](tf-idf.ipynb)
Introduced by Karen Sp√§rck Jones, TF-IDF improved on basic word counts by weighting terms based on their frequency in a document relative to their frequency across all documents, capturing importance and relevance.
    - For example, one of the most-used search functions, BM25, uses TF-IDF as a baseline
    - TF-IDF will tell you how important a single word is in a corpus by assign- ing it a weight and, at the same time, down-weight common words like, "a", "and", and "the". This calculated weight gives us a feature for a single word TF-IDF, and also the relevance of the features across the vocabulary.

### Dimensionality Reduction Era 
Machine learning approaches, shallow models using only one layer of weight and biases
    - **1988: Latent Semantic Analysis (LSA)**
    Also known as Latent Semantic Indexing (LSI), LSA applied singular value decomposition to the term-document matrix to reduce dimensionality while preserving semantic relationships. This introduced the concept of finding "latent" or hidden topics in text.
    - **2003: Latent Dirichlet Allocation (LDA)**
    Developed by David Blei, Andrew Ng, and Michael Jordan, LDA is a generative probabilistic model that represents documents as mixtures of topics, with topics being probability distributions over words. This provided a more principled approach to topic modeling than LSA.

## Prediction-based approaches
came later and instead learned the properties of a given text through models, fixed feature vocabulary

<!--
- Machine Learning Approaches
    - **1990s-2000s: Support Vector Machines (SVM) with Kernel Methods** -> not sure if this is an actual embedding method
    While SVMs themselves are classifiers rather than embedding models, kernel methods (particularly string kernels) allowed SVMs to work effectively with text by mapping text to higher-dimensional spaces where relationships could be captured.
    Developed at Bell Laboratories in the mid 1990s were used in high-dimensional spaces for NLP tasks like text categorization
-->

### 2013: [Word2Vec](word2vec.ipynb) 
https://arxiv.org/abs/1301.3781
Introduced by Mikolov et al. at Google, Word2Vec revolutionized word embeddings by training shallow neural networks to predict either a word from its context **continuous bag of words (CBOW)** or context from a word (**skip-gram**). This created dense vector representations that preserved semantic relationships.

Doc2Vec (2014) first to produce sentence/document embeddings, extending Word2Vec to paragraphs. but it had poor quality.

### 2014: [GloVe (Global Vectors)](glove.ipynb)
Developed by Pennington, Socher, and Manning at Stanford, GloVe combined global matrix factorization with local context window methods, capturing both global statistical information and local contextual information.

### 2015: [FastText](fasttext.ipynb)
Released by Facebook AI Research, FastText extended Word2Vec by representing words as bags of character n-grams, allowing the model to handle out-of-vocabulary words and morphologically rich languages better.

## Contextual Embedding

### 2018: ELMo (Embeddings from Language Models)
Introduced by Peters et al., ELMo generated contextual word representations using bidirectional LSTMs trained on language modeling tasks. Unlike previous models, ELMo produced different embeddings for the same word depending on its context.

### 2018: ULMFiT (Universal Language Model Fine-tuning)
Developed by Howard and Ruder, ULMFiT introduced effective transfer learning techniques for NLP tasks, including language model pretraining and fine-tuning.

### 2017: [Transformer](transformer.ipynb) 
was introduced in the in-famous "Attention Is All You Need" paper 

### Late 2018: BERT (Bidirectional Encoder Representations from Transformers)
Released by Google, BERT transformed NLP by using the transformer architecture with a masked language modeling objective to create deeply contextual representations. BERT was pretrained on massive text corpora and could be fine-tuned for various downstream tasks.

## Modern Large Language Models

### 2019: Sentence-BERT
Paper: https://arxiv.org/abs/1908.10084

- The Problem: 
    - BERT couldn't create good sentence embeddings. 
    - To compare two sentences, you had to feed them together through BERT.
    - computationally expensive for large-scale similarity tasks.
- Sentence-BERT Solution:
    - Siamese network architecture
    - Contrastive learning on sentence pairs
    - Generated fixed-size sentence vectors
    1000x faster similarity computation

Key innovations:
- Pooling strategies (mean, max, CLS token)
- Training on NLI datasets (SNLI, MultiNLI)
- Fine-tuning for specific tasks

Major models:
- `all-MiniLM-L6-v2` (efficiency focus)
- `all-mpnet-base-v2` (quality focus)
- Multilingual variants

Impact: Made semantic search and clustering practical at scale. Enabled the RAG systems we see today.
Limitations: Still task-specific, needed fine-tuning for different domains.

`sentence-transformers` is the Python library implementing it maintained by Hugging Face. (Documentation [SBERT.net Documentation](https://sbert.net/))

Sentence-BERT (2019) was the first to make BERT produce quality sentence embeddings that could be pre-computed and used for similarity search.

### 2020/2021: MPNet/MiniLM
all-MiniLM-L6-v2: Released 2021 - > MiniLM: Microsoft's distilled, smaller BERT variant
all-mpnet-base-v2: Released 2021 -> MPNet: Microsoft's improved BERT architecture
they built on earlier work - MiniLM (Microsoft, 2020) and MPNet (Microsoft, 2020) - but adapted them specifically for sentence embeddings.

all-MiniLM-L6-v2 and all-mpnet-base-v2 are both trained using the Sentence-BERT methodology but with different base models:

Both use SBERT's siamese training approach on sentence pairs to create quality sentence embeddings. They're part of the sentence-transformers library ecosystem that implements SBERT.

### 2023-2024: Embedding-Specific Models**
Models like OpenAI's text-embedding-ada-002 and text-embedding-3-large are specifically designed to create high-quality embeddings for retrieval, similarity search, and other tasks, separate from generative capabilities.

Early 2023

- OpenAI text-embedding-ada-002 (January 2023): Became the industry standard embedding model
- E5 (Early 2023): Microsoft released the E5 family, optimized for retrieval tasks
- BGE (Mid-2023): BAAI released BGE embeddings, showing strong performance especially in multilingual contexts

Mid-2023

- Instructor Embeddings: Specialized models supporting instruction-guided embeddings
- Sentence-Transformers: Continued improvements to open-source embedding ecosystem
- Jina Embeddings v1: Focused on multilingual capabilities

Late 2023

- GTE (General Text Embeddings): Released by Alibaba DAMO Academy
- Cohere Embed v2: Enterprise-focused embeddings with strong multilingual capabilities
- Voyage AI models: Specialized embedding models gaining attention in benchmarks

2024

- OpenAI text-embedding-3 (January): Major upgrade with variable dimensions and improved retrieval performance
- BGE-M3 (February): "Multi-functionality, Multi-linguality, Multi-granularity" embeddings with hybrid retrieval support
- Snowflake Arctic (April): High-performance open-source embedding models optimized for size and performance
- Jina v3 (September): Advanced multilingual models with task-specific adapters
- Voyage-3 models (September): Leading commercial embedding models across benchmarks
- Cohere v3/v4: Continued evolution of commercial embedding capabilities


| Company | License | Model Names |
|---------|----------|-------------|
| OpenAI | Proprietary | `text-embedding-ada-002`, `text-embedding-3` |
| Cohere | Proprietary | `embed-english-v2`, `embed-english-v3`, `embed-multilingual-v2.0`, `embed-multilingual-v3.0` |
| Jina AI | Proprietary | `jina-embeddings-v2`, `jina-embeddings-v3`, `colbert` |
| AWS | Proprietary | `titan-embed-text-v1`, `titan-embed-text-v2` |
| Google | Proprietary | `textembedding-gecko`, `text-embedding-001`, `embedding-001` (Gemini) |
| Mistral | Proprietary | `mistral-embed` |
| NVIDIA | Proprietary | `nv-embed-v1` |
| Snowflake | Apache 2.0 | `arctic` |
| Voyage AI | Proprietary | `voyage-01`, `voyage-02`, `voyage-lite-01`, `voyage-lite-02` |
| Sentence Transformers | Various Open Source | `all-mpnet-base-v2`, `all-MiniLM-L6-v2` |
| Microsoft | | `e5-large-v2` |


### Late interaction 
ColBERT


### Matryoshka
Matryoshka Representation Learning (2022) - Kusupati et al.

Flexible embedding dimensions