{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a429510",
   "metadata": {
    "id": "9a429510",
    "lines_to_next_cell": 2,
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"Transformer\"\n",
    "description: \"Implementing a Transformer Architecture from scratch in PyTorch\"\n",
    "date: \"2025-02-09\"\n",
    "#date-modified: \"2025-02-22\"\n",
    "#categories: [news]\n",
    "bread-crumbs: true\n",
    "back-to-top-navigation: true\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "#image: images/pizza-13601_256.gif\n",
    "---\n",
    "\n",
    "The Transformer is a neural network architecture for natural language processing (NLP) tasks and was introduced in the [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. in 2017. \n",
    "\n",
    "\n",
    "The core idea is that for any given position in a sequence, the transformer asks \"What other positions in this sequence should I pay attention to?\"\n",
    "\n",
    "\n",
    "This article explains the core concepts and Python implementation in PyTorch of the Transformer. \n",
    "\n",
    "Note that PyTorch has the [class `nn.Transformer` and its components (`nn.TransformerEncoder`, `nn.TransformerDecoder`, `nn.TransformerEncoderLayer`, and `nn.TransformerDecoderLayer`)](https://github.com/pytorch/pytorch/blob/v2.7.0/torch/nn/modules/transformer.py) already built-in. The goal of this article, however, is to implement the Transformer from scratch to gain a better understanding of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e535dd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#| echo: true\n",
    "#| output: false\n",
    "!pip3.7 install -q torchdata torchtext spacy altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1bf3deb7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:17.560273Z",
     "iopub.status.busy": "2022-05-02T01:25:17.559273Z",
     "iopub.status.idle": "2022-05-02T01:25:18.690005Z",
     "shell.execute_reply": "2022-05-02T01:25:18.690769Z"
    },
    "id": "1bf3deb7",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96103f9",
   "metadata": {
    "id": "c96103f9"
   },
   "source": [
    "## Transformer Architecture Overview\n",
    "\n",
    "Transformers have an encoder-decoder architecture.\n",
    "That means they have two main components:\n",
    "\n",
    "- **Encoder**: Processes the input sequence $(x_1, ..., x_n)$ and creates rich representations in the form of vector embeddings $\\mathbf{z} = (z_1, ...,z_n)$ (\"I'll read the English and create a rich understanding\")\n",
    "- **Decoder**: Generates the output sequence $(y_1,...,y_m)$ one token at a time, attending to both its own partial output and the encoder's representations (\"I'll use that understanding to generate French\")\n",
    "\n",
    "That's why they are commonly used for language translation models.\n",
    "Think of it like a human translator who first reads and comprehends the entire English sentence, then writes the French translation using that understanding.\n",
    "\n",
    "![Transformer architecture overview diagram showing encoder-decoder structure.](images/transformer_encoders_decoders.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5d4af0",
   "metadata": {
    "id": "bd5d4af0"
   },
   "source": [
    "## Core components\n",
    "\n",
    "Now that we’ve covered the entire forward-pass process through a trained Transformer, it would be useful to glance at the intuition of training the model.\n",
    "\n",
    "\n",
    "\n",
    "At a more detailed level, the Transformer has the following architecture composed of the following core components:\n",
    "\n",
    "- Input Embeddings\n",
    "- Positional Encodings: to maintain sequence order without recurrence\n",
    "- Multi-Head Attention and Masked Multi-head attention\n",
    "- Feed Forward layer\n",
    "- Add & Norm Layer\n",
    "- Residual connections and layer normalization: to stabilize training\n",
    "- Projection Layer (Softmax and Liniear)\n",
    "\n",
    "![**Figure:** Detailed Transformer architecture with encoder and decoder layers, self-attention, and feed-forward networks](images/architecture_detailed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af74513f",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "Embeddings convert discrete tokens (words, subwords, characters) into dense vector representations that the transformer can work with.\n",
    "\n",
    "The transformer uses learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\text{model}}$.  \n",
    "\n",
    "The Problem: Transformers need numerical input, but text consists of discrete symbols. For example, a vocabulary of 50,000 words would create massive, sparse one-hot vectors that are computationally inefficient and carry no semantic meaning.\n",
    "\n",
    "The Solution: Map each token to a dense vector of fixed dimension $d_{\\text{model}}$ (`d_model`)  (typically 256, 512, or 768 dimensions) that can encode semantic relationships.\n",
    "These vectors are learned during training to capture semantic similarity\n",
    "\n",
    "The original paper scales embeddings by $\\sqrt(d_{model})$ to:\n",
    "\n",
    "- ensure that embedding values and positional encoding values are roughly the same magnitude\n",
    "- when they're added together, neither dominates the other\n",
    "- helps with training stability\n",
    "\n",
    "The embedding matrix is `d_model * vocab_size`\n",
    "\n",
    "- the embedding only happens in the bottom-most encoder.\n",
    "- each encoder receives a list of vector each of the size (d_model 512)\n",
    "    - in the bttom encoder that would be the word embeddings\n",
    "    - in the other encoders, that's the output of the encoder that's directly below (the size of this list is a hyperparaeter we can set, the length of the longest sentece in the training dataset)\n",
    "\n",
    "\n",
    "- word in each position flows throgh its own path in the encoder\n",
    "    - there are dependencies between these paths in the self attention layer\n",
    "    - the feed-forard layer does not have those dependencies - thus the paths can be executed in parallel \n",
    "TODO:\n",
    "- [ ] difference between input and output embeddings\n",
    "- [ ] \n",
    "\n",
    "![](images/embeddings.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "27dbc10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of vocabulary\n",
    "            d_model: dimension of embeddings\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: input vector\n",
    "        Returns:\n",
    "            out: scaled embedding vector\n",
    "        \"\"\"\n",
    "        # Scale by sqrt(d_model) from original paper\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91272c4",
   "metadata": {},
   "source": [
    "Suppoese each embedding vector is of 512 dimension and suppose our vocab size is 100, then our embedding matrix will be of size 100x512. \n",
    "This embedding matrix will be learned during training. \n",
    "During inference each word will be mapped to the corresponding 512 dimensional vector. \n",
    "Suppose we have batch size of 32 and sequence length of 10 words, The output will be 32x10x512.\n",
    "\n",
    "\n",
    "The following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba67f744",
   "metadata": {},
   "source": [
    "### Positional Encodings\n",
    "\n",
    "Positional encodings are vectors that tell the transformer where each token sits in a sequence.\n",
    "The motivation for positional encoding is that unlike RNNs, which processes tokens one by one, the transformer looks at all tokens in a sentence at the same time. \n",
    "This makes the transformer fast but it also means that it can't distinguish between \"The cat sat on the mat\" and \"The mat sat on the cat\" because it just sees the same [bag of words](bag_of_words.ipynb).\n",
    "\n",
    "To overcome this problem, you can add positional information in the form of positional encoding vectors directly to the input embeddings. The positional encoding vectors have the same dimensions $d_{model}$ (`d_model`)  as the input embeddings, so that they can be summed element-wise.\n",
    "This gives each token a combined representation that captures both its meaning and its location in the sequence.\n",
    "\n",
    "![](images/positional_encodings.png)\n",
    "\n",
    "for eg: if we have batch size of 32 and seq length of 10 and let embedding dimension be 512. \n",
    "Then we will have embedding vector of dimension 32 x 10 x 512. \n",
    "Similarly we will have positional encoding vector of dimension 32 x 10 x 512. Then we add both.\n",
    "\n",
    "The original transformer uses the **sinusoidal positional encodings**  based on sine and cosine functions of different frequencies:\n",
    "$$\n",
    "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "$$\n",
    "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
    "$$\n",
    "\n",
    "where $pos$ is the position and $i$ is the dimension index. \n",
    "\n",
    "That is, each dimension of the positional encoding corresponds to a sinusoid.  \n",
    "The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  \n",
    "\n",
    "These formulas create unique patterns for each position by using different frequencies across the embedding dimensions. Position 0 has one pattern, position 1 has a slightly different pattern, and so on. This enables the model to learn to recognize not just absolute positions, but also relative distances between tokens.\n",
    "\n",
    "The inuition is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they are projected into QKV vectors and during dot-product attention.\n",
    "\n",
    "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  \n",
    "For the base model, we use a rate of $P_{drop}=0.1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7cb1edb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "\n",
    "    def __init__(self, d_model,  max_seq_len=5000, dropout=0.1,):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: dimension of embeddings\n",
    "            dropout: dropout rate, the original paper uses 0.1\n",
    "            max_seq_len: maximum sequence length\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "\n",
    "        # Create a div term for the denominator\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sin to even indices (0, 2, 4, ...)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "\n",
    "        # Apply cos to odd indices (1, 3, 5, ...)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add batch dimension\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Register as buffer (saved with model, not trained)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape : [batch_size, seq_len, d_model]\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len].requires_grad_(False) # TODO: what does this \"requires_grad_(False)\" do?\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1b3af",
   "metadata": {},
   "source": [
    "\n",
    "> Below the positional encoding will add in a sine wave based on\n",
    "> position. The frequency and offset of the wave is different for\n",
    "> each dimension.\n",
    "\n",
    "Note, that there are different ways to add positional information to input tokens. Read more on [positional encoding](positional_encoding.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f295a5",
   "metadata": {},
   "source": [
    "### Attention mechanism\n",
    "\n",
    "**Attention** is the mechanism where one set of elements \"pays attention to\" another set of elements to gather relevant information.\n",
    "\n",
    "If you look closely to the Transformer, you can see three different types of attention:\n",
    "- Self-attention\n",
    "- Masked self-attention\n",
    "- Cross-attention\n",
    "\n",
    "- also, what is Multi-head attention?\n",
    "\n",
    "#### Self-attention (is this describing self-attention of the general attention)\n",
    "\n",
    "As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
    "> ”The animal didn't cross the street because it was too tired”\n",
    "\n",
    "What does “it” in this sentence refer to? Is it referring to the street or to the animal? \n",
    "When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\n",
    "\n",
    "![](images/transformer_self-attention_visualization.png)\n",
    "\n",
    "<!--Be sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.\n",
    "\n",
    "- [] Add visual of formula here\n",
    "-->\n",
    "\n",
    "create three vectors from each of the encoder’s input vectors (in this case, the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n",
    "They’re abstractions that are useful for calculating and thinking about attention. \n",
    "\n",
    "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  \n",
    "The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
    "\n",
    "We call our particular attention \"Scaled Dot-Product Attention\".\n",
    "The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
    "\n",
    "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.  The keys and values are also packed together into matrices $K$ and $V$.  \n",
    "Wecompute the matrix of outputs as:\n",
    "\n",
    "$$\n",
    "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "- Query ($Q$): What we're looking for\n",
    "- Key ($K$): What we're looking at\n",
    "- Value ($V$): What we actually use\n",
    "\n",
    "1. We compute similarity scores between queries and keys using dot products\n",
    "2. Scale by $\\sqrt{d_k}$ to prevent gradients from becoming too small/prevents saturation in the softmax when d_model is large\n",
    "3.  Masking lets us ignore padded tokens or implement causal attention\n",
    "3. Apply softmax to get attention weights that sum to 1 (Softmax normalizes the scores so they’re all positive and add up to 1.)\n",
    "4. Use these weights to compute a weighted average of the values\n",
    "\n",
    "**Why √d_model scaling?** The paper \"Attention Is All You Need\" found that without this scaling factor, embeddings can dominate positional encodings. It's not arbitrary - it's mathematically necessary for stable training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dcbf7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention'\n",
    "    Attention with optional masking\n",
    "    mask shape: [batch_size, seq_len, seq_len] or broadcastable\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute scaled attention scores\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    # Apply mask before softmax (set masked positions to large negative value)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    # Softmax over the last dimension\n",
    "    p_attn = scores.softmax(dim=-1)\n",
    "\n",
    "    # Apply droppout after softmax\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9fd79",
   "metadata": {},
   "source": [
    "### Multi-head attention\n",
    "\n",
    "\n",
    "The multi-headed attention improves the performance of the attention layer in two ways:\n",
    "\n",
    "    It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.\n",
    "\n",
    "    It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\n",
    "\n",
    "We concat the matrices then multiply them by an additional weights matrix WO."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a82e35",
   "metadata": {},
   "source": [
    "Self-Attention vs Cross-Attention\n",
    "Where Q, K, V Come From\n",
    "- Self-attention: all from same sequence (Q = K = V = same_input)\n",
    "- Cross-attention: from different sequences (Q = sequence_A, K = V = sequence_B)\n",
    "\n",
    "\n",
    "Key points:\n",
    "- Supports both self-attention and cross-attention\n",
    "- Handles different sequence lengths for encoder/decoder\n",
    "\n",
    "Implementation tips:\n",
    "- Use separate Q,K,V projections\n",
    "- Handle masking through addition (not masked_fill)\n",
    "- Remember to use braodcasting and reshape for multi-head attention\n",
    "\n",
    "\n",
    "Why masking? In the decoder, we need to prevent tokens from seeing future tokens during training.\n",
    "\n",
    "The goal of reducing sequential computation also forms the\n",
    "foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of\n",
    "which use convolutional neural networks as basic building block,\n",
    "computing hidden representations in parallel for all input and\n",
    "output positions. In these models, the number of operations required\n",
    "to relate signals from two arbitrary input or output positions grows\n",
    "in the distance between positions, linearly for ConvS2S and\n",
    "logarithmically for ByteNet. This makes it more difficult to learn\n",
    "dependencies between distant positions. In the Transformer this is\n",
    "reduced to a constant number of operations, albeit at the cost of\n",
    "reduced effective resolution due to averaging attention-weighted\n",
    "positions, an effect we counteract with Multi-Head Attention.\n",
    "\n",
    "Self-attention, sometimes called intra-attention is an attention\n",
    "mechanism relating different positions of a single sequence in order\n",
    "to compute a representation of the sequence. Self-attention has been\n",
    "used successfully in a variety of tasks including reading\n",
    "comprehension, abstractive summarization, textual entailment and\n",
    "learning task-independent sentence representations. End-to-end\n",
    "memory networks are based on a recurrent attention mechanism instead\n",
    "of sequencealigned recurrence and have been shown to perform well on\n",
    "simple-language question answering and language modeling tasks.\n",
    "\n",
    "To the best of our knowledge, however, the Transformer is the first\n",
    "transduction model relying entirely on self-attention to compute\n",
    "representations of its input and output without using sequence\n",
    "aligned RNNs or convolution.\n",
    "\n",
    "\n",
    "The self-attention layer helps the encoder look at other words in the input sentence as it encodes a specific word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83caa59d",
   "metadata": {},
   "source": [
    "The two most commonly used attention functions are additive\n",
    "attention [(cite)](https://arxiv.org/abs/1409.0473), and dot-product\n",
    "(multiplicative) attention.  Dot-product attention is identical to\n",
    "our algorithm, except for the scaling factor of\n",
    "$\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the\n",
    "compatibility function using a feed-forward network with a single\n",
    "hidden layer.  While the two are similar in theoretical complexity,\n",
    "dot-product attention is much faster and more space-efficient in\n",
    "practice, since it can be implemented using highly optimized matrix\n",
    "multiplication code.\n",
    "\n",
    "\n",
    "While for small values of $d_k$ the two mechanisms perform\n",
    "similarly, additive attention outperforms dot product attention\n",
    "without scaling for larger values of $d_k$\n",
    "[(cite)](https://arxiv.org/abs/1703.03906). We suspect that for\n",
    "large values of $d_k$, the dot products grow large in magnitude,\n",
    "pushing the softmax function into regions where it has extremely\n",
    "small gradients (To illustrate why the dot products get large,\n",
    "assume that the components of $q$ and $k$ are independent random\n",
    "variables with mean $0$ and variance $1$.  Then their dot product,\n",
    "$q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance\n",
    "$d_k$.). To counteract this effect, we scale the dot products by\n",
    "$\\frac{1}{\\sqrt{d_k}}$.\n",
    "\n",
    "![Multi-Head Attention mechanism diagram: visualization of how multiple attention heads process input sequences in parallel, each focusing on different representation subspaces, then concatenating their outputs.](images/multihead-attention.png)\n",
    "\n",
    "Multi-head attention allows the model to jointly attend to\n",
    "information from different representation subspaces at different\n",
    "positions. With a single attention head, averaging inhibits this.\n",
    "\n",
    "$$\n",
    "\\mathrm{MultiHead}(Q, K, V) =\n",
    "    \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\\n",
    "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
    "$$\n",
    "\n",
    "Where the projections are parameter matrices $W^Q_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in\n",
    "\\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in\n",
    "\\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.\n",
    "\n",
    "In this work we employ $h=8$ parallel attention layers, or\n",
    "heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due\n",
    "to the reduced dimension of each head, the total computational cost\n",
    "is similar to that of single-head attention with full\n",
    "dimensionality.\n",
    "\n",
    "Interview questions:\n",
    "\n",
    "- *How would you implement multi-head attention from scratch?*\n",
    "- *What is self-attention and how does it work mathematically?*\n",
    "- *Why do transformers use multiple attention heads? What does each head learn?*\n",
    "- *Derive the attention formula: Attention(Q,K,V) = softmax(QK^T/√d_k)V*\n",
    "- *Why do we scale by √d_k in scaled dot-product attention?*\n",
    "- *What is the computational complexity of self-attention?*\n",
    "- *How does causal/masked attention work in decoder models*\n",
    "- *Explain cross-attention vs self-attention*\n",
    "- What are some alternatives to standard attention (sparse attention, linear attention, flash atention)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef3af60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # dimension per head\n",
    "        \n",
    "        # Linear projections for queries, keys, and values\n",
    "        self.W_query = nn.Linear(d_model, d_model)\n",
    "        self.W_key = nn.Linear(d_model, d_model)\n",
    "        self.W_value = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output projection\n",
    "        self.W_output = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        seq_len_query = query.size(1)\n",
    "        seq_len_key = key.size(1)\n",
    "        \n",
    "        # Step 1: Linear projections for all heads at once\n",
    "        # Shape: [batch_size, seq_len, d_model]\n",
    "        Q = self.W_query(query)\n",
    "        K = self.W_key(key)\n",
    "        V = self.W_value(value)\n",
    "        \n",
    "        # Step 2: Reshape to separate heads\n",
    "        # From [batch_size, seq_len, d_model] to [batch_size, seq_len, num_heads, d_k]\n",
    "        Q = Q.view(batch_size, seq_len_query, self.num_heads, self.d_k)\n",
    "        K = K.view(batch_size, seq_len_key, self.num_heads, self.d_k)\n",
    "        V = V.view(batch_size, seq_len_key, self.num_heads, self.d_k)\n",
    "        \n",
    "        # Step 3: Transpose to [batch_size, num_heads, seq_len, d_k] for efficient computation\n",
    "        Q = Q.transpose(1, 2)  # [batch_size, num_heads, seq_len_query, d_k]\n",
    "        K = K.transpose(1, 2)  # [batch_size, num_heads, seq_len_key, d_k]\n",
    "        V = V.transpose(1, 2)  # [batch_size, num_heads, seq_len_key, d_k]\n",
    "        \n",
    "        # Step 4: Apply scaled dot-product attention to each head\n",
    "        attention_output, attention_weights = scaled_dot_product_attention(\n",
    "            Q, K, V, mask\n",
    "        )\n",
    "        # attention_output: [batch_size, num_heads, seq_len_query, d_k]\n",
    "        \n",
    "        # Step 5: Concatenate heads\n",
    "        # Transpose back: [batch_size, seq_len_query, num_heads, d_k]\n",
    "        attention_output = attention_output.transpose(1, 2)\n",
    "        \n",
    "        # Reshape to concatenate heads: [batch_size, seq_len_query, d_model]\n",
    "        attention_output = attention_output.contiguous().view(\n",
    "            batch_size, seq_len_query, self.d_model\n",
    "        )\n",
    "        \n",
    "        # Step 6: Final linear projection\n",
    "        output = self.W_output(attention_output)\n",
    "        \n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "14b189d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 10, 512])\n",
      "Output shape: torch.Size([2, 10, 512])\n",
      "Attention weights shape: torch.Size([2, 8, 10, 10])\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "def example_usage():\n",
    "    batch_size, seq_len, d_model = 2, 10, 512\n",
    "    num_heads = 8\n",
    "    \n",
    "    # Create sample input\n",
    "    x = torch.randn(batch_size, seq_len, d_model)\n",
    "    \n",
    "    # Initialize multi-head attention\n",
    "    mha = MultiHeadAttention(d_model, num_heads)\n",
    "    \n",
    "    # Self-attention (query, key, value are all the same)\n",
    "    output, weights = mha(x, x, x)\n",
    "    \n",
    "    print(f\"Input shape: {x.shape}\")\n",
    "    print(f\"Output shape: {output.shape}\")\n",
    "    print(f\"Attention weights shape: {weights.shape}\")\n",
    "    \n",
    "    # For encoder-decoder attention, you'd use different inputs:\n",
    "    # output, weights = mha(decoder_hidden, encoder_output, encoder_output)\n",
    "\n",
    "example_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca4eea2",
   "metadata": {},
   "source": [
    "Multi-head attention runs multiple attention mechanisms in parallel, each focusing on different aspects of the relationships, then concatenates and projects the results.\n",
    "\n",
    "Applications of Attention in our Model\n",
    "\n",
    "The Transformer uses multi-head attention in three different ways:\n",
    "1) In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.  This allows every position in the decoder to attend over all positions in the input sequence.  This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence\n",
    "models such as [(cite)](https://arxiv.org/abs/1609.08144).\n",
    "\n",
    "The “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n",
    "\n",
    "2) The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.  Each\n",
    "position in the encoder can attend to all positions in the previous layer of the encoder.\n",
    "\n",
    "\n",
    "3) self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections.\n",
    "\n",
    "In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n",
    "\n",
    "The encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V. These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f83c7",
   "metadata": {},
   "source": [
    "### Position-wise Feed-Forward Networks\n",
    "\n",
    "The outputs of the self-attention layer are fed to a feed-forward neural network in the encoder.\n",
    "\n",
    "In addition to attention sub-layers, each of the layers in our\n",
    "encoder and decoder contains a fully connected feed-forward network,\n",
    "which is applied to each position separately and identically.  This\n",
    "consists of two linear transformations with a ReLU activation in\n",
    "between.\n",
    "\n",
    "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
    "\n",
    "While the linear transformations are the same across different\n",
    "positions, they use different parameters from layer to\n",
    "layer. Another way of describing this is as two convolutions with\n",
    "kernel size 1.  The dimensionality of input and output is\n",
    "$d_{\\text{model}}=512$, and the inner-layer has dimensionality\n",
    "$d_{ff}=2048$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5627c0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c238f36",
   "metadata": {},
   "source": [
    "### Layer normalization\n",
    "\n",
    "*What is the purpose of layer normalization in transformers?*\n",
    "\n",
    "We employ a residual connection [(cite)](https://arxiv.org/abs/1512.03385) around each of the two sub-layers, followed by layer normalization [(cite)](https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "When we look at the encoder and decoder blocks, we see several normalization layers called Add & Norm.\n",
    "\n",
    "The `LayerNormalization` class below performs layer normalization on the input data. \n",
    "During its forward pass, we compute the mean and standard deviation of the input data. \n",
    "We then normalize the input data by subtracting the mean and dividing by the standard deviation plus a small number called epsilon to avoid any divisions by zero. \n",
    "This process results in a normalized output with a mean 0 and a standard deviation 1.\n",
    "\n",
    "We will then scale the normalized output by a learnable parameter alpha and add a learnable parameter called bias. The training process is responsible for adjusting these parameters. The final result is a layer-normalized tensor, which ensures that the scale of the inputs to layers in the network is consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d723e2c",
   "metadata": {},
   "source": [
    "Layer normalization helps the transformer learn better and faster. \n",
    "\n",
    "Think of it like this: imagine you're trying to learn math, but every day the teacher uses completely different scales - sometimes numbers from 1-10, sometimes 1-1000, sometimes -500 to +500. It would be really hard to focus on the actual math concepts because you'd be constantly adjusting to these different scales.\n",
    "\n",
    "Layer normalization solves this problem by keeping all the numbers in a consistent, predictable range. For each example, it looks at all the features and normalizes them so they have an average of 0 and spread nicely around that average. This way, each layer in the transformer gets inputs that are always in the same comfortable range.\n",
    "\n",
    "This makes training much more stable and allows the model to learn the important patterns instead of getting distracted by wildly varying number scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "eb4d6162",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fqv3mbxw4h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data (notice the different scales):\n",
      "Sequence 1 (small numbers):\n",
      "tensor([[0.1000, 0.2000, 0.3000, 0.4000],\n",
      "        [0.2000, 0.1000, 0.4000, 0.3000],\n",
      "        [0.3000, 0.4000, 0.1000, 0.2000]])\n",
      "Sequence 2 (large numbers):\n",
      "tensor([[100., 200., 300., 400.],\n",
      "        [150., 250., 350., 450.],\n",
      "        [200., 300., 400., 500.]])\n",
      "\n",
      "After layer normalization:\n",
      "Sequence 1:\n",
      "tensor([[-1.1619, -0.3873,  0.3873,  1.1619],\n",
      "        [-0.3873, -1.1619,  1.1619,  0.3873],\n",
      "        [ 0.3873,  1.1619, -1.1619, -0.3873]], grad_fn=<SelectBackward0>)\n",
      "Sequence 2:\n",
      "tensor([[-1.1619, -0.3873,  0.3873,  1.1619],\n",
      "        [-1.1619, -0.3873,  0.3873,  1.1619],\n",
      "        [-1.1619, -0.3873,  0.3873,  1.1619]], grad_fn=<SelectBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create some example data with different scales and ranges\n",
    "torch.manual_seed(42)  # For reproducible results\n",
    "\n",
    "# Example 1: Data with wildly different scales\n",
    "batch_size, seq_len, d_model = 2, 3, 4\n",
    "x = torch.tensor([\n",
    "    # First sequence: small numbers\n",
    "    [[0.1, 0.2, 0.3, 0.4],\n",
    "     [0.2, 0.1, 0.4, 0.3], \n",
    "     [0.3, 0.4, 0.1, 0.2]],\n",
    "    \n",
    "    # Second sequence: large numbers  \n",
    "    [[100, 200, 300, 400],\n",
    "     [150, 250, 350, 450],\n",
    "     [200, 300, 400, 500]]\n",
    "], dtype=torch.float32)\n",
    "\n",
    "print(\"Original data (notice the different scales):\")\n",
    "print(\"Sequence 1 (small numbers):\")\n",
    "print(x[0])\n",
    "print(\"Sequence 2 (large numbers):\")  \n",
    "print(x[1])\n",
    "print()\n",
    "\n",
    "# Show statistics before normalization\n",
    "#print(\"Statistics BEFORE layer norm:\")\n",
    "#print(f\"Sequence 1 - Mean: {x[0].mean(-1):.2f}, Std: {x[0].std(-1):.2f}\")\n",
    "#print(f\"Sequence 2 - Mean: {x[1].mean(-1):.2f}, Std: {x[1].std(-1):.2f}\")\n",
    "#print()\n",
    "\n",
    "# Apply layer normalization\n",
    "layer_norm = LayerNorm(d_model)\n",
    "x_normalized = layer_norm(x)\n",
    "\n",
    "print(\"After layer normalization:\")\n",
    "print(\"Sequence 1:\")\n",
    "print(x_normalized[0])\n",
    "print(\"Sequence 2:\")\n",
    "print(x_normalized[1])\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9cd2bb",
   "metadata": {},
   "source": [
    "### Residual connection\n",
    "\n",
    "\n",
    "\n",
    "When we look at the architecture of the Transformer, we see that each sub-layer, including the self-attention and Feed Forward blocks, adds its output to its input before passing it to the Add & Norm layer. This approach integrates the output with the original input in the Add & Norm layer. This process is known as the skip connection, which allows the Transformer to train deep networks more effectively by providing a shortcut for the gradient to flow through during backpropagation.\n",
    "\n",
    "The ResidualConnection class below is responsible for this process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a1521c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e4aa4d",
   "metadata": {},
   "source": [
    "### Linear Projection\n",
    "\n",
    "\n",
    "The decoder stack outputs a vector of floats. How do we turn that into a word? That’s the job of the final Linear layer which is followed by a Softmax Layer.\n",
    "\n",
    "The Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\n",
    "\n",
    "Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset. This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer.\n",
    "\n",
    "The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n",
    "\n",
    "![](images/transformer_decoder_output_softmax.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef04378",
   "metadata": {
    "id": "0ef04378"
   },
   "source": [
    "## Transformer Architecture\n",
    "Now, we have all the core components to build the Transformer.\n",
    "\n",
    "- The original Transformer is an encoder-decoder \n",
    "- decoder-only\n",
    "- encoder-only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c1edc4",
   "metadata": {
    "id": "c9c1edc4"
   },
   "source": [
    "### Encoder\n",
    "\n",
    "The encoders are all identical in structure but don't share any weights.\n",
    "\n",
    "Each encoder has two sub-layers:\n",
    "1. Self-attention layer: helps the encoder look at other words in the input sentence as it encode a specific word\n",
    "2. Feed Forward Neural Network: what does this do?\n",
    "\n",
    "...we employ residual connections around each of the sub-layers, followed by layer normalization.\n",
    "\n",
    "![](images/encoder-architecture.png)\n",
    "\n",
    "\n",
    "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x +\n",
    "\\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function\n",
    "implemented by the sub-layer itself.  We apply dropout\n",
    "[(cite)](http://jmlr.org/papers/v15/srivastava14a.html) to the\n",
    "output of each sub-layer, before it is added to the sub-layer input\n",
    "and normalized.\n",
    "\n",
    "To facilitate these residual connections, all sub-layers in the\n",
    "model, as well as the embedding layers, produce outputs of dimension\n",
    "$d_{\\text{model}}=512$.\n",
    "\n",
    "Each layer has two sub-layers. The first is a multi-head\n",
    "self-attention mechanism, and the second is a simple, position-wise\n",
    "fully connected feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3db97336",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.779893Z",
     "iopub.status.busy": "2022-05-02T01:25:18.778804Z",
     "iopub.status.idle": "2022-05-02T01:25:18.780994Z",
     "shell.execute_reply": "2022-05-02T01:25:18.781710Z"
    },
    "id": "3db97336"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sub-layer 1: Multi-head self-attention\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Sub-layer 2: Feed-forward network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Layer normalization for each sub-layer\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Sub-layer 1: Multi-head self-attention with residual connection\n",
    "        # For self-attention: query, key, and value are all the same input\n",
    "        attention_output, attention_weights = self.self_attention(\n",
    "            query=x,    # Same input\n",
    "            key=x,      # Same input  \n",
    "            value=x,    # Same input\n",
    "            mask=mask\n",
    "        )\n",
    "        \n",
    "        # Post-norm: residual connection then normalize\n",
    "        x = self.layer_norm_1(x + self.dropout(attention_output))\n",
    "        \n",
    "        # Sub-layer 2: Feed-forward with residual connection  \n",
    "        feed_forward_output = self.feed_forward(x)\n",
    "\n",
    "        # Post-norm: residual connection then normalize\n",
    "        x = self.layer_norm_2(x + self.dropout(feed_forward_output))\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83075daf",
   "metadata": {},
   "source": [
    "TODO: add something here\n",
    "like that the encoder is a stack of multiple encoder layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1d643018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.744483Z",
     "iopub.status.busy": "2022-05-02T01:25:18.743617Z",
     "iopub.status.idle": "2022-05-02T01:25:18.745891Z",
     "shell.execute_reply": "2022-05-02T01:25:18.746578Z"
    },
    "id": "1d643018"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, source_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input processing\n",
    "        self.embedding = Embeddings(source_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Stack of encoder layers\n",
    "        self.encoder_layers = nn.ModuleList([TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, source_tokens, source_mask=None):\n",
    "        # Step 1: Convert tokens to embeddings + add positional encoding\n",
    "        embeddings = self.embedding(source_tokens)  # [batch, seq_len, d_model]\n",
    "        encoder_input = self.positional_encoding(embeddings)\n",
    "        \n",
    "        # Step 2: Pass the input (and mask) through each encoder layer\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            encoder_input = encoder_layer(encoder_input, source_mask)\n",
    "        \n",
    "        return encoder_input  # [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c90d6ee",
   "metadata": {
    "id": "9c90d6ee"
   },
   "source": [
    "### Decoder\n",
    "\n",
    "The decode has the following layers:\n",
    "- Self-attention layer (masked multi head ) \n",
    "    - Sub-layer 1: Self-attention (same sequence)\n",
    "    - query=key=value=target_input\n",
    "- Cross-attention (multi head Ecoder-Decode attention layer): \n",
    "    - helps the decode focus on relevant parts of the input sentenc, similar to `seq2seq` models.\n",
    "    - In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  \n",
    "    - Sub-layer 2: Cross-attention (different sequences)  \n",
    "    - query=target_input:          What we're generating\n",
    "    - key=value=encoder_output :    What we can look at\n",
    "- Feed forward layer\n",
    "\n",
    "\n",
    "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3b1df6b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.803099Z",
     "iopub.status.busy": "2022-05-02T01:25:18.796132Z",
     "iopub.status.idle": "2022-05-02T01:25:18.806479Z",
     "shell.execute_reply": "2022-05-02T01:25:18.805667Z"
    },
    "id": "3b1df6b1"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Sub-layer 1: Masked multi-head self-attention\n",
    "        self.masked_self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Sub-layer 2: Multi-head encoder-decoder attention\n",
    "        self.encoder_decoder_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        \n",
    "        # Sub-layer 3: Feed-forward network\n",
    "        self.feed_forward = FeedForward(d_model, d_ff)\n",
    "        \n",
    "        # Layer normalization for each sub-layer\n",
    "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
    "        self.layer_norm_3 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, target_input, encoder_output, source_mask=None, target_mask=None):\n",
    "        # Sub-layer 1: Masked self-attention on target sequence\n",
    "        # For self-attention: query, key, and value are all the same input (target)\n",
    "        masked_attention_output, masked_attention_weights = self.masked_self_attention(\n",
    "            query=target_input,     # Same target input\n",
    "            key=target_input,       # Same target input\n",
    "            value=target_input,     # Same target input\n",
    "            mask=target_mask        # Causal mask to prevent seeing future tokens\n",
    "        )\n",
    "        # Post-norm: residual connection then normalize\n",
    "        target_input = self.layer_norm_1(target_input + self.dropout(masked_attention_output))\n",
    "        \n",
    "        # Sub-layer 2: Encoder-decoder attention\n",
    "        # Query comes from decoder, key and value come from encoder\n",
    "        encoder_attention_output, encoder_attention_weights = self.encoder_decoder_attention(\n",
    "            query=target_input,     # What the decoder is generating\n",
    "            key=encoder_output,     # What information is available from encoder\n",
    "            value=encoder_output,   # What information to retrieve from encoder\n",
    "            mask=source_mask        # Mask for padding tokens in source\n",
    "        )\n",
    "        # Post-norm: residual connection then normalize\n",
    "        target_input = self.layer_norm_2(target_input + self.dropout(encoder_attention_output))\n",
    "        \n",
    "        # Sub-layer 3: Feed-forward network\n",
    "        feed_forward_output = self.feed_forward(target_input)\n",
    "        # Post-norm: residual connection then normalize\n",
    "        target_input = self.layer_norm_3(target_input + self.dropout(feed_forward_output))\n",
    "        \n",
    "        return target_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736addc1",
   "metadata": {},
   "source": [
    "TODO: add something here\n",
    "like that the encoder is a stack of multiple encoder layersljkslajd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "49f8ddca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, target_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input processing\n",
    "        self.embedding = Embeddings(target_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Stack of decoder layers\n",
    "        self.decoder_layers = nn.ModuleList([TransformerDecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, target_tokens, encoder_output, source_mask=None, target_mask=None):\n",
    "        # Step 1: Convert tokens to embeddings + add positional encoding\n",
    "        embeddings = self.embedding(target_tokens)  # [batch, seq_len, d_model]\n",
    "        decoder_input = self.positional_encoding(embeddings)\n",
    "        \n",
    "        # Step 2: Pass through each decoder layer\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            decoder_input = decoder_layer(\n",
    "                target_input=decoder_input,\n",
    "                encoder_output=encoder_output,\n",
    "                source_mask=source_mask,\n",
    "                target_mask=target_mask\n",
    "            )\n",
    "        \n",
    "        return decoder_input  # [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6852ba5",
   "metadata": {
    "id": "a6852ba5"
   },
   "source": [
    "\n",
    "We also modify the self-attention sub-layer in the decoder stack to\n",
    "prevent positions from attending to subsequent positions.  This\n",
    "masking, combined with fact that the output embeddings are offset by\n",
    "one position, ensures that the predictions for position $i$ can\n",
    "depend only on the known outputs at positions less than $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1fe1b467",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-02T01:25:18.813309Z",
     "iopub.status.busy": "2022-05-02T01:25:18.812387Z",
     "iopub.status.idle": "2022-05-02T01:25:18.815171Z",
     "shell.execute_reply": "2022-05-02T01:25:18.814519Z"
    },
    "id": "1fe1b467"
   },
   "outputs": [],
   "source": [
    "# \"Mask out subsequent positions.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8cb1c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"Create causal mask to prevent attending to future positions\"\"\"\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
    "    return mask == 0  # Convert to boolean mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad536665",
   "metadata": {
    "id": "ad536665"
   },
   "source": [
    "\n",
    "> Below the attention mask shows the position each tgt word (row) is\n",
    "> allowed to look at (column). Words are blocked for attending to\n",
    "> future words during training.\n",
    "\n",
    "\n",
    "TODO: add visualization of mask here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c162ba",
   "metadata": {
    "id": "e4c162ba",
    "lines_to_next_cell": 2
   },
   "source": [
    "![](images/ModalNet-20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74aef3a8",
   "metadata": {
    "id": "74aef3a8"
   },
   "source": [
    "### Transformer Model\n",
    "\n",
    "Both the encoder and decode blocks are repeated N times. In the original paper, they defined N=6, and we will define a similar value in this notebook.\n",
    "\n",
    "![](images/transformer_encoders_decoder_stacks.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0c4acc",
   "metadata": {},
   "source": [
    "- **Output Projection**: The transformer outputs 256-dimensional vectors, but we need probabilities over 1790 French words. The linear layer projects from 256 → 1790 dimensions, then softmax gives us a probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e92c9c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, \n",
    "                 source_vocab_size,     # Source vocabulary size\n",
    "                 target_vocab_size,     # Target vocabulary size  \n",
    "                 d_model=512,           # Model dimension\n",
    "                 num_heads=8,           # Number of attention heads\n",
    "                 num_layers=6,          # Number of encoder/decoder layers\n",
    "                 d_ff=2048,             # Feed-forward dimension\n",
    "                 max_seq_len=5000,      # Maximum sequence length\n",
    "                 dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Encoder stack\n",
    "        self.encoder = TransformerEncoder(\n",
    "            source_vocab_size=source_vocab_size,\n",
    "            d_model=d_model, \n",
    "            num_heads=num_heads, \n",
    "            num_layers=num_layers,\n",
    "            d_ff=d_ff, \n",
    "            max_seq_len=max_seq_len, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Decoder stack  \n",
    "        self.decoder = TransformerDecoder(\n",
    "            target_vocab_size=target_vocab_size,\n",
    "            d_model=d_model, \n",
    "            num_heads=num_heads, \n",
    "            num_layers=num_layers,\n",
    "            d_ff=d_ff, \n",
    "            max_seq_len=max_seq_len, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Final output projection to target vocabulary\n",
    "        self.output_projection = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source_tokens, target_tokens, source_mask=None, target_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass for training (teacher forcing)\n",
    "        \n",
    "        source_tokens: [batch_size, source_seq_len] - source token ids\n",
    "        target_tokens: [batch_size, target_seq_len] - target token ids  \n",
    "        \"\"\"\n",
    "        # Step 1: Encode source sequence\n",
    "        encoder_output = self.encoder(source_tokens, source_mask) # Shape: [batch_size, source_seq_len, d_model]\n",
    "        \n",
    "        # Step 2: Decode target sequence\n",
    "        decoder_output = self.decoder(\n",
    "            target_tokens=target_tokens,\n",
    "            encoder_output=encoder_output,\n",
    "            source_mask=source_mask,\n",
    "            target_mask=target_mask\n",
    "        )\n",
    "        # Shape: [batch_size, target_seq_len, d_model]\n",
    "        \n",
    "        # Step 3: Project to vocabulary logits\n",
    "        output_logits = self.output_projection(decoder_output)\n",
    "        # Shape: [batch_size, target_seq_len, target_vocab_size]\n",
    "        \n",
    "        return output_logits\n",
    "\n",
    "    def encode(self, source_tokens, source_mask=None):\n",
    "        \"\"\"Encode source sequence (for inference)\"\"\"\n",
    "        return self.encoder(source_tokens, source_mask)\n",
    "    \n",
    "    def decode_step(self, target_tokens, encoder_output, source_mask=None, target_mask=None):\n",
    "        \"\"\"Decode one step (for autoregressive generation)\"\"\"\n",
    "        decoder_output = self.decoder(target_tokens, encoder_output, source_mask, target_mask)\n",
    "        return self.output_projection(decoder_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2e8445",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "https://www.kaggle.com/datasets/mohamedlotfy50/wmt-2014-english-german?select=wmt14_translate_de-en_train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ce2a5e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'english_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-34339ecb7758>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m model = TransformerModel(\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menglish_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrench_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'english_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "# Model hyperparameters\n",
    "d_model = 256\n",
    "nhead = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "\n",
    "# Create model\n",
    "model = TransformerModel(\n",
    "    src_vocab_size=english_vocab.n_words,\n",
    "    tgt_vocab_size=french_vocab.n_words,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_encoder_layers,\n",
    "    num_decoder_layers=num_decoder_layers,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "# Set models to evaluation mode to disable dropout\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f00ba",
   "metadata": {},
   "source": [
    "Interview Questions:\n",
    "- Walk through the forward pass of a transformer block*\n",
    "- What are the differences between encoder-only, decoder-only, and encoder-decoder transformers?*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b7db72",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90d43c31",
   "metadata": {},
   "source": [
    "TODO: ### Softmax\n",
    "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  \n",
    "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "y8pwgh3dxn",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'english_ids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-72f20147de1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Create padded tensors for our dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0msource_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menglish_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mtarget_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgerman_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'english_ids' is not defined"
     ]
    }
   ],
   "source": [
    "class BuiltinTransformerWrapper(nn.Module):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size, d_model=512, num_heads=8, \n",
    "                 num_layers=6, d_ff=2048, max_seq_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Embeddings and positional encoding\n",
    "        self.source_embedding = Embeddings(source_vocab_size, d_model)\n",
    "        self.target_embedding = Embeddings(target_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
    "        \n",
    "        # Built-in transformer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_projection = nn.Linear(d_model, target_vocab_size)\n",
    "        \n",
    "    def forward(self, source_tokens, target_tokens, source_mask=None, target_mask=None):\n",
    "        # Apply embeddings and positional encoding\n",
    "        source_embeddings = self.positional_encoding(self.source_embedding(source_tokens))\n",
    "        target_embeddings = self.positional_encoding(self.target_embedding(target_tokens))\n",
    "        \n",
    "        # Use PyTorch's built-in transformer\n",
    "        transformer_output = self.transformer(\n",
    "            src=source_embeddings,\n",
    "            tgt=target_embeddings,\n",
    "            src_key_padding_mask=source_mask,\n",
    "            tgt_key_padding_mask=target_mask,\n",
    "            tgt_mask=create_causal_mask(target_tokens.size(1)) if target_mask is None else target_mask\n",
    "        )\n",
    "        \n",
    "        # Project to vocabulary\n",
    "        return self.output_projection(transformer_output)\n",
    "\n",
    "# Helper function to pad sequences\n",
    "def pad_sequences(sequences, pad_token=0):\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        padded.append(seq + [pad_token] * (max_len - len(seq)))\n",
    "    return torch.tensor(padded, dtype=torch.long)\n",
    "\n",
    "# Create padded tensors for our dataset\n",
    "source_tensor = pad_sequences(english_ids)\n",
    "target_tensor = pad_sequences(german_ids)\n",
    "\n",
    "print(f\"Source tensor shape: {source_tensor.shape}\")\n",
    "print(f\"Target tensor shape: {target_tensor.shape}\")\n",
    "print(f\"Source tensor:\\n{source_tensor}\")\n",
    "print(f\"Target tensor:\\n{target_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ayxa6s07hma",
   "metadata": {},
   "source": [
    "### Verification Pipeline with Our Custom Transformer\n",
    "\n",
    "Now let's test our custom Transformer implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s55ydo6yytc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built-in model parameters: 3,405,800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BuiltinTransformerWrapper(\n",
       "  (source_embedding): Embeddings(\n",
       "    (embedding): Embedding(1000, 256)\n",
       "  )\n",
       "  (target_embedding): Embeddings(\n",
       "    (embedding): Embedding(1000, 256)\n",
       "  )\n",
       "  (positional_encoding): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (1): TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (output_projection): Linear(in_features=256, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize both models with same parameters for comparison\n",
    "d_model = 256  # Smaller for faster verification\n",
    "num_heads = 8\n",
    "num_layers = 2  # Fewer layers for faster verification\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "\n",
    "# Create both models\n",
    "builtin_model = BuiltinTransformerWrapper(\n",
    "    source_vocab_size=1000,#len(english_vocab),\n",
    "    target_vocab_size=1000,#len(german_vocab),\n",
    "    d_model=d_model, \n",
    "    num_heads=num_heads,\n",
    "    num_layers=num_layers,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"Built-in model parameters: {sum(p.numel() for p in builtin_model.parameters()):,}\")\n",
    "#print(f\"Custom model parameters: {sum(p.numel() for p in custom_model.parameters()):,}\")\n",
    "\n",
    "# Set models to evaluation mode to disable dropout\n",
    "builtin_model.eval()\n",
    "#custom_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a420d051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchview in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (0.2.6)\n",
      "Collecting graphviz\n",
      "  Downloading graphviz-0.20.1-py3-none-any.whl (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.0/47.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting torchinfo\n",
      "  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
      "Collecting torchviz\n",
      "  Using cached torchviz-0.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torchviz) (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages (from torch->torchviz) (4.3.0)\n",
      "Installing collected packages: torchinfo, graphviz, torchviz\n",
      "Successfully installed graphviz-0.20.1 torchinfo-1.8.0 torchviz-0.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3.7 install torchview graphviz torchinfo torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6414df32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================================================================================================\n",
      "Layer (type:depth-idx)                             Input Shape               Output Shape              Param #                   Kernel Shape\n",
      "======================================================================================================================================================\n",
      "BuiltinTransformerWrapper                          [2, 10]                   [2, 10, 1000]             --                        --\n",
      "├─Embeddings: 1-1                                  [2, 10]                   [2, 10, 256]              --                        --\n",
      "│    └─Embedding: 2-1                              [2, 10]                   [2, 10, 256]              256,000                   --\n",
      "├─PositionalEncoding: 1-2                          [2, 10, 256]              [2, 10, 256]              --                        --\n",
      "│    └─Dropout: 2-2                                [2, 10, 256]              [2, 10, 256]              --                        --\n",
      "├─Embeddings: 1-3                                  [2, 10]                   [2, 10, 256]              --                        --\n",
      "│    └─Embedding: 2-3                              [2, 10]                   [2, 10, 256]              256,000                   --\n",
      "├─PositionalEncoding: 1-4                          [2, 10, 256]              [2, 10, 256]              --                        --\n",
      "│    └─Dropout: 2-4                                [2, 10, 256]              [2, 10, 256]              --                        --\n",
      "├─Transformer: 1-5                                 --                        [2, 10, 256]              --                        --\n",
      "│    └─TransformerEncoder: 2-5                     [2, 10, 256]              [2, 10, 256]              --                        --\n",
      "│    │    └─ModuleList: 3-1                        --                        --                        1,054,208                 --\n",
      "│    │    └─LayerNorm: 3-2                         [2, 10, 256]              [2, 10, 256]              512                       --\n",
      "│    └─TransformerDecoder: 2-6                     [2, 10, 256]              [2, 10, 256]              --                        --\n",
      "│    │    └─ModuleList: 3-3                        --                        --                        1,581,568                 --\n",
      "│    │    └─LayerNorm: 3-4                         [2, 10, 256]              [2, 10, 256]              512                       --\n",
      "├─Linear: 1-6                                      [2, 10, 256]              [2, 10, 1000]             257,000                   --\n",
      "======================================================================================================================================================\n",
      "Total params: 3,405,800\n",
      "Trainable params: 3,405,800\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 2.60\n",
      "======================================================================================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.82\n",
      "Params size (MB): 5.20\n",
      "Estimated Total Size (MB): 6.01\n",
      "======================================================================================================================================================\n"
     ]
    },
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/graphviz/backend/execute.py\u001b[0m in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1521\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: PosixPath('dot'): PosixPath('dot')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-58-7c91c27c1dcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltin_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_source\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mdot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuiltin_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mdot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'transformer_computation_graph'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/graphviz/_tools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                               category=category)\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/graphviz/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, filename, directory, view, cleanup, format, renderer, formatter, neato_no_op, quiet, quiet_view, outfile, engine, raise_if_result_exists, overwrite_source)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mrendered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/graphviz/_tools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m                               category=category)\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/graphviz/backend/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, renderer, formatter, neato_no_op, quiet, outfile, raise_if_result_exists, overwrite_filepath)\u001b[0m\n\u001b[1;32m    325\u001b[0m                       \u001b[0mcwd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparts\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                       \u001b[0mquiet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquiet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m                       capture_output=True)\n\u001b[0m\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/graphviz/backend/execute.py\u001b[0m in \u001b[0;36mrun_check\u001b[0;34m(cmd, input_lines, encoding, quiet, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute PosixPath('dot'), make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "# Visualizing the Model\n",
    "\n",
    "## Option 1: Using torchview (recommended)\n",
    "# !pip install torchview\n",
    "\"\"\"from torchview import draw_graph\n",
    "\n",
    "# Create sample inputs\n",
    "batch_size = 2\n",
    "seq_len = 10\n",
    "sample_source = torch.randint(1, 16, (batch_size, seq_len))\n",
    "sample_target = torch.randint(1, 16, (batch_size, seq_len))\n",
    "\n",
    "# Visualize model\n",
    "model_graph = draw_graph(\n",
    "    builtin_model, \n",
    "    input_data=[sample_source, sample_target],\n",
    "    expand_nested=True\n",
    ")\n",
    "model_graph.visual_graph.render('transformer_model', format='png')\n",
    "\"\"\"\n",
    "## Option 2: Using torchinfo for detailed summary\n",
    "# !pip install torchinfo\n",
    "from torchinfo import summary\n",
    "\n",
    "print(summary(builtin_model, \n",
    "              input_data=[sample_source, sample_target],\n",
    "              col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"],\n",
    "              depth=3))\n",
    "\n",
    "## Option 3: Using torchviz for computational graph\n",
    "# !pip install torchviz\n",
    "from torchviz import make_dot\n",
    "\n",
    "# Forward pass to create computation graph\n",
    "output = builtin_model(sample_source, sample_target)\n",
    "dot = make_dot(output.mean(), params=dict(builtin_model.named_parameters()))\n",
    "dot.render('transformer_computation_graph', format='png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ae9794",
   "metadata": {},
   "source": [
    "## References\n",
    "- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
    "- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
    "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "<!-- https://www.youtube.com/watch?v=rBCqOTEfxvg -->"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
