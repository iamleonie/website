{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: KV Caching\n",
    "subtitle: Key-Value caching inference optimization for LLMs\n",
    "description: test test test\n",
    "date: \"2025-06-06\"\n",
    "#date-modified: \"2025-02-22\"\n",
    "#categories: [news]\n",
    "bread-crumbs: true\n",
    "back-to-top-navigation: true\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "#image: images/pizza-13601_256.gif\n",
    "---\n",
    "\n",
    "\n",
    "Transformers are almost used everywhere in Natural Language Processing (ChatGPT, Bing)\n",
    "\n",
    "As you generate more text, you will use more GPU memory\n",
    "\n",
    "Why do transformers require more memory when dealing with longer texts\n",
    "\n",
    "openAI's pricing: longer context models are more expensive than smaller context ones\n",
    "high memory usage when you handle large context lengths\n",
    "\n",
    "high memory is taken up by the KV cache\n",
    "\n",
    "\n",
    "\"you know how you wait a while for the first token to generat but then the rest of it rips\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is KV cache\n",
    "\n",
    "KV caching (Key-Value caching) is an inference optimization technique for [Transformer](transformer.ipynb)-based models, particularly large language models (LLMs) to increase inference speed and reduce computational costs.\n",
    "\n",
    "The technique was introduced in the 2023 paper Efficiently scaling transformer inference\n",
    "\n",
    "KV caching eliminates redundant calculations during text generation by storing previously computed key and value tensors from the attention mechanism.\n",
    "This makes the AI responses faster and more efficient.\n",
    "\n",
    "This is the foundation of how modern LLMs can generate long outputs efficiently.\n",
    "\n",
    "KV -> refers to key and vaule used in attention mechanism\n",
    "The key and value states are used for calculating the self-attention mechanism.\n",
    "It caches the Key (K) and Value (V) states in Transformer-based language models\n",
    "\n",
    "## Motivation for KV Caching\n",
    "\n",
    "TODO: KV caching is motivated by an inefficiency in the Transformer-based, attention mechanism, autoregressive models\n",
    "\n",
    "Autoregressive Transformer-based language models like GPT generate text sequentially, one token at a time. \n",
    "During inference, the model processes the input sequence of previous tokens $[t_0, \\dots, t_i]$ (e.g., \"TODO ADD EXAMPLE HERE\") to predict the next token $t_{i+1}$​ (e.g., \"TODO\"). \n",
    "Then the model adds the generated token to the input sequence, creating a new input sequence of previous tokens  $[t_0, \\dots, t_{i+1}]$ and repeats the process until some stopping criterion is reached (e.g., generating an `<EOS>` token or reaching maximum length).\n",
    "\n",
    "![](images/autoregression.png)\n",
    "\n",
    "This creates a fundamental inefficiency: Imagine writing a sentence where, for each new word, you must re-read the entire text from the beginning to understand the context. While this might be manageable for short sentences, this becomes more and more inefficient the longer the text becomes.\n",
    "\n",
    "The extent of this inefficiency becomes clear when we examine the decoder's masked self-attention mechanism's key and value calculations in Transformer-based language models.\n",
    "\n",
    "In transformer architectures, the attention mechanism processes input sequences by computing three matrices Query (Q), Key (K), and Value (V). \n",
    "$$\n",
    "Attention(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "![](images/attention.png)\n",
    "\n",
    "During decoding, these three matrices are not the same size\n",
    "The input vector $x$ is multiplied by three different matrices $W$, which are learned from data:\n",
    "- $q = W_q x$\n",
    "- $k = W_k x$\n",
    "- $v = W_v x$\n",
    "\n",
    "\n",
    "you can think of the dot product between q and K as doing attention between the current token that we care about and all of the previous tokens at the same time.\n",
    "\n",
    "as we generate a sequence one token at a time the K and V matrices dont change very much. a token corresponds to a colum of the K matrix and a row of the V matrix.\n",
    "\n",
    "The cruicial informaiton is that once we've computed the embedding for this token, it's not going to change again, no matter how many more tokens we generate\n",
    "\n",
    "but the model still has to do the heavy work of computing the key and value vectors for this word on all subsequent steps.\n",
    "\n",
    "\n",
    "\n",
    "this results in a quadratic number of matrix vector multiplications. which is very slow\n",
    "\n",
    "![](images/attention_matrices.png)\n",
    "![](images/kv_cache_attention.png)\n",
    "\n",
    "\n",
    "\n",
    "### Inefficiency and where KV caching comes to play\n",
    "\n",
    "This (what? the redundancy, computation) is inefficient. \n",
    "KV Caching is an optimisation technique that mitigates this inefficiency.\n",
    "\n",
    "- Although transformers are internally parallel, each new prediction requires a full forward pass through all transformer layers, which incurs a quadratic memory/compute in terms of the sequence length. (verbatim)\n",
    "This repetition also leads to computational redundancy.  (verbatim)\n",
    "\n",
    "## How does KV caching work?\n",
    "\n",
    "https://www.dailydoseofds.com/p/kv-caching-in-llms-explained-visually/\n",
    "\n",
    "\n",
    "- The decode phase generates a single token at each time step, but each token depends on the key and value tensors of all previous tokens (including the input tokens’ KV tensors computed at prefill, and any new KV tensors computed until the current time step). \n",
    "- To avoid recomputing all these tensors for all tokens at each time step, it’s possible to cache them in GPU memory\n",
    "- Every iteration, when new elements are computed, they are simply added to the running cache to be used in the next iteration. \n",
    "- In some implementations, there is one KV cache for each layer of the model.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "KV caching works by storing the computed key and value tensors from previous tokens, \n",
    "To eliminate this inefficiency, we use KV Caching:\n",
    "1. After processing the initial prompt, we cache the computed keys ( KK ) and values ( VV ) for each layer.\n",
    "2. During generation, we only compute KK and VV for the new token, and append them to the cache.\n",
    "3. We compute QQ for the current token and use it with the cached KK and VV to get the output. (verbatim)\n",
    "\n",
    "\n",
    "![](images/key-value-caching.png)\n",
    "\n",
    "\n",
    "This changes generation from full-sequence re-computation to a lightweight, incremental update.\n",
    "\n",
    "allowing the model to:\n",
    "\n",
    "Reuse computations: Previously calculated K and V matrices are stored in memory\n",
    "Avoid redundant calculations: Only new tokens require fresh K and V computations\n",
    "Maintain context: The cached values preserve the model's understanding of earlier context\n",
    "Accelerate generation: Subsequent tokens generate much faster using cached data\n",
    "\n",
    "\n",
    "## Benefits, Limitations, and Trafe-offs of KV Caching\n",
    "the matrices obtained with KV caching are way smaller, which leads to faster matrix multiplications. \n",
    "- Speed Enhancement: KV caching can reduce inference time by 50-90% (TODO verify) for longer sequences, as the model doesn't need to recompute attention weights for previously processed tokens.\n",
    "\n",
    "\n",
    "KV caching eliminates unnecessary computation during autoregressive generation, enabling faster and more efficient inference, \n",
    "\n",
    "When to use KV caching:\n",
    "especially in long sequences and real-time applications. \n",
    "KV caching becomes increasingly beneficial with longer conversations and documents, making it essential for applications like: (multi-turn) Chat interfaces with extended conversations, Document analysis and summarization, Code generation with large codebases,\n",
    "\n",
    "\n",
    "KV caching is a popular method for speeding up LLM inference, making it possible to run them on consumer hardware\n",
    "\n",
    "\n",
    "The only downside is that it needs more GPU VRAM (or CPU RAM if GPU is not being used) to cache the Key and Value states.\n",
    "This is a trade-off between speed and memory, and its drawbacks can be more complex code and restricting fancier inference schemes, like beam-search, etc. \n",
    "\n",
    "\n",
    "## Where is KV caching used?\n",
    "KV caching is commonly used the self-attention layers in decoder-only models.\n",
    "\n",
    "KV caching occurs during multiple token generation steps and only happens in the decoder (i.e., in decoder-only models like GPT, or in the decoder part of encoder-decoder models like T5). Models like BERT are not generative and therefore do not have KV caching.) [verbatim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KV cache implementation (in Python, PyTorch)\n",
    "\n",
    "This is a simplified example of implementing KV caching in PyTorch: (verbatim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pseudocode for KV Caching in PyTorch\n",
    "class KVCache:\n",
    "    def __init__(self):\n",
    "        self.cache = {\"key\": None, \"value\": None}\n",
    "\n",
    "    def update(self, key, value):\n",
    "        if self.cache[\"key\"] is None:\n",
    "            self.cache[\"key\"] = key\n",
    "            self.cache[\"value\"] = value\n",
    "        else:\n",
    "            self.cache[\"key\"] = torch.cat([self.cache[\"key\"], key], dim=1)\n",
    "            self.cache[\"value\"] = torch.cat([self.cache[\"value\"], value], dim=1)\n",
    "\n",
    "    def get_cache(self):\n",
    "        return self.cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face `transformers` library \n",
    "\n",
    "Hugging Face Transformers includes built-in KV caching support for most transformer models, automatically managing cache creation and updates during generation. (slop)\n",
    "\n",
    "When using the transformers library this behavior is enabled by default through the `use_cache` parameter, you can also access multiple caching methods through the cache_implementation parameter, here's a minimalistic code : (verbati)\n",
    "\n",
    "Different cache implementations offer various trade-offs between speed and memory usage KV cache strategies:\n",
    "\n",
    "DynamicCache: Default, grows dynamically\n",
    "StaticCache: Fixed size, faster for known sequence lengths\n",
    "SinkCache: Keeps important tokens, evicts others\n",
    "SlidingWindowCache: Fixed window size\n",
    "OffloadedCache: CPU offloading for memory-constrained GPUs\n",
    "QuantizedCache: Reduces memory through quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('HuggingFaceTB/SmolLM2-1.7B')\n",
    "model = AutoModelForCausalLM.from_pretrained('HuggingFaceTB/SmolLM2-1.7B').cuda()\n",
    "\n",
    "tokens = tokenizer.encode(\"The red cat was\", return_tensors=\"pt\").cuda()\n",
    "output = model.generate(\n",
    "    tokens, max_new_tokens=300, use_cache = True # by default is set to True\n",
    ")\n",
    "output_text = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Inference Speed difference comparison between KV caching vs. standard inference\n",
    "Let's explore the inference speed difference with and without KV caching using the Hugging Face Transformers library\n",
    "Speed of ??? without KV caching:\n",
    "Speed of ??? with KV caching\n",
    "\n",
    "The difference in inference speed was huge while the GPU VRAM usage was neglectable, as reported here, so make sure to use KV caching in your transformer model!\n",
    "\n",
    "\n",
    "We benchmarked the code above with/without kv caching on a T4 GPU we got the following results : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2610e601727b417fb5392ac752f301a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e7f5809612f4c389b82ae14cd499528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6962d9202fe424189d601b8c7810472",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d68604608fd047098ece67ba7b27a9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2df2e4b0c7ef401083a2d4baadaa71a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4b3332822340a595518a320bbe3a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8006071207a145b68933534715ac9e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with KV caching: 108.446 +- 10.526 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "for use_cache in (True, False):\n",
    "  times = []\n",
    "  for _ in range(10):  # measuring 10 generations\n",
    "    start = time.time()\n",
    "    model.generate(**tokenizer(\"What is KV caching?\", return_tensors=\"pt\").to(device), use_cache=use_cache, max_new_tokens=1000)\n",
    "    times.append(time.time() - start)\n",
    "  print(f\"{'with' if use_cache else 'without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| echo: false\n",
    "#| output: false\n",
    "\n",
    "\n",
    "## Where is the KV Cache stored\n",
    "in memory\n",
    "\n",
    "## Does ChatGPT use cache, how to clean cache\n",
    "\n",
    "## How big is the KV cache\n",
    "\n",
    "## What is KV cache used for?\n",
    "\n",
    "## Can BERT use KV Cache\n",
    "\n",
    "- LLM, vLLM\n",
    "- code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refenreces\n",
    "\n",
    "- Pope, Reiner, et al. \"Efficiently scaling transformer inference.\" Proceedings of Machine Learning and Systems 5 (2023): 606-624.\n",
    "- https://huggingface.co/docs/transformers/main/en/kv_cache"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
