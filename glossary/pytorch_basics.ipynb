{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"PyTorch Fundamentals\"\n",
    "description: \"A comprehensive guide to PyTorch basics and essential operations\"\n",
    "date: \"2025-02-09\"\n",
    "date-modified: \"2025-02-22\"\n",
    "bread-crumbs: true\n",
    "back-to-top-navigation: true\n",
    "toc: true\n",
    "toc-depth: 3\n",
    "---\n",
    "\n",
    "Welcome to this comprehensive guide covering the essential PyTorch operations and concepts you need to master deep learning with PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/site-packages (from torch) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/site-packages (from torch) (2.8.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/site-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.13.1\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Tensor Operations\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch. Let's explore how to create and manipulate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Creation\n",
    "\n",
    "Creating tensors is the first step in any PyTorch workflow. Let's start by creating basic tensors from Python data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1, 2], [3, 4]]\n",
    "x = torch.tensor(data)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common tensor creation functions provide efficient ways to initialize tensors with specific patterns or random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(2, 3)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2781, -0.4898,  2.1639],\n",
       "        [-1.3768, -0.7805,  0.1928]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randn = torch.randn(2, 3)  # Normal distribution\n",
    "randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0310, 0.5926, 0.4155],\n",
       "        [0.5278, 0.0866, 0.6558]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.rand(2, 3)    # Uniform [0, 1)\n",
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 2, 4, 6, 8])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arange = torch.arange(0, 10, 2)  # Range with step\n",
    "arange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Properties\n",
    "\n",
    "The `.shape` property tells you the dimensions of your tensor. This is crucial for understanding what operations you can perform and for debugging tensor size mismatches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.dim()` method returns the number of dimensions (axes) in your tensor. A scalar has 0 dimensions, a vector has 1, a matrix has 2, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dim()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data types (`dtype`) control memory usage and computational precision. PyTorch supports various data types, with `float32` being the default for most operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing & Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor indexing works similarly to NumPy arrays, supporting integer indexing, slicing, boolean indexing, and fancy indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10, 11],\n",
       "        [12, 13, 14, 15, 16, 17],\n",
       "        [18, 19, 20, 21, 22, 23]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.arange(24).reshape(4, 6)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single element [1, 3]: tensor(9)\n",
      "First row: tensor([0, 1, 2, 3, 4, 5])\n",
      "First column: tensor([ 0,  6, 12, 18])\n",
      "Last row: tensor([18, 19, 20, 21, 22, 23])\n",
      "Submatrix [1:3, 2:5]:\n",
      "tensor([[ 8,  9, 10],\n",
      "        [14, 15, 16]])\n"
     ]
    }
   ],
   "source": [
    "# Basic indexing\n",
    "print(\"Single element [1, 3]:\", tensor[1, 3])\n",
    "print(\"First row:\", tensor[0])\n",
    "print(\"First column:\", tensor[:, 0])\n",
    "print(\"Last row:\", tensor[-1])\n",
    "print(\"Submatrix [1:3, 2:5]:\")\n",
    "print(tensor[1:3, 2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements > 10:\n",
      "tensor([[False, False, False, False, False, False],\n",
      "        [False, False, False, False, False,  True],\n",
      "        [ True,  True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True,  True]])\n",
      "tensor([11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23])\n",
      "Elements at positions (0,1) and (2,4):\n",
      "tensor([ 1, 16])\n"
     ]
    }
   ],
   "source": [
    "# Boolean indexing\n",
    "mask = tensor > 10\n",
    "print(\"Elements > 10:\")\n",
    "print(mask)\n",
    "print(tensor[mask])\n",
    "\n",
    "# Advanced indexing with lists\n",
    "rows = [0, 2]\n",
    "cols = [1, 4]\n",
    "print(\"Elements at positions (0,1) and (2,4):\")\n",
    "print(tensor[rows, cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Operations\n",
    "\n",
    "Dimension operations allow you to reshape, squeeze, unsqueeze, and transpose tensors. These operations are essential for preparing data for neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "Original: torch.Size([12])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(6)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `.reshape()` returns a tensor with the new shape, creating a copy if necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3],\n",
       "        [ 4,  5,  6,  7],\n",
       "        [ 8,  9, 10, 11]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape(2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `.view()` returns a view of the original tensor sharing the same data, but requires the tensor to be contiguous in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5],\n",
       "        [ 6,  7,  8,  9, 10, 11]])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Squeezing and unsqueezing are essential for matching tensor dimensions in neural networks, especially when handling batch dimensions or preparing tensors for operations that require specific shapes.\n",
    "\n",
    "- `.squeeze()` removes dimensions of size 1 \n",
    "- `.unsqueeze()` adds dimensions of size 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.9495,  1.1701,  0.2863,  1.2709]],\n",
      "\n",
      "         [[ 0.8985, -1.2210, -0.2796, -1.1833]],\n",
      "\n",
      "         [[-0.5155, -0.1029,  1.3931,  0.3732]]]])\n",
      "Original shape: torch.Size([1, 3, 1, 4])\n",
      "tensor([[-0.9495,  1.1701,  0.2863,  1.2709],\n",
      "        [ 0.8985, -1.2210, -0.2796, -1.1833],\n",
      "        [-0.5155, -0.1029,  1.3931,  0.3732]])\n",
      "After squeeze: torch.Size([3, 4])\n",
      "tensor([[[-0.9495,  1.1701,  0.2863,  1.2709],\n",
      "         [ 0.8985, -1.2210, -0.2796, -1.1833],\n",
      "         [-0.5155, -0.1029,  1.3931,  0.3732]]])\n",
      "After unsqueeze(0): torch.Size([1, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Squeeze and unsqueeze\n",
    "tensor_with_ones = torch.randn(1, 3, 1, 4)\n",
    "print(tensor_with_ones)\n",
    "print(\"Original shape:\", tensor_with_ones.shape)\n",
    "\n",
    "# Remove dimensions of size 1\n",
    "squeezed = tensor_with_ones.squeeze()\n",
    "print(squeezed)\n",
    "print(\"After squeeze:\", squeezed.shape)\n",
    "\n",
    "# Add dimension of size 1 at position 0\n",
    "unsqueezed = squeezed.unsqueeze(0)\n",
    "print(unsqueezed)\n",
    "print(\"After unsqueeze(0):\", unsqueezed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transpose and permute operations rearrange tensor dimensions:\n",
    "\n",
    "- `.T` - Reverses all dimensions (for 2D: rows become columns)\n",
    "- `.permute()` - Rearranges dimensions in a specific order you specify\n",
    "- Useful for matrix operations and changing data layout for neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([3, 4])\n",
      "Transposed shape: torch.Size([4, 3])\n",
      "3D tensor shape: torch.Size([2, 3, 4])\n",
      "Permuted shape: torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# Transpose operations\n",
    "matrix = torch.randn(3, 4)\n",
    "print(\"Original shape:\", matrix.shape)\n",
    "\n",
    "# Transpose (swap dimensions)\n",
    "transposed = matrix.T\n",
    "print(\"Transposed shape:\", transposed.shape)\n",
    "\n",
    "# Permute (rearrange dimensions)\n",
    "tensor_3d = torch.randn(2, 3, 4)\n",
    "permuted = tensor_3d.permute(2, 0, 1)  # (4, 2, 3)\n",
    "print(\"3D tensor shape:\", tensor_3d.shape)\n",
    "print(\"Permuted shape:\", permuted.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broadcasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device Management\n",
    "\n",
    "Managing GPU/CPU placement is critical when working with large language models. Understanding device handling ensures efficient memory usage and prevents device mismatch errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device placement determines whether tensors are stored in CPU or GPU memory. This is crucial for performance and memory management in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Computation and Autograd\n",
    "\n",
    "PyTorch's automatic differentiation system tracks operations on tensors to compute gradients automatically. This is essential for understanding how LLMs learn and update their parameters during training."
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Neural Network Training Basics\n\nTraining a neural network follows a consistent pattern regardless of the specific architecture. This section covers the fundamental training loop that forms the backbone of all deep learning workflows, including training large language models.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "### Basic Training Template\n\nThe neural network training process consists of several key components that work together in a structured loop. Let's examine each component and then see them in action with a complete example.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 1. Define a Simple Neural Network\n\nFirst, let's create a simple feedforward neural network for a regression task. This network will learn to map input features to target values.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "class SimpleNet(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, output_size)\n        self.relu = nn.ReLU()\n        \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n# Create model instance\ninput_size = 4\nhidden_size = 64\noutput_size = 1\n\nmodel = SimpleNet(input_size, hidden_size, output_size)\nprint(f\"Model architecture:\\n{model}\")\nprint(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 2. Generate Dummy Data\n\nFor this example, we'll create synthetic data with a known relationship between inputs and outputs. This allows us to verify that our training process is working correctly.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Generate synthetic regression data\nn_samples = 1000\nX = torch.randn(n_samples, input_size)\n\n# Create a known relationship: y = 2*x1 + 3*x2 - x3 + 0.5*x4 + noise\nweights = torch.tensor([2.0, 3.0, -1.0, 0.5])\ny = X @ weights + 0.1 * torch.randn(n_samples)  # Add some noise\ny = y.unsqueeze(1)  # Add dimension for output\n\nprint(f\"Input shape: {X.shape}\")\nprint(f\"Target shape: {y.shape}\")\nprint(f\"First 5 samples:\")\nprint(f\"X: {X[:5]}\")\nprint(f\"y: {y[:5].flatten()}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 3. Create DataLoader\n\nDataLoaders handle batching and shuffling of data, which is essential for efficient training and good convergence.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Split data into train and validation sets\ntrain_size = int(0.8 * n_samples)\nval_size = n_samples - train_size\n\nX_train, X_val = X[:train_size], X[train_size:]\ny_train, y_val = y[:train_size], y[train_size:]\n\n# Create datasets and dataloaders\ntrain_dataset = TensorDataset(X_train, y_train)\nval_dataset = TensorDataset(X_val, y_val)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")\nprint(f\"Number of training batches: {len(train_loader)}\")\nprint(f\"Number of validation batches: {len(val_loader)}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 4. Set Up Loss Function and Optimizer\n\nThe loss function measures how far our predictions are from the target values. The optimizer updates model parameters based on the computed gradients.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Loss function for regression\ncriterion = nn.MSELoss()\n\n# Optimizer - Adam is a good default choice\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\nprint(f\"Loss function: {criterion}\")\nprint(f\"Optimizer: {optimizer}\")\nprint(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 5. The Training Loop\n\nThis is the core of neural network training. The basic template follows these steps for each epoch:\n\n1. **Training phase**: Process batches, compute gradients, update parameters\n2. **Validation phase**: Evaluate model performance without updating parameters\n3. **Track metrics**: Monitor loss and other metrics to assess training progress",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def train_epoch(model, train_loader, criterion, optimizer):\n    \"\"\"Train the model for one epoch\"\"\"\n    model.train()  # Set model to training mode\n    total_loss = 0\n    num_batches = 0\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Clear gradients from previous iteration\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = model(data)\n        \n        # Compute loss\n        loss = criterion(output, target)\n        \n        # Backward pass - compute gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        # Track metrics\n        total_loss += loss.item()\n        num_batches += 1\n    \n    return total_loss / num_batches\n\ndef validate_epoch(model, val_loader, criterion):\n    \"\"\"Validate the model for one epoch\"\"\"\n    model.eval()  # Set model to evaluation mode\n    total_loss = 0\n    num_batches = 0\n    \n    with torch.no_grad():  # Disable gradient computation for efficiency\n        for data, target in val_loader:\n            # Forward pass only\n            output = model(data)\n            loss = criterion(output, target)\n            \n            # Track metrics\n            total_loss += loss.item()\n            num_batches += 1\n    \n    return total_loss / num_batches\n\nprint(\"Training and validation functions defined\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 6. Run the Complete Training Loop",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Training configuration\nnum_epochs = 100\ntrain_losses = []\nval_losses = []\n\nprint(\"Starting training...\")\nprint(f\"Epochs: {num_epochs}\")\nprint(f\"Batch size: {batch_size}\")\nprint(f\"Learning rate: {optimizer.param_groups[0]['lr']}\")\nprint(\"-\" * 50)\n\n# Main training loop\nfor epoch in range(num_epochs):\n    # Train for one epoch\n    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n    \n    # Validate for one epoch\n    val_loss = validate_epoch(model, val_loader, criterion)\n    \n    # Store losses for plotting\n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    \n    # Print progress every 10 epochs\n    if (epoch + 1) % 10 == 0:\n        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n              f\"Train Loss: {train_loss:.6f}, \"\n              f\"Val Loss: {val_loss:.6f}\")\n\nprint(\"-\" * 50)\nprint(\"Training completed!\")\nprint(f\"Final train loss: {train_losses[-1]:.6f}\")\nprint(f\"Final validation loss: {val_losses[-1]:.6f}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 7. Visualize Training Progress",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Plot training curves\nplt.figure(figsize=(12, 4))\n\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label='Training Loss', color='blue')\nplt.plot(val_losses, label='Validation Loss', color='red')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Training and Validation Loss')\nplt.legend()\nplt.grid(True)\n\nplt.subplot(1, 2, 2)\nplt.plot(train_losses[-50:], label='Training Loss (Last 50 epochs)', color='blue')\nplt.plot(val_losses[-50:], label='Validation Loss (Last 50 epochs)', color='red')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.title('Loss (Last 50 Epochs)')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### 8. Test the Trained Model",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Create a mini English-German translation dataset\ntranslation_data = [\n    (\"Hello world\", \"Hallo Welt\"),\n    (\"Good morning\", \"Guten Morgen\"),\n    (\"Thank you\", \"Danke schön\"),\n    (\"How are you?\", \"Wie geht es dir?\"),\n    (\"I love programming\", \"Ich liebe Programmieren\"),\n    (\"The weather is nice\", \"Das Wetter ist schön\"),\n    (\"I need help\", \"Ich brauche Hilfe\"),\n    (\"See you later\", \"Bis später\"),\n    (\"What time is it?\", \"Wie spät ist es?\"),\n    (\"I am learning German\", \"Ich lerne Deutsch\"),\n    (\"The book is interesting\", \"Das Buch ist interessant\"),\n    (\"Where is the station?\", \"Wo ist der Bahnhof?\"),\n    (\"I would like coffee\", \"Ich hätte gern Kaffee\"),\n    (\"Happy birthday\", \"Alles Gute zum Geburtstag\"),\n    (\"It's raining today\", \"Es regnet heute\"),\n    (\"I don't understand\", \"Ich verstehe nicht\"),\n    (\"Can you help me?\", \"Können Sie mir helfen?\"),\n    (\"The cat is sleeping\", \"Die Katze schläft\"),\n    (\"I am hungry\", \"Ich habe Hunger\"),\n    (\"Good night\", \"Gute Nacht\")\n]\n\nprint(\"English-German Translation Dataset (20 samples):\")\nprint(\"-\" * 60)\nfor i, (english, german) in enumerate(translation_data, 1):\n    print(f\"{i:2d}. EN: {english:<25} DE: {german}\")\n\n# Convert to tensor format for model training\nimport torch.nn.functional as F\n\n# Simple tokenization (in practice, you'd use a proper tokenizer)\ndef simple_tokenize(text, vocab):\n    \"\"\"Convert text to token indices\"\"\"\n    tokens = text.lower().split()\n    return [vocab.get(token, vocab['<UNK>']) for token in tokens]\n\n# Build vocabulary from the dataset\nenglish_texts = [pair[0] for pair in translation_data]\ngerman_texts = [pair[1] for pair in translation_data]\n\n# Create vocabulary\nenglish_vocab = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\ngerman_vocab = {'<PAD>': 0, '<UNK>': 1, '<SOS>': 2, '<EOS>': 3}\n\nfor text in english_texts:\n    for word in text.lower().split():\n        if word not in english_vocab:\n            english_vocab[word] = len(english_vocab)\n\nfor text in german_texts:\n    for word in text.lower().split():\n        if word not in german_vocab:\n            german_vocab[word] = len(german_vocab)\n\nprint(f\"\\nVocabulary sizes:\")\nprint(f\"English: {len(english_vocab)} tokens\")\nprint(f\"German: {len(german_vocab)} tokens\")\n\n# Convert texts to token sequences\nmax_len = 10  # Maximum sequence length\nenglish_sequences = []\ngerman_sequences = []\n\nfor eng_text, ger_text in translation_data:\n    # Tokenize and convert to indices\n    eng_tokens = simple_tokenize(eng_text, english_vocab)\n    ger_tokens = simple_tokenize(ger_text, german_vocab)\n    \n    # Add special tokens and pad\n    eng_tokens = eng_tokens[:max_len-1] + [english_vocab['<EOS>']]\n    ger_tokens = [german_vocab['<SOS>']] + ger_tokens[:max_len-2] + [german_vocab['<EOS>']]\n    \n    # Pad sequences to max_len\n    eng_tokens += [english_vocab['<PAD>']] * (max_len - len(eng_tokens))\n    ger_tokens += [german_vocab['<PAD>']] * (max_len - len(ger_tokens))\n    \n    english_sequences.append(eng_tokens)\n    german_sequences.append(ger_tokens)\n\n# Convert to tensors\nenglish_tensor = torch.tensor(english_sequences)\ngerman_tensor = torch.tensor(german_sequences)\n\nprint(f\"\\nTensor shapes:\")\nprint(f\"English: {english_tensor.shape}\")\nprint(f\"German: {german_tensor.shape}\")\nprint(f\"\\nSample tokenized pair:\")\nprint(f\"English: {english_sequences[0]} -> '{english_texts[0]}'\")\nprint(f\"German: {german_sequences[0]} -> '{german_texts[0]}'\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Key Training Loop Components Summary\n\nThe neural network training template follows this essential pattern:\n\n```python\n# 1. Setup\nmodel = YourModel()\ncriterion = nn.MSELoss()  # or nn.CrossEntropyLoss() for classification\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# 2. Training Loop\nfor epoch in range(num_epochs):\n    # Training Phase\n    model.train()\n    for batch in train_loader:\n        optimizer.zero_grad()      # Clear gradients\n        outputs = model(batch)     # Forward pass\n        loss = criterion(outputs, targets)  # Compute loss\n        loss.backward()            # Backward pass (compute gradients)\n        optimizer.step()           # Update parameters\n    \n    # Validation Phase\n    model.eval()\n    with torch.no_grad():\n        for batch in val_loader:\n            outputs = model(batch)\n            val_loss = criterion(outputs, targets)\n```\n\nThis template applies universally whether you're training simple feedforward networks, CNNs, RNNs, Transformers, or large language models. The core loop remains the same - only the model architecture and data preprocessing change.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module\n",
    "\n",
    "The base class for all neural network components in PyTorch. Understanding nn.Module is crucial since all transformer layers, attention mechanisms, and LLM architectures inherit from this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## nn.Embedding\n",
    "\n",
    "Embedding layers convert discrete tokens into dense vector representations. These are fundamental to all language models and determine how words, subwords, or characters are represented numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking\n",
    "\n",
    "Masking handles variable-length sequences and prevents attention to padding tokens. This is essential for efficient batch processing of text sequences in transformers and language models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Loss functions measure how far model predictions are from target values. CrossEntropyLoss is the standard choice for language modeling tasks, making it essential for training and fine-tuning LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizers and Learning Rate Scheduling\n",
    "\n",
    "Optimizers update model parameters based on computed gradients. Understanding Adam, learning rate schedules, and warmup strategies is crucial for effective LLM training and fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading for Text\n",
    "\n",
    "DataLoaders handle batching of variable-length text sequences efficiently. Understanding collate functions and padding strategies is essential for working with text data in production LLM workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Saving and Loading\n",
    "\n",
    "Saving and loading model checkpoints is crucial for LLM workflows due to long training times and large model sizes. Understanding state_dict and checkpoint management prevents data loss and enables model deployment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}