{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a429510",
      "metadata": {
        "id": "9a429510",
        "lines_to_next_cell": 2,
        "tags": []
      },
      "source": [
        "---\n",
        "title: \"Transformer\"\n",
        "description: \"Implementing a Transformer Architecture from scratch in PyTorch\"\n",
        "date: \"2025-08-09\"\n",
        "#date-modified: \"2025-02-22\"\n",
        "#categories: [news]\n",
        "bread-crumbs: true\n",
        "back-to-top-navigation: true\n",
        "toc: true\n",
        "toc-depth: 3\n",
        "#image: images/pizza-13601_256.gif\n",
        "---\n",
        "\n",
        "The Transformer architecture is the fundamental concept of LLMs (make this a better sentence).\n",
        "If you're working in Generative AI or you aspire to work in Generativ AI, you need to understand this. something sothine NLP\n",
        "\n",
        "The Transformer is a neural network architecture mostly used for natural language processing (NLP) tasks and was introduced in the [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. in 2017.\n",
        "\n",
        "<!-- Something about the Transformer architecture and the Attention mechanism -->\n",
        "\n",
        "The core idea is that for any given position in a sequence, the transformer asks \"What other positions in this sequence should I pay attention to?\"\n",
        "\n",
        "This article explains the core concepts of the Transformer architecture and its Python implementation from scratch using PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd0bc073",
      "metadata": {},
      "source": [
        "## Motivation for the Transformer\n",
        "\n",
        "[Sequence-to-sequence (seq2seq) models](TODO: Sutskever, 2014) transform an input sequence into an output sequence, where both sequences can be of arbitrary length. \n",
        "Therefore, they are useful for many NLP tasks, such as machine translation, question answering, or text summarization.\n",
        "\n",
        "<!-- Motivation for RNNs -->\n",
        "Early approaches for these models used [**Recurrent Neural Networks (RNNs)**](TODO), which process sequences step-by-step by maintaining a hidden state that captures information from previous inputs. \n",
        "This enabled memory of past information for tasks like language modeling and time series prediction.\n",
        "\n",
        "<!-- Limitation of RNNs -->\n",
        "However, vanilla RNNs suffer from the **vanishing gradients problem**, making them unable to remember information from earlier parts of longer input sequences.\n",
        "During backpropagation through time gradients are computed by repeatedly multiplying small partial derivatives as they flow backward through the network.\n",
        "This chain of multiplications causes gradients to become vanishingly small for earlier time steps, causing their weights to barely update during training.\n",
        "Since the network can't effectively learn how to use information from earlier time steps, it can't remember that information when making predictions.\n",
        "\n",
        "![](./images/translation_seq2seq.png)\n",
        "\n",
        "<!-- Motivation for LSTMs -->\n",
        "[**Long Short-Term Memory networks (LSTMs)**](TODO) are a specialized type of RNN, specifically developed to address this limitation. LSTMs use gating mechanisms to selectively remember and forget information over long sequences.\n",
        "Although they improved learning long-range dependency compared to vanilla RNNs, they still process sequences sequentially, resulting in training inefficiency.\n",
        "\n",
        "<!-- Motivation for Transformer -->\n",
        "The Transformer architecture was motivated by the need to overcome these sequential processing limitations while maintaining the ability to capture long-term dependencies. \n",
        "**TODO:** Add something more here...\n",
        "\n",
        "<!-- The difference between RNNs and Transformers -->\n",
        "<!-- Benefits of Transformers over RNNs -->\n",
        "- **Training efficiency:** The key difference between RNN-based (vanilla RNNs, LSTMs, GRUs) models and Transformer-based models is whether they process the tokens in a sequence step-by-step (sequentially) or simultaneously (in parallel). The parallelization enables faster training on modern hardware\n",
        "- **Handling of long sequences:** Transformers use self-attention mechanisms to focus on any part of a sequence, regardless of their distance. This enables them to bebetter at capturing relationships between distant tokens in a sequence\n",
        "\n",
        "<!-- When would you use which -->\n",
        "Today, Transformers dominate most sequence-to-sequence tasks due to their superior performance  over RNNs. \n",
        "Therefore, they are used in most generative AI systems today.\n",
        "However, RNNs are still useful for simple NLP tasks or time series forecasting or when computational resources are limited due to their simple architecture. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c96103f9",
      "metadata": {
        "id": "c96103f9"
      },
      "source": [
        "## Transformer Architecture Overview\n",
        "\n",
        "::: {.callout-warning}\n",
        "Interview Questions:\n",
        "- Walk through the forward pass of a transformer block*\n",
        ":::\n",
        "\n",
        "The original Transformer has an encoder-decoder architecture. Think of it like a human translator who first reads and comprehends the entire English sentence, then writes the French translation using that understanding:\n",
        "\n",
        "\n",
        "- **Encoder**: Processes the input sequence $(x_1, ..., x_n)$ and creates rich representations in the form of vector embeddings $\\mathbf{z} = (z_1, ...,z_n)$  (\"I'll read the English and create a rich understanding\")\n",
        "- **Decoder**: Generates the output sequence $(y_1,...,y_m)$ one token at a time, attending to both its own partial output and the encoder's representations (\"I'll use that understanding to generate French\")\n",
        "\n",
        "![Transformer architecture overview diagram showing encoder-decoder structure.](images/transformer_encoders_decoders.png)\n",
        "\n",
        "Both the encoder and decoder components are stacks of multiple ($N$) repeated encoder and decoder layers respectively.\n",
        "In the original paper, they defined N=6.\n",
        "\n",
        "The output of each encoder layer is passed as input to the next encoder layer.\n",
        "The output of the last encoder layer is passed as an input to every decoder layer in the decoder components.\n",
        "\n",
        "The encoder and decoder layers are all identical in structure but don't share any weights.\n",
        "\n",
        "![](images/transformer_encoders_decoder_stacks.png)\n",
        "\n",
        "At a more detailed level, the Transformer has the following architecture composed of the following core components:\n",
        "\n",
        "![Detailed Transformer architecture with encoder and decoder layers, self-attention, and feed-forward networks](images/architecture_detailed.png)\n",
        "\n",
        "::: {.callout-warning}\n",
        "What to do with this?\n",
        "- Input Embeddings (Inputs, Input embedding, output embedding, outputs shifted right)\n",
        "- Positional Encodings: to maintain sequence order without recurrence\n",
        "- Projection Layer (Softmax and Liniear)\n",
        ":::\n",
        "\n",
        "### Encoder Layer\n",
        "Each encoder layer has two sub-layers:\n",
        "\n",
        "1. **Multi-head self-attention layer**: helps the encoder look at other words in the input sentence as it encodes a specific word `MultiHeadAttention`\n",
        "2. **Position-wise fully-connected Feed Forward Neural Network:** what does this do? `FeedForward`\n",
        "\n",
        "> We employ residual connections around each of the sub-layers, followed by layer normalization `nn.LayerNorm`.\n",
        "\n",
        "\n",
        "![](images/encoder-architecture.png)\n",
        "\n",
        "### Decoder Layer\n",
        "Each decoder layer has the following sub-layers:\n",
        "\n",
        "- **Masked multi-Head self-attention layer** (masked multi head )\n",
        "    - Sub-layer 1: Self-attention (same sequence)\n",
        "    - query=key=value=target_input\n",
        "- **Mutli-Head Cross-attention**:\n",
        "    - helps the decoder focus on relevant parts of the input sentence\n",
        "    - query=target_input:          What we're generating\n",
        "    - key=value=encoder_output :    What we can look at\n",
        "- **Feed forward layer**\n",
        "\n",
        "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization.\n",
        "\n",
        "> We employ residual connections around each of the sub-layers, followed by layer normalization `nn.LayerNorm`.\n",
        "\n",
        "![](images/decoder-architecture.png)\n",
        "![\"Placeholder\"](imgages/placeholder.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3a83f49",
      "metadata": {},
      "source": [
        "## Implementing the Transformer from Scratch\n",
        "\n",
        "::: {.callout-tip}\n",
        "A Note on \"from scratch\"\n",
        "\n",
        "Note that PyTorch has the [class `nn.Transformer` and its components](https://github.com/pytorch/pytorch/blob/v2.7.0/torch/nn/modules/transformer.py) already built-in.\n",
        "The goal of this article, however, is to implement the Transformer from scratch to gain a better understanding of it.\n",
        "\n",
        "We will use some built-in PyTorch functions, such as the following because TODO:\n",
        "\n",
        "`nn.Linear`, `nn.Embedding`, `nn.LayerNorm`, `nn.Dropout`, `nn.ReLu`, `F.softmax`, TODO complete list...\n",
        ":::"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f8d663e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af74513f",
      "metadata": {
        "id": "af74513f"
      },
      "source": [
        "### Embeddings\n",
        "\n",
        "The transformer uses **learned embeddings** to represent the discrete tokens of text as numerical input for the neural network.\n",
        "Each unique token is mapped to a dense vector of fixed dimension $d_{\\text{model}}$ (typically 256, 512, or 768 dimensions).\n",
        "During training, these embedding vectors are learned to capture semantic relationships between tokens.\n",
        "\n",
        "![](images/embeddings.png)\n",
        "\n",
        "\n",
        "::: {.callout-tip}\n",
        "Early NLP approaches created vector representations of discrete tokens by using one-hot encoding of the entire vocabulary. \n",
        "This resulted in large, sparse vectors with the dimension of the vocabulary size (e.g., 50,000 tokens).\n",
        "These numerical representations were computationally inefficient and didn't carry any semantic meaning.\n",
        ":::\n",
        "\n",
        "\n",
        "The original paper scales embeddings by $\\sqrt{d_{model}}$ to:\n",
        "\n",
        "- ensure that embedding values and positional encoding values are roughly the same magnitude so that when they are  added together, neither dominates the other (TODO: we're already talking about positional embeddings here) \n",
        "- helps with **training stability**\n",
        "\n",
        "```pytorch\n",
        "src_emb = self.src_embedding(src) * math.sqrt(self.d_model)\n",
        "```\n",
        "\n",
        "The resulting embedding matrix is `d_model * vocab_size`\n",
        "\n",
        "<!-- Example \n",
        "- Suppose each embedding vector is of 512 dimension\n",
        "- suppose our vocab size is 100, \n",
        "- then our embedding matrix will be of size 100x512.\n",
        "This embedding matrix will be learned during training.\n",
        "During inference each word will be mapped to the corresponding 512 dimensional vector.\n",
        "Suppose we have batch size of 32 and sequence length of 10 words, The output will be 32x10x512.\n",
        "-->\n",
        "\n",
        "<!-- Difference between input and output embeddings -->\n",
        "\n",
        "**Input embeddings:**\n",
        "\n",
        "- the embedding only happens in the bottom-most encoder.\n",
        "- each encoder receives a list of vector each of the size (d_model 512)\n",
        "    - in the bottom encoder that would be the token embeddings\n",
        "    - in the other encoders, that's the output of the encoder that's directly below (the size of this list is a hyperparaeter we can set, the length of the longest sentece in the training dataset)\n",
        "- token in each position flows throgh its own path in the encoder\n",
        "    - there are dependencies between these paths in the self attention layer\n",
        "    - the feed-forard layer does not have those dependencies - thus the paths can be executed in parallel\n",
        "\n",
        "**Output embeddings:**\n",
        "\n",
        "1. The output of each step is fed to the bottom decoder in the next time step,\n",
        "2. the decoders bubble up their decoding results just like the encoders did.\n",
        "3. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\n",
        "4. The following steps repeat the process until a special symbol (e.g. TODO `'<EOS>'`) is reached indicating the transformer decoder has completed its output. -> where should this go? auto-regression?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba67f744",
      "metadata": {
        "id": "ba67f744"
      },
      "source": [
        "### Positional Encodings\n",
        "\n",
        "Positional encodings are vectors that tell the transformer where each token sits in a sequence.\n",
        "The motivation for positional encoding is that unlike RNNs, which processes tokens one by one, the transformer looks at all tokens in a sequence at the same time.\n",
        "This makes the transformer fast but it also means that - without positional information - it can't distinguish between \"The cat sat on the mat\" and \"The mat sat on the cat\" because it just sees the same [bag of words](bag_of_words.ipynb).\n",
        "\n",
        "To overcome this problem, you can add positional information in the form of positional encoding vectors directly to the input embeddings. The positional encoding vectors have the same dimensions $d_{model}$ (`d_model`)  as the input embeddings, so that they can be summed element-wise.\n",
        "This gives each token a combined representation that captures both its meaning and its location in the sequence.\n",
        "\n",
        "TODO: also add the vector visuals?\n",
        "![](images/positional_encodings.png)\n",
        "<!-- Visual comment \n",
        "for eg: if we have batch size of 32 and seq length of 10 and let embedding dimension be 512.\n",
        "Then we will have embedding vector of dimension 32 x 10 x 512.\n",
        "Similarly we will have positional encoding vector of dimension 32 x 10 x 512. Then we add both.\n",
        "-->\n",
        "\n",
        "The original transformer uses the **sinusoidal positional encodings**  based on sine and cosine functions of different frequencies:\n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
        "$$\n",
        "$$\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
        "$$\n",
        "\n",
        "where $pos$ is the position and $i$ is the dimension index.\n",
        "\n",
        "> Below the positional encoding will add in a sine wave based on position. The frequency and offset of the wave is different for each dimension.\n",
        "\n",
        "That is, each dimension of the positional encoding corresponds to a sinusoid.  \n",
        "The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  \n",
        "\n",
        "These formulas create unique patterns for each position by using different frequencies across the embedding dimensions. \n",
        "Position 0 has one pattern, position 1 has a slightly different pattern, and so on. \n",
        "This enables the model to learn to recognize not just absolute positions, but also relative distances between tokens.\n",
        "\n",
        "![\"Placeholder\"](images/placeholder.png)\n",
        "\n",
        "The inuition is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they are projected into QKV vectors and during dot-product attention.\n",
        "\n",
        "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  \n",
        "For the base model, we use a rate of $P_{drop}=0.1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cb1edb7",
      "metadata": {
        "id": "7cb1edb7"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model,  max_seq_len=5000, dropout=0.1,):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: dimension of embeddings\n",
        "            dropout: dropout rate, the original paper uses 0.1\n",
        "            max_seq_len: maximum sequence length\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
        "\n",
        "        # Create a div term for the denominator\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sin to even indices (0, 2, 4, ...)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply cos to odd indices (1, 3, 5, ...)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register as buffer (saved with model, not trained)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape : [batch_size, seq_len, d_model]\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len].requires_grad_(False) # TODO: what does this \"requires_grad_(False)\" do?\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6b1b3af",
      "metadata": {
        "id": "a6b1b3af"
      },
      "source": [
        "::: {.callout-tip}\n",
        "Note, that there are different ways to add positional information to input tokens, such as RoPE.\n",
        "<!-- Read more on [positional encoding](positional_encoding.ipynb) -->\n",
        ":::"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f295a5",
      "metadata": {
        "id": "48f295a5"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "The main component in the Transformer is the **multi-head self-attention mechanism**.\n",
        "\n",
        "<!-- Explain the attention mechanism in simple terms>\n",
        "<!-- Attention in simple terms -->\n",
        "The **attention mechanism**  allows the model to look (\"pay attention\") at different tokens to gather relevant information when processing a sequence.\n",
        "Instead of treating every token equally, the model learns which tokens are most relevant to each other for building good representations.\n",
        "\"It computes attention weights to determine the relevance of different parts.\"\n",
        "\n",
        "<!-- Example -->\n",
        "Take the following sentence for example:\n",
        "\n",
        "> ”The animal didn't cross the street because **it** was too tired”\n",
        "\n",
        "When processing the token \"it\", the model must determine whether it refers to the \"street\" or to the \"animal\".\n",
        "The self-attention mechanism allows the model to resolve this pronoun ambiguity and associate the token \"it\" with the token \"animal\".\n",
        "\n",
        "<!-- Attention visualization -->\n",
        "![](./images/attention_visualized.png)\n",
        "\n",
        "<!--\n",
        "![](images/transformer_self-attention_visualization.png)\n",
        "Be sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.\n",
        "- [] Add visual of formula here\n",
        "-->\n",
        "\n",
        "::: {.callout-tip}\n",
        "<!-- Other notes -->\n",
        "Note, that the attention mechanism is not a new concept. The concept of attention  was introduced by [Bahdanau 2014](https://arxiv.org/abs/1409.0473)\n",
        "\n",
        "There's many different attention mechanisms:\n",
        "\n",
        "- Content-base attention (Graves 2014)\n",
        "- [Additive: Bahdanau 2015](https://arxiv.org/abs/1409.0473):\n",
        "    Additive attention computes the compatibility function using a feed-forward network with a single hidden layer.  \n",
        "- Location-base (Luong2015)\n",
        "- General (Luong 2015)\n",
        "- Dot-product (multiplicative) (Luong 2014):\n",
        "    Dot-product attention is identical to our algorithm, except for the scaling factor of $\\frac{1}{\\sqrt{d_k}}$. \n",
        "- Scaled dot-product (Vaswani2017)\n",
        "\n",
        "The two most commonly used attention functions are additive attention, and dot-product (multiplicative) attention. \n",
        "While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code.\n",
        ":::\n",
        "\n",
        "#### Scaled dot-product attention\n",
        "\n",
        "We call our particular attention **\"Scaled Dot-Product Attention\"**.\n",
        "\n",
        "Create three vectors from each of the encoder’s input vectors (in this case, the embedding of each token).\n",
        "- Query and key vectors of dimension $d_k$\n",
        "- Value vectors of dimension $d_v$\n",
        "\n",
        "<!-- How are these vectors generated -->\n",
        "These vectors are created by multiplying the embedding by three matrices that we trained during the training process.\n",
        "<!-- What are these vectors? -->\n",
        "They’re abstractions that are useful for calculating and thinking about attention.\n",
        "\n",
        "The transformer views the encoded representation of the input as a set of key-value pairs $(K, V)$\n",
        "An attention function can be described as mapping a query and a set of key-value pairs to an output. \n",
        "The output vector is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.\n",
        "\n",
        "\n",
        "We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
        "\n",
        "<!-- Why do we scale the dot-product attention?-->\n",
        "While for small values of $d_k$ the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of $d_k$ [(cite)](https://arxiv.org/abs/1703.03906). \n",
        "We suspect that for large values of $d_k$, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.\n",
        "To illustrate why the dot products get large, assume that the components of $q$ and $k$ are independent random variables with mean $0$ and variance $1$.  \n",
        "Then their dot product, $q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance $d_k$.\n",
        "To counteract this effect, we scale the dot products by $\\frac{1}{\\sqrt{d_k}}$.\n",
        "Without the scaling factor, embeddings can dominate positional encodings. \n",
        "It's not arbitrary - it's mathematically necessary for stable training.\n",
        "Scale by $\\sqrt{d_k}$ to prevent gradients from becoming too small/prevents saturation in the softmax when `d_model` is large\n",
        "\n",
        "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.  \n",
        "The keys and values are also packed together into matrices $K$ and $V$.  \n",
        "We compute the matrix of outputs as:\n",
        "\n",
        "<!-- - *Derive the attention formula. How does it work mathematically? -->\n",
        "$$\n",
        "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "- Query ($Q$): What we're looking for\n",
        "- Key ($K$): What we're looking at\n",
        "- Value ($V$): What we actually use\n",
        "\n",
        "1. We compute similarity scores between queries and keys using dot products\n",
        "2. Scale by $\\sqrt{d_k}$ to prevent gradients from becoming too small/prevents saturation in the softmax when `d_model` is large\n",
        "3.  Masking lets us ignore padded tokens or implement causal attention\n",
        "3. Apply softmax to get attention weights that sum to 1 (Softmax normalizes the scores so they’re all positive and add up to 1.)\n",
        "4. Use these weights to compute a weighted average of the values\n",
        "\n",
        "![](./images/sclaed_dot_product_attention.png)\n",
        "\n",
        "<!-- *How would you implement multi-head attention from scratch?* -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcbf7f5f",
      "metadata": {
        "id": "dcbf7f5f"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    Compute 'Scaled Dot Product Attention'\n",
        "    Attention with optional masking\n",
        "    mask shape: [batch_size, seq_len, seq_len] or broadcastable\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute scaled attention scores\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    # Apply mask before softmax (set masked positions to large negative value)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # Softmax over the last dimension\n",
        "    p_attn = scores.softmax(dim=-1)\n",
        "\n",
        "    # Apply droppout after softmax\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83caa59d",
      "metadata": {
        "id": "83caa59d"
      },
      "source": [
        "#### Multi-head attentions (duplicated heading)\n",
        "\n",
        "- Instead of computing the attention once, the multi-head attention mechanism runs through the scaled dot-product attention multiple times in parallen\n",
        "- The independent attention outputs are concatenated in linearly transformed into the expected dimensions\n",
        "\n",
        "(IMG: multi head Ecoder-Decode attention layer)\n",
        "\n",
        "<!-- Motivation: Why do transformers use multiple attention heads? What does each head learn? -->\n",
        "- (I assume the motivation is because ensembling always helps? - Lilian weng)\n",
        "- The multi-headed attention improves the performance of the attention layer in two ways:\n",
        "\n",
        "1. It expands the model’s ability to focus on different positions. Yes, in the example above, z1 contains a little bit of every other encoding, but it could be dominated by the actual word itself. If we’re translating a sentence like “The animal didn’t cross the street because it was too tired”, it would be useful to know which word “it” refers to.\n",
        "2. It gives the attention layer multiple “representation subspaces”. As we’ll see next, with multi-headed attention we have not only one, but multiple sets of Query/Key/Value weight matrices (the Transformer uses eight attention heads, so we end up with eight sets for each encoder/decoder). Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings (or vectors from lower encoders/decoders) into a different representation subspace.\n",
        "\n",
        "We concat the matrices then multiply them by an additional weights matrix $W_O$.\n",
        "\n",
        "![Multi-Head Attention mechanism diagram: visualization of how multiple attention heads process input sequences in parallel, each focusing on different representation subspaces, then concatenating their outputs.](images/multihead-attention.png)\n",
        "\n",
        "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this.\n",
        "\n",
        "$$\n",
        "\\mathrm{MultiHead}(Q, K, V) =\n",
        "    \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\\n",
        "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
        "$$\n",
        "\n",
        "\n",
        "Where the projections are parameter matrices $W^Q_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in \\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.\n",
        "\n",
        "In this work we employ $h=8$ parallel attention layers, or heads.\n",
        "For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$.\n",
        "Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca4eea2",
      "metadata": {
        "id": "7ca4eea2"
      },
      "source": [
        "**Multi-head attention** runs multiple attention mechanisms in parallel, each focusing on different aspects of the relationships, then concatenates and projects the results.\n",
        "\n",
        "<!-- Attention vs. Multi-Head attention -->\n",
        "---\n",
        "\n",
        "The Transformer uses multi-head attention in three different ways:\n",
        "\n",
        "![Difference between Self-Attention in Encoder and Decoder vs. Cross-Attention](images/transformer_self_attention_vs_cross_attention.png)\n",
        "\n",
        "#### Self-Attention\n",
        "\n",
        "The encoder contains self-attention layers.  In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.  Each position in the encoder can attend to all positions in the previous layer of the encoder.\n",
        "\n",
        "#### Masked Self-Attention\n",
        "self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.  We need to prevent leftward information flow in the decoder to preserve the auto-regressive property.  We implement this inside of scaled dot-product attention by masking out (setting to $-\\infty$) all values in the input of the softmax which correspond to illegal connections.\n",
        "In the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation.\n",
        "Why masking? In the decoder, we need to prevent tokens from seeing future tokens during training.\n",
        "\n",
        "The encoder start by processing the input sequence.\n",
        "The output of the top encoder is then transformed into a set of attention vectors K and V.\n",
        "These are to be used by each decoder in its “encoder-decoder attention” layer which helps the decoder focus on appropriate places in the input sequence:\n",
        "\n",
        "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions.  This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position $i$ can depend only on the known outputs at positions less than $i$.\n",
        "\n",
        "> Below the attention mask shows the position each tgt word (row) is allowed to look at (column). Words are blocked for attending to future words during training.\n",
        "\n",
        "TODO: add visualization of mask here\n",
        "\n",
        "<!--  \n",
        "- How does causal/masked attention work in decoder models\n",
        "- Self-attention vs. Masked self-attention\n",
        "\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d030624d",
      "metadata": {
        "id": "d030624d"
      },
      "outputs": [],
      "source": [
        "# \"Mask out subsequent positions.\"\n",
        "def create_causal_mask(seq_len):\n",
        "    \"\"\"Create causal mask to prevent attending to future positions\"\"\"\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "    return mask == 0  # Convert to boolean mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cf3276c",
      "metadata": {
        "id": "9cf3276c"
      },
      "source": [
        "#### Cross-Attention\n",
        "\n",
        "The cross-attention mechanism, or often also called \"encoder-decoder attention\", works just like the self-attention mechanism, with the exception of where queries, keys, and values come from. While in the self-attention mechanism queries, keys, and values all come from same input sequence, in the cross-attention mechanism querie come from different sequences than  keys and values:\n",
        "\n",
        "- Queries: come from the previous decoder layer\n",
        "- Keys and values: come from the output of the encoder stack.\n",
        "\n",
        "This allows every position in the decoder to attend over all position in the encoder sequence.  \n",
        "\n",
        "<!-- Explain cross-attention vs self-attention -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de7360b",
      "metadata": {},
      "source": [
        "Implementation\n",
        "- Supports both self-attention and cross-attention\n",
        "- Handles different sequence lengths for encoder/decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "761cd3f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, dropout):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # dimension per head\n",
        "\n",
        "        # Linear projections for queries, keys, and values\n",
        "        self.W_query = nn.Linear(d_model, d_model)\n",
        "        self.W_key = nn.Linear(d_model, d_model)\n",
        "        self.W_value = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # Output projection\n",
        "        self.W_output = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        seq_len_query = query.size(1)\n",
        "        seq_len_key = key.size(1)\n",
        "\n",
        "        # Step 1: Linear projections for all heads at once\n",
        "        # Shape: [batch_size, seq_len, d_model]\n",
        "        Q = self.W_query(query)\n",
        "        K = self.W_key(key)\n",
        "        V = self.W_value(value)\n",
        "\n",
        "        # Step 2: Reshape to separate heads\n",
        "        # From [batch_size, seq_len, d_model] to [batch_size, seq_len, num_heads, d_k]\n",
        "        Q = Q.view(batch_size, seq_len_query, self.num_heads, self.d_k)\n",
        "        K = K.view(batch_size, seq_len_key, self.num_heads, self.d_k)\n",
        "        V = V.view(batch_size, seq_len_key, self.num_heads, self.d_k)\n",
        "\n",
        "        # Step 3: Transpose to [batch_size, num_heads, seq_len, d_k] for efficient computation\n",
        "        Q = Q.transpose(1, 2)  # [batch_size, num_heads, seq_len_query, d_k]\n",
        "        K = K.transpose(1, 2)  # [batch_size, num_heads, seq_len_key, d_k]\n",
        "        V = V.transpose(1, 2)  # [batch_size, num_heads, seq_len_key, d_k]\n",
        "\n",
        "        # Step 4: Apply scaled dot-product attention to each head\n",
        "        attention_output, attention_weights = scaled_dot_product_attention(\n",
        "            Q, K, V, mask\n",
        "        )\n",
        "        # attention_output: [batch_size, num_heads, seq_len_query, d_k]\n",
        "\n",
        "        # Step 5: Concatenate heads\n",
        "        # Transpose back: [batch_size, seq_len_query, num_heads, d_k]\n",
        "        attention_output = attention_output.transpose(1, 2)\n",
        "\n",
        "        # Reshape to concatenate heads: [batch_size, seq_len_query, d_model]\n",
        "        attention_output = attention_output.contiguous().view(\n",
        "            batch_size, seq_len_query, self.d_model\n",
        "        )\n",
        "\n",
        "        # Step 6: Final linear projection\n",
        "        output = self.W_output(attention_output)\n",
        "\n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1978bd3d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "def example_usage():\n",
        "    batch_size, seq_len, d_model = 2, 10, 512\n",
        "    num_heads = 8\n",
        "    dropout= 0.1\n",
        "    # Create sample input\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "\n",
        "    # Initialize multi-head attention\n",
        "    mha = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "    # Self-attention (query, key, value are all the same)\n",
        "    output, weights = mha(x, x, x)\n",
        "\n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Attention weights shape: {weights.shape}\")\n",
        "\n",
        "    # For encoder-decoder attention, you'd use different inputs:\n",
        "    # output, weights = mha(decoder_hidden, encoder_output, encoder_output)\n",
        "\n",
        "example_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582f83c7",
      "metadata": {
        "id": "582f83c7"
      },
      "source": [
        "### Position-wise Feed-Forward Networks\n",
        "\n",
        "The outputs of the self-attention layer are fed to a feed-forward neural network in the encoder.\n",
        "\n",
        "In addition to attention sub-layers, each of the layers in our\n",
        "encoder and decoder contains a fully connected feed-forward network,\n",
        "which is applied to each position separately and identically.  This\n",
        "consists of two linear transformations with a ReLU activation in\n",
        "between.\n",
        "\n",
        "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
        "\n",
        "While the linear transformations are the same across different\n",
        "positions, they use different parameters from layer to\n",
        "layer. Another way of describing this is as two convolutions with\n",
        "kernel size 1.  The dimensionality of input and output is\n",
        "$d_{\\text{model}}=512$, and the inner-layer has dimensionality\n",
        "$d_{ff}=2048$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5627c0d0",
      "metadata": {
        "id": "5627c0d0"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(self.w_1(x).relu()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c238f36",
      "metadata": {
        "id": "4c238f36"
      },
      "source": [
        "### Residual Connection and Layer normalization\n",
        "\n",
        "When we look at the encoder and decoder blocks, we see several normalization layers called Add & Norm.\n",
        "\n",
        "> We employ a [residual connection](https://arxiv.org/abs/1512.03385) around each of the two sub-layers, followed by [layer normalization](https://arxiv.org/abs/1607.06450).\n",
        "\n",
        "\n",
        "**Residual connection**: TODO: what is the purpose of this?\n",
        "we employ residual connections around each of the sub-layers, (The residual connection itself is just the addition operation: `x + Sublayer(x))`\n",
        "\n",
        "- The Core Idea: Instead of having each layer learn a complete new representation, we have it learn only the changes or refinements to make to the existing representation.\n",
        "- Think of it like editing a document:\n",
        "- Without residual connections: Each editor throws away the previous version and writes a completely new document from scratch\n",
        "- With residual connections: Each editor takes the existing document and only adds their improvements to it\n",
        "\n",
        "\n",
        "When we look at the architecture of the Transformer, we see that each sub-layer, including the self-attention and Feed Forward blocks, adds its output to its input before passing it to the Add & Norm layer. This approach integrates the output with the original input in the Add & Norm layer. This process is known as the skip connection, which allows the Transformer to train deep networks more effectively by providing a shortcut for the gradient to flow through during backpropagation.\n",
        "\n",
        "**Layer Normalization**: TODO: what is the purpose of this?\n",
        "*What is the purpose of layer normalization in transformers?*\n",
        "\n",
        "The output of each sub-layer is $\\mathrm{LayerNorm}(x +\\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function implemented by the sub-layer itself.\n",
        "We apply [dropout](http://jmlr.org/papers/v15/srivastava14a.html) to the output of each sub-layer, before it is added to the sub-layer input and normalized.\n",
        "To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension $d_{\\text{model}}=512$.\n",
        "\n",
        "\n",
        "The `LayerNormalization` class below performs layer normalization on the input data.\n",
        "During its forward pass, we compute the mean and standard deviation of the input data.\n",
        "We then normalize the input data by subtracting the mean and dividing by the standard deviation plus a small number called epsilon to avoid any divisions by zero.\n",
        "This process results in a normalized output with a mean 0 and a standard deviation 1.\n",
        "\n",
        "We will then scale the normalized output by a learnable parameter alpha and add a learnable parameter called bias. The training process is responsible for adjusting these parameters. The final result is a layer-normalized tensor, which ensures that the scale of the inputs to layers in the network is consistent.\n",
        "\n",
        "Layer normalization helps the transformer learn better and faster.\n",
        "\n",
        "Think of it like this: imagine you're trying to learn math, but every day the teacher uses completely different scales - sometimes numbers from 1-10, sometimes 1-1000, sometimes -500 to +500. It would be really hard to focus on the actual math concepts because you'd be constantly adjusting to these different scales.\n",
        "\n",
        "Layer normalization solves this problem by keeping all the numbers in a consistent, predictable range. For each example, it looks at all the features and normalizes them so they have an average of 0 and spread nicely around that average. This way, each layer in the transformer gets inputs that are always in the same comfortable range.\n",
        "\n",
        "This makes training much more stable and allows the model to learn the important patterns instead of getting distracted by wildly varying number scales.\n",
        "\n",
        "\n",
        "```python\n",
        "x = self.layer_norm_1(x + self.dropout(attention_output))\n",
        "```\n",
        "\n",
        "Layer Normalization dramatically improves trainability.\n",
        "\n",
        "- Post-norm (original) $z_i = \\text{LN}(\\text{Module}(x_i) + x_i)$\n",
        "- Post-norm (modern) $z_i = \\text{Module}(\\text{LN}(x_i)) + x_i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fqv3mbxw4h",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqv3mbxw4h",
        "outputId": "1f330ef6-cae3-4b92-b3f2-dfa0922161f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original data (notice the different scales):\n",
            "Sequence 1 (small numbers):\n",
            "tensor([[0.1000, 0.2000, 0.3000, 0.4000],\n",
            "        [0.2000, 0.1000, 0.4000, 0.3000],\n",
            "        [0.3000, 0.4000, 0.1000, 0.2000]])\n",
            "Sequence 2 (large numbers):\n",
            "tensor([[100., 200., 300., 400.],\n",
            "        [150., 250., 350., 450.],\n",
            "        [200., 300., 400., 500.]])\n",
            "\n",
            "After layer normalization:\n",
            "Sequence 1:\n",
            "tensor([[-1.3411, -0.4470,  0.4470,  1.3411],\n",
            "        [-0.4470, -1.3411,  1.3411,  0.4470],\n",
            "        [ 0.4470,  1.3411, -1.3411, -0.4470]], grad_fn=<SelectBackward0>)\n",
            "Sequence 2:\n",
            "tensor([[-1.3416, -0.4472,  0.4472,  1.3416],\n",
            "        [-1.3416, -0.4472,  0.4472,  1.3416],\n",
            "        [-1.3416, -0.4472,  0.4472,  1.3416]], grad_fn=<SelectBackward0>)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create some example data with different scales and ranges\n",
        "torch.manual_seed(42)  # For reproducible results\n",
        "\n",
        "# Example 1: Data with wildly different scales\n",
        "batch_size, seq_len, d_model = 2, 3, 4\n",
        "x = torch.tensor([\n",
        "    # First sequence: small numbers\n",
        "    [[0.1, 0.2, 0.3, 0.4],\n",
        "     [0.2, 0.1, 0.4, 0.3],\n",
        "     [0.3, 0.4, 0.1, 0.2]],\n",
        "\n",
        "    # Second sequence: large numbers\n",
        "    [[100, 200, 300, 400],\n",
        "     [150, 250, 350, 450],\n",
        "     [200, 300, 400, 500]]\n",
        "], dtype=torch.float32)\n",
        "\n",
        "print(\"Original data (notice the different scales):\")\n",
        "print(\"Sequence 1 (small numbers):\")\n",
        "print(x[0])\n",
        "print(\"Sequence 2 (large numbers):\")\n",
        "print(x[1])\n",
        "print()\n",
        "\n",
        "# Show statistics before normalization\n",
        "#print(\"Statistics BEFORE layer norm:\")\n",
        "#print(f\"Sequence 1 - Mean: {x[0].mean(-1):.2f}, Std: {x[0].std(-1):.2f}\")\n",
        "#print(f\"Sequence 2 - Mean: {x[1].mean(-1):.2f}, Std: {x[1].std(-1):.2f}\")\n",
        "#print()\n",
        "\n",
        "# Apply layer normalization\n",
        "layer_norm = nn.LayerNorm(d_model)\n",
        "x_normalized = layer_norm(x)\n",
        "\n",
        "print(\"After layer normalization:\")\n",
        "print(\"Sequence 1:\")\n",
        "print(x_normalized[0])\n",
        "print(\"Sequence 2:\")\n",
        "print(x_normalized[1])\n",
        "print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e4aa4d",
      "metadata": {
        "id": "17e4aa4d"
      },
      "source": [
        "### Linear Projection\n",
        "\n",
        "The decoder stack outputs a vector of floats.\n",
        "How do we turn that into a word?\n",
        "\n",
        "The linear projection consists of two layers:\n",
        "\n",
        "- **Linear layer:** is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector.\n",
        "    Let’s assume that our model knows 10,000 unique English words (our model’s “output vocabulary”) that it’s learned from its training dataset.\n",
        "    This would make the logits vector 10,000 cells wide – each cell corresponding to the score of a unique word.\n",
        "    That is how we interpret the output of the model followed by the Linear layer.\n",
        "- **Softmax layer**: then turns those scores into probabilities (all positive, all add up to 1.0).\n",
        "The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n",
        "\n",
        "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  \n",
        "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation.\n",
        "\n",
        "Example:\n",
        "The transformer outputs 256-dimensional vectors, but we need probabilities over 1790 French words.\n",
        "The linear layer projects from 256 → 1790 dimensions, then softmax gives us a probability distribution.\n",
        "\n",
        "\n",
        "![](images/transformer_decoder_output_softmax.png)\n",
        "\n",
        "\n",
        "```python\n",
        "# Final output projection to target vocabulary\n",
        "self.output_projection = nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "# Step 3: Project to vocabulary logits\n",
        "output_logits = self.output_projection(decoder_output)\n",
        "```\n",
        "\n",
        "\n",
        "Notice, how we don't have the Softmax layer here?\n",
        "\n",
        "During training the loss function handles the softmax internally\n",
        "\n",
        "```python\n",
        "loss = nn.CrossEntropyLoss()(logits.view(-1, vocab_size), targets.view(-1))\n",
        "```\n",
        "\n",
        "And during Apply softmax to get probabilities\n",
        "\n",
        "```python\n",
        "probs = torch.softmax(logits[:, -1, :], dim=-1)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74aef3a8",
      "metadata": {
        "id": "74aef3a8"
      },
      "source": [
        "## Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60219a38",
      "metadata": {
        "id": "60219a38"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 num_layers,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # Stack of encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "            ])\n",
        "\n",
        "    def forward(self, x, source_mask=None):\n",
        "\n",
        "        # Pass the input (and mask) through each encoder layer\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            x = encoder_layer(x, source_mask)\n",
        "\n",
        "        return x  # [batch, seq_len, d_model]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987bb631",
      "metadata": {
        "id": "987bb631"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model,\n",
        "            num_heads,\n",
        "            d_ff,\n",
        "            dropout\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Sub-layer 1: Multi-head self-attention\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "        # Sub-layer 2: Feed-forward network\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        # Layer normalization for each sub-layer\n",
        "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x,\n",
        "            mask=None\n",
        "        ):\n",
        "\n",
        "        # Sub-layer 1: Multi-head self-attention with residual connection\n",
        "        # For self-attention: query, key, and value are all the same input\n",
        "        attention_output, attention_weights = self.self_attention(\n",
        "            query=x,    # Same input\n",
        "            key=x,      # Same input\n",
        "            value=x,    # Same input\n",
        "            mask=mask\n",
        "        )\n",
        "\n",
        "        # Post-norm: residual connection then normalize\n",
        "        x = self.layer_norm_1(x + self.dropout(attention_output))\n",
        "\n",
        "        # Sub-layer 2: Feed-forward with residual connection\n",
        "        feed_forward_output = self.feed_forward(x)\n",
        "\n",
        "        # Post-norm: residual connection then normalize\n",
        "        x = self.layer_norm_2(x + self.dropout(feed_forward_output))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd774ac1",
      "metadata": {
        "id": "fd774ac1"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 num_layers,\n",
        "                 d_ff,\n",
        "                 dropout):\n",
        "        super().__init__()\n",
        "\n",
        "        # Stack of decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "            ])\n",
        "\n",
        "    def forward(self, x, encoder_output, source_mask=None, target_mask=None):\n",
        "\n",
        "        # Pass through each decoder layer\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            x = decoder_layer(x, encoder_output, source_mask, target_mask)\n",
        "\n",
        "        return x  # [batch, seq_len, d_model] # TODO: revisit if we should include layer norm here..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f76a69ac",
      "metadata": {
        "id": "f76a69ac"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 d_model,\n",
        "                 num_heads,\n",
        "                 d_ff,\n",
        "                 dropout\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Sub-layer 1: Masked multi-head self-attention\n",
        "        self.masked_self_attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "\n",
        "        # Sub-layer 2: Multi-head encoder-decoder attention\n",
        "        self.encoder_decoder_attention = MultiHeadAttention(d_model, num_heads, dropout) # should this be called cross attention``\n",
        "\n",
        "        # Sub-layer 3: Feed-forward network\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "\n",
        "        # Layer normalization for each sub-layer\n",
        "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm_3 = nn.LayerNorm(d_model)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                x,\n",
        "                encoder_output,\n",
        "                source_mask=None,\n",
        "                target_mask=None\n",
        "                ):\n",
        "\n",
        "        # Sub-layer 1: Masked self-attention on target sequence\n",
        "        # For self-attention: query, key, and value are all the same input (target)\n",
        "        masked_attention_output, masked_attention_weights = self.masked_self_attention(\n",
        "            query=x,     # Same target input\n",
        "            key=x,       # Same target input\n",
        "            value=x,     # Same target input\n",
        "            mask=target_mask        # Causal mask to prevent seeing future tokens\n",
        "        )\n",
        "\n",
        "        # Post-norm: residual connection then normalize\n",
        "        x = self.layer_norm_1(x + self.dropout(masked_attention_output))\n",
        "\n",
        "        # Sub-layer 2: Encoder-decoder attention\n",
        "        # Query comes from decoder, key and value come from encoder\n",
        "        encoder_attention_output, encoder_attention_weights = self.encoder_decoder_attention(\n",
        "            query=x,     # What the decoder is generating\n",
        "            key=encoder_output,     # What information is available from encoder\n",
        "            value=encoder_output,   # What information to retrieve from encoder\n",
        "            mask=source_mask        # Mask for padding tokens in source\n",
        "        )\n",
        "\n",
        "        # Post-norm: residual connection then normalize\n",
        "        x = self.layer_norm_2(x + self.dropout(encoder_attention_output))\n",
        "\n",
        "        # Sub-layer 3: Feed-forward network\n",
        "        feed_forward_output = self.feed_forward(x)\n",
        "\n",
        "        # Post-norm: residual connection then normalize\n",
        "        x = self.layer_norm_3(x + self.dropout(feed_forward_output))\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e92c9c23",
      "metadata": {
        "id": "e92c9c23"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 source_vocab_size,     # Source vocabulary size\n",
        "                 target_vocab_size,     # Target vocabulary size\n",
        "                 d_model=512,           # Model dimension\n",
        "                 num_heads=8,           # Number of attention heads\n",
        "                 num_layers=6,          # Number of encoder/decoder layers\n",
        "                 d_ff=2048,             # Feed-forward dimension\n",
        "                 max_seq_len=5000,      # Maximum sequence length\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Input processing of embeddings and positional encodings \n",
        "        self.src_embedding = nn.Embedding(source_vocab_size, d_model) \n",
        "        self.tgt_embedding = nn.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len, dropout)\n",
        "\n",
        "        # Encoder stack\n",
        "        self.encoder = Encoder(\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            d_ff=d_ff,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Decoder stack\n",
        "        self.decoder = Decoder(\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_layers,\n",
        "            d_ff=d_ff,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Output projection to target vocabulary\n",
        "        self.output_projection = nn.Linear(d_model, target_vocab_size)\n",
        "\n",
        "    def forward(self, source_tokens, target_tokens, source_mask=None, target_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for training (teacher forcing)\n",
        "\n",
        "        source_tokens: [batch_size, source_seq_len] - source token ids\n",
        "        target_tokens: [batch_size, target_seq_len] - target token ids\n",
        "        \"\"\"\n",
        "\n",
        "        # Scale embeddings\n",
        "        src_emb = self.src_embedding(source_tokens) * math.sqrt(self.d_model)\n",
        "        tgt_emb = self.tgt_embedding(target_tokens) * math.sqrt(self.d_model)\n",
        "\n",
        "        # Add positional encoding\n",
        "        src_emb = self.pos_encoder(src_emb)\n",
        "        tgt_emb = self.pos_encoder(tgt_emb)\n",
        "\n",
        "        # Step 1: Encode source sequence\n",
        "        encoder_output = self.encoder(src_emb, \n",
        "                                      source_mask) # Shape: [batch_size, source_seq_len, d_model]\n",
        "\n",
        "        # Step 2: Decode target sequence\n",
        "        decoder_output = self.decoder(\n",
        "            tgt_emb,\n",
        "            encoder_output=encoder_output,\n",
        "            source_mask=source_mask,\n",
        "            target_mask=target_mask\n",
        "        )\n",
        "        # Shape: [batch_size, target_seq_len, d_model]\n",
        "\n",
        "        # Step 3: Project to vocabulary logits\n",
        "        output_logits = self.output_projection(decoder_output)\n",
        "        # Shape: [batch_size, target_seq_len, target_vocab_size]\n",
        "\n",
        "        return output_logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4da27d73",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create a dummy model instance with placeholder vocab sizes\n",
        "# Replace with actual vocab sizes if available\n",
        "model = TransformerModel(source_vocab_size=100, \n",
        "                         target_vocab_size=100,\n",
        "                         ).to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c332f71a",
      "metadata": {
        "id": "c332f71a"
      },
      "source": [
        "Let's visualize the implemented model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LsjWBSSQE22z",
      "metadata": {
        "id": "LsjWBSSQE22z"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install torchview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3eddbc3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "id": "f3eddbc3",
        "outputId": "38df27b0-1f2e-4f4f-a21c-6b88789635da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'images/transformer_model_pytorch.png'"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torchview import draw_graph\n",
        "\n",
        "# Create sample inputs\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "\n",
        "sample_source = torch.randint(1, 16, (batch_size, seq_len)).to(device) # Move to device\n",
        "sample_target = torch.randint(1, 16, (batch_size, seq_len)).to(device) # Move to device\n",
        "\n",
        "# Visualize model\n",
        "model_graph = draw_graph(\n",
        "    model, # Use the dummy model instance\n",
        "    input_data=[sample_source, sample_target],\n",
        "    expand_nested=True\n",
        ")\n",
        "\n",
        "model_graph.visual_graph.render('images/', format='png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4OlbJINF_s3",
      "metadata": {
        "id": "f4OlbJINF_s3"
      },
      "source": [
        "![](images/transformer_model_pytorch.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DFelPHKxGYeP",
      "metadata": {
        "id": "DFelPHKxGYeP"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8cc187c5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cc187c5",
        "outputId": "6928a5c1-ae06-4848-8c17-02fa2ab7d602"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=================================================================================================================================================\n",
            "Layer (type:depth-idx)                        Input Shape               Output Shape              Param #                   Kernel Shape\n",
            "=================================================================================================================================================\n",
            "TransformerModel                              [2, 10]                   [2, 10, 100]              --                        --\n",
            "├─Embedding: 1-1                              [2, 10]                   [2, 10, 512]              51,200                    --\n",
            "├─Embedding: 1-2                              [2, 10]                   [2, 10, 512]              51,200                    --\n",
            "├─PositionalEncoding: 1-3                     [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    └─Dropout: 2-1                           [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "├─PositionalEncoding: 1-4                     [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    └─Dropout: 2-2                           [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "├─Encoder: 1-5                                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    └─ModuleList: 2-3                        --                        --                        --                        --\n",
            "│    │    └─EncoderLayer: 3-1                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-1      --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-2                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-3               [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-4             [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-5                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-6               [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    └─EncoderLayer: 3-2                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-7      --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-8                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-9               [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-10            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-11                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-12              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    └─EncoderLayer: 3-3                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-13     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-14                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-15              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-16            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-17                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-18              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    └─EncoderLayer: 3-4                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-19     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-20                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-21              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-22            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-23                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-24              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    └─EncoderLayer: 3-5                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-25     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-26                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-27              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-28            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-29                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-30              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    └─EncoderLayer: 3-6                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-31     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-32                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-33              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-34            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-35                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-36              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "├─Decoder: 1-6                                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    └─ModuleList: 2-4                        --                        --                        --                        --\n",
            "│    │    └─DecoderLayer: 3-7                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-37     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-38                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-39              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─MultiHeadAttention: 4-40     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-41                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-42              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-43            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-44                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-45              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    └─DecoderLayer: 3-8                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-46     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-47                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-48              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─MultiHeadAttention: 4-49     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-50                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-51              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-52            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-53                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-54              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    └─DecoderLayer: 3-9                 [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-55     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-56                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-57              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─MultiHeadAttention: 4-58     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-59                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-60              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-61            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-62                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-63              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    └─DecoderLayer: 3-10                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-64     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-65                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-66              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─MultiHeadAttention: 4-67     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-68                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-69              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-70            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-71                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-72              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    └─DecoderLayer: 3-11                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-73     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-74                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-75              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─MultiHeadAttention: 4-76     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-77                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-78              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-79            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-80                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-81              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    └─DecoderLayer: 3-12                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─MultiHeadAttention: 4-82     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-83                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-84              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─MultiHeadAttention: 4-85     --                        [2, 10, 512]              1,050,624                 --\n",
            "│    │    │    └─Dropout: 4-86                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-87              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "│    │    │    └─FeedForward: 4-88            [2, 10, 512]              [2, 10, 512]              2,099,712                 --\n",
            "│    │    │    └─Dropout: 4-89                [2, 10, 512]              [2, 10, 512]              --                        --\n",
            "│    │    │    └─LayerNorm: 4-90              [2, 10, 512]              [2, 10, 512]              1,024                     --\n",
            "├─Linear: 1-7                                 [2, 10, 512]              [2, 10, 100]              51,300                    --\n",
            "=================================================================================================================================================\n",
            "Total params: 44,292,196\n",
            "Trainable params: 44,292,196\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (Units.MEGABYTES): 88.58\n",
            "=================================================================================================================================================\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 13.45\n",
            "Params size (MB): 177.17\n",
            "Estimated Total Size (MB): 190.62\n",
            "=================================================================================================================================================\n"
          ]
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "print(summary(model,\n",
        "              input_data=[sample_source, sample_target],\n",
        "              col_names=[\"input_size\", \"output_size\", \"num_params\"],\n",
        "              depth=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc2e8445",
      "metadata": {
        "id": "bc2e8445"
      },
      "source": [
        "## Training and Inference\n",
        "\n",
        "Now that we’ve covered the entire forward-pass process through an untrained Transformer, it would be useful to glance at the intuition of training the model.\n",
        "\n",
        "Since the original encoder-decoder Transformer model is useful for translation tasks, we will use an XYZ dataset:\n",
        "https://www.kaggle.com/datasets/mohamedlotfy50/wmt-2014-english-german?select=wmt14_translate_de-en_train.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1565f134",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2dd26d80",
      "metadata": {},
      "source": [
        "### Dataset Creation\n",
        "\n",
        "We'll create a sample English-German translation dataset. This can be easily replaced with larger datasets like WMT or Multi30k later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c9265e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download data\n",
        "import kagglehub\n",
        "from kagglehub import KaggleDatasetAdapter\n",
        "\n",
        "# Set the path to the file you'd like to load\n",
        "file_path = \"eng_-french.csv\"\n",
        "\n",
        "# Load the latest version\n",
        "df = kagglehub.load_dataset(\n",
        "  KaggleDatasetAdapter.PANDAS,\n",
        "  \"devicharith/language-translation-englishfrench\",\n",
        "  file_path,\n",
        "  # Provide any additional arguments like\n",
        "  # sql_query or pandas_kwargs. See the\n",
        "  # documenation for more information:\n",
        "  # https://github.com/Kaggle/kagglehub/blob/main/README.md#kaggledatasetadapterpandas\n",
        ")\n",
        "\n",
        "# Shuffle the dataframe with a fixed seed\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "df = df[:1000]\n",
        "print(\"First 5 records:\", df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e63759b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use the downloaded data from the dataframe 'df'\n",
        "# Assuming 'df' has columns named 'English words/sentences' and 'French words/sentences'\n",
        "translation_pairs = []\n",
        "for index, row in df.iterrows():\n",
        "    english_sentence = row['English words/sentences']\n",
        "    french_sentence = row['French words/sentences']\n",
        "    translation_pairs.append((english_sentence, french_sentence))\n",
        "\n",
        "# Split into training and validation sets\n",
        "train_size = int(0.8 * len(translation_pairs))\n",
        "train_pairs = translation_pairs[:train_size]\n",
        "val_pairs = translation_pairs[train_size:]\n",
        "\n",
        "print(f\"Total pairs: {len(translation_pairs)}\")\n",
        "print(f\"Training pairs: {len(train_pairs)}\")\n",
        "print(f\"Validation pairs: {len(val_pairs)}\")\n",
        "print(f\"\\\\nExample pairs:\")\n",
        "for i, (en, fr) in enumerate(train_pairs[:3]):\n",
        "    print(f\"{i+1}. English: '{en}' -> French: '{fr}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe61c59",
      "metadata": {},
      "source": [
        "### Data Preprocessing and Tokenization\n",
        "\n",
        "We'll create vocabularies and tokenization functions for both English and German.\n",
        "\n",
        " tokens can be (words, subwords, characters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a15c4e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "        self.idx2word = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n",
        "        self.word_count = {}\n",
        "        self.n_words = 4  # Count default tokens\n",
        "\n",
        "    def add_sentence(self, sentence):\n",
        "        # Simple split by space for now - can be improved with a proper tokenizer\n",
        "        for word in sentence.lower().split():\n",
        "            self.add_word(word)\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.word2idx:\n",
        "            self.word2idx[word] = self.n_words\n",
        "            self.idx2word[self.n_words] = word\n",
        "            self.word_count[word] = 1\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word_count[word] += 1\n",
        "\n",
        "    def sentence_to_indices(self, sentence, add_eos=True):\n",
        "        indices = [self.word2idx[\"<sos>\"]]\n",
        "        for word in sentence.lower().split():\n",
        "            if word in self.word2idx:\n",
        "                indices.append(self.word2idx[word])\n",
        "            else:\n",
        "                indices.append(self.word2idx[\"<unk>\"])\n",
        "        if add_eos:\n",
        "            indices.append(self.word2idx[\"<eos>\"])\n",
        "        return indices\n",
        "\n",
        "    def indices_to_sentence(self, indices):\n",
        "        words = []\n",
        "        for idx in indices:\n",
        "            if idx == self.word2idx[\"<eos>\"]:\n",
        "                break\n",
        "            if idx not in [self.word2idx[\"<pad>\"], self.word2idx[\"<sos>\"]]:\n",
        "                words.append(self.idx2word[idx])\n",
        "        return \" \".join(words)\n",
        "\n",
        "# Create vocabularies\n",
        "english_vocab = Vocabulary()\n",
        "french_vocab = Vocabulary() # Changed from german_vocab\n",
        "\n",
        "# Build vocabularies from training data\n",
        "print(\"Building vocabularies...\")\n",
        "for en_sentence, fr_sentence in train_pairs: # Changed from de_sentence\n",
        "    english_vocab.add_sentence(en_sentence)\n",
        "    french_vocab.add_sentence(fr_sentence) # Changed from german_vocab.add_sentence\n",
        "\n",
        "print(f\"English vocabulary size: {english_vocab.n_words}\")\n",
        "print(f\"French vocabulary size: {french_vocab.n_words}\") # Changed from German\n",
        "\n",
        "# Show some vocabulary examples\n",
        "print(f\"\\\\nEnglish words: {list(english_vocab.word2idx.keys())[:10]}\")\n",
        "print(f\"French words: {list(french_vocab.word2idx.keys())[:10]}\") # Changed from German\n",
        "\n",
        "# Test tokenization\n",
        "test_en = \"Hello world\"\n",
        "test_fr = \"Bonjour le monde\" # Changed from German\n",
        "en_indices = english_vocab.sentence_to_indices(test_en)\n",
        "fr_indices = french_vocab.sentence_to_indices(test_fr) # Changed from German\n",
        "\n",
        "print(f\"\\\\nTokenization test:\")\n",
        "print(f\"'{test_en}' -> {en_indices}\")\n",
        "print(f\"'{test_fr}' -> {fr_indices}\") # Changed from German\n",
        "print(f\"Back to text: '{english_vocab.indices_to_sentence(en_indices)}'\")\n",
        "print(f\"Back to text: '{french_vocab.indices_to_sentence(fr_indices)}')\") # Changed from German"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d9bcaff",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_vocab, tgt_vocab):\n",
        "        self.pairs = pairs\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src_sentence, tgt_sentence = self.pairs[idx]\n",
        "\n",
        "        # Convert to indices\n",
        "        src_indices = self.src_vocab.sentence_to_indices(src_sentence)\n",
        "        tgt_indices = self.tgt_vocab.sentence_to_indices(tgt_sentence)\n",
        "\n",
        "        return {\n",
        "            'src': torch.tensor(src_indices, dtype=torch.long),\n",
        "            'tgt': torch.tensor(tgt_indices, dtype=torch.long),\n",
        "            'src_text': src_sentence,\n",
        "            'tgt_text': tgt_sentence\n",
        "        }\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to pad sequences in a batch\"\"\"\n",
        "    src_sequences = [item['src'] for item in batch]\n",
        "    tgt_sequences = [item['tgt'] for item in batch]\n",
        "    src_texts = [item['src_text'] for item in batch]\n",
        "    tgt_texts = [item['tgt_text'] for item in batch]\n",
        "\n",
        "    # Pad sequences\n",
        "    src_padded = pad_sequence(src_sequences, batch_first=True, padding_value=0)\n",
        "    tgt_padded = pad_sequence(tgt_sequences, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        'src': src_padded,\n",
        "        'tgt': tgt_padded,\n",
        "        'src_text': src_texts,\n",
        "        'tgt_text': tgt_texts\n",
        "    }\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = TranslationDataset(train_pairs, english_vocab, french_vocab)\n",
        "val_dataset = TranslationDataset(val_pairs, english_vocab, french_vocab)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 4\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Training batches: {len(train_loader)}\")\n",
        "print(f\"Validation batches: {len(val_loader)}\")\n",
        "\n",
        "# Test the data loader\n",
        "print(\"\\\\nTesting data loader:\")\n",
        "for batch in train_loader:\n",
        "    print(f\"Source batch shape: {batch['src'].shape}\")\n",
        "    print(f\"Target batch shape: {batch['tgt'].shape}\")\n",
        "    print(f\"Source texts: {batch['src_text']}\")\n",
        "    print(f\"Target texts: {batch['tgt_text']}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ce2a5e1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "6ce2a5e1",
        "outputId": "e8dc327b-e4e9-4cbb-e2f2-3cfd2dee7c70"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'english_vocab' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-441194266.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Create model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m model = TransformerModel(\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msrc_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menglish_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mtgt_vocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrench_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_words\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0md_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'english_vocab' is not defined"
          ]
        }
      ],
      "source": [
        "# Create model\n",
        "model = TransformerModel(\n",
        "    src_vocab_size=english_vocab.n_words,\n",
        "    tgt_vocab_size=french_vocab.n_words,\n",
        "    d_model=512,\n",
        "    nhead=8,\n",
        "    num_layers=6,\n",
        "    dim_feedforward=2048,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "print(f\"Model created with {sum(p.numel() for p in model.parameters() if p.requires_grad):,} trainable parameters\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "# Set models to evaluation mode to disable dropout --> TODO, what does this mean?\n",
        "#model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y8pwgh3dxn",
      "metadata": {
        "id": "y8pwgh3dxn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Helper function to pad sequences\n",
        "def pad_sequences(sequences, pad_token=0):\n",
        "    max_len = max(len(seq) for seq in sequences)\n",
        "    padded = []\n",
        "    for seq in sequences:\n",
        "        padded.append(seq + [pad_token] * (max_len - len(seq)))\n",
        "    return torch.tensor(padded, dtype=torch.long)\n",
        "\n",
        "# Create padded tensors for our dataset\n",
        "source_tensor = pad_sequences(english_ids)\n",
        "target_tensor = pad_sequences(german_ids)\n",
        "\n",
        "print(f\"Source tensor shape: {source_tensor.shape}\")\n",
        "print(f\"Target tensor shape: {target_tensor.shape}\")\n",
        "print(f\"Source tensor:\\n{source_tensor}\")\n",
        "print(f\"Target tensor:\\n{target_tensor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80fb1c66",
      "metadata": {},
      "source": [
        "### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61f45425",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop\n",
        "# Training configuration\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 1#0\n",
        "#patience = 10  # For early stopping\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
        "\n",
        "# Training history\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "print(f\"Starting training for {num_epochs} epochs...\")\n",
        "print(f\"Learning rate: {learning_rate}\")\n",
        "#print(f\"Patience: {patience}\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_train_loss = 0\n",
        "    num_train_steps = 0\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        src = batch['src'].to(device)\n",
        "        tgt = batch['tgt'].to(device)\n",
        "\n",
        "        # Prepare input and target for training\n",
        "        tgt_input = tgt[:, :-1]  # Remove last token (<EOS>)\n",
        "        tgt_output = tgt[:, 1:]  # Remove first token (<SOS>)\n",
        "\n",
        "        # Zero gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        output = model(src, tgt_input)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping to prevent exploding gradients\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        total_train_loss += loss.item()\n",
        "        num_train_steps += 1\n",
        "\n",
        "        # Print progress\n",
        "        if (batch_idx + 1) % 100 == 0:\n",
        "            avg_loss = total_train_loss / num_train_steps\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # Calculate average training loss\n",
        "    avg_train_loss = total_train_loss / num_train_steps\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    total_val_loss = 0\n",
        "    num_val_steps = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            src = batch['src'].to(device)\n",
        "            tgt = batch['tgt'].to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            loss = criterion(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
        "\n",
        "            total_val_loss += loss.item()\n",
        "            num_val_steps += 1\n",
        "\n",
        "    avg_val_loss = total_val_loss / num_val_steps if num_val_steps > 0 else float('inf')\n",
        "    val_losses.append(avg_val_loss)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(avg_val_loss)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    \"\"\"\n",
        "    # Early stopping\n",
        "    if avg_val_loss < best_val_loss:\n",
        "        best_val_loss = avg_val_loss\n",
        "        patience_counter = 0\n",
        "        # Save best model\n",
        "        torch.save(model.state_dict(), 'best_transformer_model.pth')\n",
        "        print(f\"New best validation loss: {best_val_loss:.4f} - Model saved!\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch + 1} epochs\")\n",
        "            break\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "print(\"Training completed!\")\n",
        "\n",
        "# Load best model\n",
        "#model.load_state_dict(torch.load('best_transformer_model.pth'))\n",
        "print(f\"Best validation loss: {best_val_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d40600d7",
      "metadata": {},
      "source": [
        "### Evaluation and Metrics\n",
        "\n",
        "Let's visualize the training progress and evaluate the model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a63696d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot training and validation loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Plot on the first subplot\n",
        "plt.plot(train_losses, 'b-', label='Training Loss')\n",
        "plt.plot(val_losses, 'r-', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(f\"Final training loss: {train_losses[-1]:.4f}\")\n",
        "print(f\"Final validation loss: {val_losses[-1]:.4f}\")\n",
        "\n",
        "# Calculate accuracy on validation set\n",
        "def calculate_accuracy(model, data_loader, vocab):\n",
        "    model.eval()\n",
        "    correct_tokens = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            src = batch['src'].to(device)\n",
        "            tgt = batch['tgt'].to(device)\n",
        "\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            output = model(src, tgt_input)\n",
        "            predictions = output.argmax(dim=-1)\n",
        "\n",
        "            # Only count non-padding tokens\n",
        "            mask = (tgt_output != 0)\n",
        "            correct_tokens += ((predictions == tgt_output) & mask).sum().item()\n",
        "            total_tokens += mask.sum().item()\n",
        "\n",
        "    return correct_tokens / total_tokens if total_tokens > 0 else 0\n",
        "\n",
        "val_accuracy = calculate_accuracy(model, val_loader, french_vocab)\n",
        "print(f\"Validation accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a35684f1",
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate_sentence(model, sentence, src_vocab, tgt_vocab, max_length=50):\n",
        "    \"\"\"\n",
        "    Translate a single sentence using the trained model\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Convert input sentence to indices\n",
        "    src_indices = src_vocab.sentence_to_indices(sentence, add_eos=True)\n",
        "    src_tensor = torch.tensor([src_indices], dtype=torch.long).to(device)\n",
        "\n",
        "    # Initialize target with SOS token\n",
        "    tgt_indices = [tgt_vocab.word2idx[\"<sos>\"]]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            tgt_tensor = torch.tensor([tgt_indices], dtype=torch.long).to(device)\n",
        "\n",
        "            # Get model prediction\n",
        "            output = model(src_tensor, tgt_tensor)\n",
        "\n",
        "            # Get the prediction for the last token\n",
        "            next_token_logits = output[0, -1, :]\n",
        "            next_token = next_token_logits.argmax().item()\n",
        "\n",
        "            # Add predicted token to target sequence\n",
        "            tgt_indices.append(next_token)\n",
        "\n",
        "            # Stop if we predict EOS token\n",
        "            if next_token == tgt_vocab.word2idx[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "    # Convert indices back to sentence\n",
        "    translated_sentence = tgt_vocab.indices_to_sentence(tgt_indices)\n",
        "    return translated_sentence\n",
        "\n",
        "# Test on validation examples\n",
        "for i, (en_sentence, de_sentence) in enumerate(val_pairs[:5]):\n",
        "    translation = translate_sentence(model, en_sentence, english_vocab, french_vocab)\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"English: '{en_sentence}'\")\n",
        "    print(f\"Expected: '{de_sentence}'\")\n",
        "    print(f\"Generated: '{translation}'\")\n",
        "    print(\"-\" * 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fa118df",
      "metadata": {
        "id": "4fa118df"
      },
      "source": [
        "## Discussion\n",
        "\n",
        "### Architecture Variations\n",
        "<!-- What are the differences between encoder-only, decoder-only, and encoder-decoder transformers? -->\n",
        "Transformer-based models have three primary variations:\n",
        "Each variation has minor architectureal differences that make them suitable for süecific tasks\n",
        "\n",
        "- encoder-only\n",
        "    - architecture:\n",
        "    - how it works: proceesses input sequence as a whole and makes predictions about it\n",
        "    - use case: sentiment analysis, sentence classification, named entity recognition (NER), used for tasks that required understanding the overall meaning of a text\n",
        "    - Examples: Google's BERT, Meta's RoBERTa\n",
        "- decoder-only\n",
        "    - architecture:\n",
        "    - specifically designed to generate new seqeunces\n",
        "    - how it works: processes input sequence and generates a new sequence iteratively\n",
        "    - use case: text generation\n",
        "    - examples: most LLMs, scuha s OpenAI's GPT-4, Meta's LLaMa, Google's Gemini, Anthropic's Claude, xAI's Grok\n",
        "- The original Transformer is an encoder-decoder: \n",
        "    - The original Transformer architecture, introduced in the paper \"Attention Is All You Need\" is an encoder-decoder architecture.\n",
        "    - encoder component processes input seuaence and decoder uses that processes information to gnerate the output sequence\n",
        "    - use case: where the output is a transformation of the input, translation\n",
        "    - examples: MEta's BART, Google's T5\n",
        "\n",
        "![](./images/encoder-only_vs_decoder-only_transformer.png)\n",
        "\n",
        "### Limitations of the Attention mechanism\n",
        "\n",
        "The main limitation of the original Transformer architecture and it's derivatives is the self-attention mechanism's [**quadratic memory and computational requirements**](https://research.google/blog/constructing-transformers-for-longer-sequences-with-sparse-attention-methods/) ($O(N^2)$) with respect to the input sequence length ($N$). \n",
        "\n",
        "<!-- Why -->\n",
        "- Memory: Storing the attention matrix (N×N) requires O(N²) space\n",
        "- Computational: Computing all pairwise attention scores requires O(N²) operations\n",
        "\n",
        "![](./images/attention_visualized.png)\n",
        "\n",
        "<!-- Why do we want to process long sequences? -->\n",
        "This quadratic scaling was historically a major limitation that made Transformers expensive or unsuitable for tasks requiring long input sequences. For example, they couldn't effectively process entire articles or books for document summarization or long-form question answering.\n",
        "\n",
        "<!-- How to solve it -->\n",
        " but modern techniques have enabled \n",
        " Various techniques are introduced to reduce the complexity of attention.\n",
        "- There have been many $O(N)$ approximations, such as Linear Transformer, but they always come with a [speed and quality trade-off](https://arxiv.org/abs/2011.04006) (2020)\n",
        "- Quadratic memory isn't required for the attention mechanism if you're using [FlashAttention](https://arxiv.org/abs/2205.14135) (2022), [FlashAttention-2](https://arxiv.org/abs/2307.08691) (2023), /FlashAttention removes the quadratic memory requirement\n",
        "- Mamba\n",
        "- to cite: Efficient Transformers: A Survey\" by Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler\n",
        "- groupattention\n",
        "\n",
        "As an example, an average novel has about 100,000 words. \n",
        "The rule of thumb is that one English word is about 1.3 tokens.\n",
        "That means, an average novel has about 130,000 tokens.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ae9794",
      "metadata": {
        "id": "c1ae9794"
      },
      "source": [
        "## References\n",
        "- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
        "- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
        "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
        "<!-- https://www.youtube.com/watch?v=rBCqOTEfxvg -->\n",
        "- https://peterbloem.nl/blog/transformers\n",
        "- https://sebastianraschka.com/blog/2023/self-attention-from-scratch.html\n",
        "- https://docs.google.com/presentation/d/1ZXFIhYczos679r70Yu8vV9uO6B1J0ztzeDxbnBxD1S0/edit?slide=id.g13dd67c5ab8_0_2543#slide=id.g13dd67c5ab8_0_2543 \n",
        "- https://lilianweng.github.io/posts/2018-06-24-attention/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9878ae25",
      "metadata": {
        "id": "9878ae25"
      },
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "a=b\n",
        "import math\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: size of vocabulary\n",
        "            d_model: dimension of embeddings\n",
        "        \"\"\"\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input vector\n",
        "        Returns:\n",
        "            out: scaled embedding vector\n",
        "        \"\"\"\n",
        "        # Scale by sqrt(d_model) from original paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79436561",
      "metadata": {
        "id": "79436561"
      },
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "class ResidualConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(ResidualConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d4544d1",
      "metadata": {
        "id": "1d4544d1"
      },
      "outputs": [],
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "def encode(self, source_tokens, source_mask=None):\n",
        "    \"\"\"Encode source sequence (for inference)\"\"\"\n",
        "    return self.encoder(source_tokens, source_mask)\n",
        "\n",
        "def decode_step(self, target_tokens, encoder_output, source_mask=None, target_mask=None):\n",
        "    \"\"\"Decode one step (for autoregressive generation)\"\"\"\n",
        "    decoder_output = self.decoder(target_tokens, encoder_output, source_mask, target_mask)\n",
        "    return self.output_projection(decoder_output)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
