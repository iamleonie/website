{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9a429510",
      "metadata": {
        "id": "9a429510",
        "lines_to_next_cell": 2,
        "tags": []
      },
      "source": [
        "---\n",
        "title: \"Transformer\"\n",
        "description: \"Implementing a Transformer Architecture from scratch in PyTorch\"\n",
        "date: \"2025-02-09\"\n",
        "#date-modified: \"2025-02-22\"\n",
        "#categories: [news]\n",
        "bread-crumbs: true\n",
        "back-to-top-navigation: true\n",
        "toc: true\n",
        "toc-depth: 3\n",
        "#image: images/pizza-13601_256.gif\n",
        "---\n",
        "\n",
        "The Transformer is a neural network architecture for natural language processing (NLP) tasks and was introduced in the [\"Attention Is All You Need\"](https://arxiv.org/abs/1706.03762) paper by Vaswani et al. in 2017. \n",
        "\n",
        "\n",
        "The core idea is that for any given position in a sequence, the transformer asks \"What other positions in this sequence should I pay attention to?\"\n",
        "\n",
        "\n",
        "This article explains the core concepts and Python implementation in PyTorch of the Transformer. \n",
        "\n",
        "Note that PyTorch has the [class `nn.Transformer` and its components (`nn.TransformerEncoder`, `nn.TransformerDecoder`, `nn.TransformerEncoderLayer`, and `nn.TransformerDecoderLayer`)](https://github.com/pytorch/pytorch/blob/v2.7.0/torch/nn/modules/transformer.py) already built-in. The goal of this article, however, is to implement the Transformer from scratch to gain a better understanding of it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e535dd8a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.7/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "!pip3.7 install -q torchdata torchtext spacy altair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1bf3deb7",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-02T01:25:17.560273Z",
          "iopub.status.busy": "2022-05-02T01:25:17.559273Z",
          "iopub.status.idle": "2022-05-02T01:25:18.690005Z",
          "shell.execute_reply": "2022-05-02T01:25:18.690769Z"
        },
        "id": "1bf3deb7",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import math\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c96103f9",
      "metadata": {
        "id": "c96103f9"
      },
      "source": [
        "## Transformer Architecture Overview\n",
        "\n",
        "Transformers have an encoder-decoder architecture.\n",
        "That means they have two main components:\n",
        "\n",
        "- **Encoder**: Processes the input sequence $(x_1, ..., x_n)$ and creates rich representations in the form of vector embeddings $\\mathbf{z} = (z_1, ...,z_n)$\n",
        "- **Decoder**: Generates the output sequence $(y_1,...,y_m)$ one token at a time, attending to both its own partial output and the encoder's representations\n",
        "\n",
        "That's why they are commonly used for language translation models.\n",
        "\n",
        "![Transformer architecture overview diagram showing encoder-decoder structure.](images/transformer_encoders_decoders.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd5d4af0",
      "metadata": {
        "id": "bd5d4af0"
      },
      "source": [
        "## Core components\n",
        "At a more detailed level, the Transformer has the following architecture composed of the following core components:\n",
        "\n",
        "- Input Embeddings\n",
        "- Positional Encodings: to maintain sequence order without recurrence\n",
        "- Multi-Head Attention and Masked Multi-head attention\n",
        "- Feed Forward layer\n",
        "- Add & Norm Layer\n",
        "- Residual connections and layer normalization: to stabilize training\n",
        "- Projection Layer (Softmax and Liniear)\n",
        "\n",
        "![**Figure:** Detailed Transformer architecture with encoder and decoder layers, self-attention, and feed-forward networks](images/architecture_detailed.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af74513f",
      "metadata": {},
      "source": [
        "### Embeddings\n",
        "\n",
        "Embeddings convert discrete tokens (words, subwords, characters) into dense vector representations that the transformer can work with.\n",
        "\n",
        "The transformer uses learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{\\text{model}}$.  \n",
        "\n",
        "The Problem: Transformers need numerical input, but text consists of discrete symbols. For example, a vocabulary of 50,000 words would create massive, sparse one-hot vectors that are computationally inefficient and carry no semantic meaning.\n",
        "\n",
        "The Solution: Map each token to a dense vector of fixed dimension $d_{\\text{model}}$ (`d_model`)  (typically 256, 512, or 768 dimensions) that can encode semantic relationships.\n",
        "These vectors are learned during training to capture semantic similarity\n",
        "\n",
        "The original paper scales embeddings by $\\sqrt(d_{model})$ to:\n",
        "\n",
        "- ensure that embedding values and positional encoding values are roughly the same magnitude\n",
        "- when they're added together, neither dominates the other\n",
        "- helps with training stability\n",
        "\n",
        "The embedding matrix is `d_model * vocab_size`\n",
        "\n",
        "TODO:\n",
        "- [ ] difference between input and output embeddings\n",
        "\n",
        "![](images/embeddings.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "27dbc10c",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Embeddings(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            vocab_size: size of vocabulary\n",
        "            d_model: dimension of embeddings\n",
        "        \"\"\"\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: input vector\n",
        "        Returns:\n",
        "            out: scaled embedding vector\n",
        "        \"\"\"\n",
        "        # Scale by sqrt(d_model) from original paper\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d91272c4",
      "metadata": {},
      "source": [
        "Suppoese each embedding vector is of 512 dimension and suppose our vocab size is 100, then our embedding matrix will be of size 100x512. \n",
        "This embedding matrix will be learned during training. \n",
        "During inference each word will be mapped to the corresponding 512 dimensional vector. \n",
        "Suppose we have batch size of 32 and sequence length of 10 words, The output will be 32x10x512."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba67f744",
      "metadata": {},
      "source": [
        "### Positional Encodings\n",
        "\n",
        "Positional encodings are vectors that tell the transformer where each token sits in a sequence.\n",
        "The motivation for positional encoding is that unlike RNNs, which processes tokens one by one, the transformer looks at all tokens in a sentence at the same time. \n",
        "This makes the transformer fast but it also means that it can't distinguish between \"The cat sat on the mat\" and \"The mat sat on the cat\" because it just sees the same [bag of words](bag_of_words.ipynb).\n",
        "\n",
        "To overcome this problem, you can add positional information in the form of positional encoding vectors directly to the input embeddings. The positional encoding vectors have the same dimensions $d_{model}$ (`d_model`)  as the input embeddings, so that they can be summed element-wise.\n",
        "This gives each token a combined representation that captures both its meaning and its location in the sequence.\n",
        "\n",
        "![](images/positional_encodings.png)\n",
        "\n",
        "for eg: if we have batch size of 32 and seq length of 10 and let embedding dimension be 512. \n",
        "Then we will have embedding vector of dimension 32 x 10 x 512. \n",
        "Similarly we will have positional encoding vector of dimension 32 x 10 x 512. Then we add both.\n",
        "\n",
        "The original transformer uses the **sinusoidal positional encodings**  based on sine and cosine functions of different frequencies:\n",
        "$$\n",
        "PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
        "$$\n",
        "$$\n",
        "PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)\n",
        "$$\n",
        "\n",
        "where $pos$ is the position and $i$ is the dimension index. \n",
        "\n",
        "That is, each dimension of the positional encoding corresponds to a sinusoid.  \n",
        "The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.  \n",
        "\n",
        "These formulas create unique patterns for each position by using different frequencies across the embedding dimensions. Position 0 has one pattern, position 1 has a slightly different pattern, and so on. This enables the model to learn to recognize not just absolute positions, but also relative distances between tokens.\n",
        "\n",
        "The inuition is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they are projected into QKV vectors and during dot-product attention.\n",
        "\n",
        "In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks.  \n",
        "For the base model, we use a rate of $P_{drop}=0.1$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "7cb1edb7",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "\n",
        "    def __init__(self, d_model,  max_seq_len=5000, dropout=0.1,):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            d_model: dimension of embeddings\n",
        "            dropout: dropout rate, the original paper uses 0.1\n",
        "            max_seq_len: maximum sequence length\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_seq_len, d_model)\n",
        "        position = torch.arange(0, max_seq_len).unsqueeze(1)\n",
        "\n",
        "        # Create a div term for the denominator\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
        "\n",
        "        # Apply sin to even indices (0, 2, 4, ...)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "\n",
        "        # Apply cos to odd indices (1, 3, 5, ...)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "        # Add batch dimension\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        # Register as buffer (saved with model, not trained)\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape : [batch_size, seq_len, d_model]\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len].requires_grad_(False)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6b1b3af",
      "metadata": {},
      "source": [
        "\n",
        "> Below the positional encoding will add in a sine wave based on\n",
        "> position. The frequency and offset of the wave is different for\n",
        "> each dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "2b8b90e4",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'import altair as alt\\nimport pandas as pd\\n\\ndef example_positional():\\n    pe = PositionalEncoding(20, 0)\\n    y = pe.forward(torch.zeros(1, 100, 20))\\n\\n    data = pd.concat(\\n        [\\n            pd.DataFrame(\\n                {\\n                    \"embedding\": y[0, :, dim],\\n                    \"dimension\": dim,\\n                    \"position\": list(range(100)),\\n                }\\n            )\\n            for dim in [4, 5, 6, 7]\\n        ]\\n    )\\n\\n    return (\\n        alt.Chart(data)\\n        .mark_line()\\n        .properties(width=800)\\n        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\\n        .interactive()\\n    )\\n\\n\\nshow_example(example_positional)'"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#| echo: false\n",
        "#| output: true\n",
        "\"\"\"import altair as alt\n",
        "import pandas as pd\n",
        "\n",
        "def example_positional():\n",
        "    pe = PositionalEncoding(20, 0)\n",
        "    y = pe.forward(torch.zeros(1, 100, 20))\n",
        "\n",
        "    data = pd.concat(\n",
        "        [\n",
        "            pd.DataFrame(\n",
        "                {\n",
        "                    \"embedding\": y[0, :, dim],\n",
        "                    \"dimension\": dim,\n",
        "                    \"position\": list(range(100)),\n",
        "                }\n",
        "            )\n",
        "            for dim in [4, 5, 6, 7]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    return (\n",
        "        alt.Chart(data)\n",
        "        .mark_line()\n",
        "        .properties(width=800)\n",
        "        .encode(x=\"position\", y=\"embedding\", color=\"dimension:N\")\n",
        "        .interactive()\n",
        "    )\n",
        "\n",
        "\n",
        "show_example(example_positional)\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4ab4d4a",
      "metadata": {},
      "source": [
        "Note, that there are different ways to add positional information to input tokens. Read more on [positional encoding](positional_encoding.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48f295a5",
      "metadata": {},
      "source": [
        "### Attention mechanism\n",
        "\n",
        "**Attention** is the mechanism where one set of elements \"pays attention to\" another set of elements to gather relevant information.\n",
        "\n",
        "#### Self-attention\n",
        "\n",
        "*What is self-attention and how does it work mathematically?*\n",
        "\n",
        "*Why do transformers use multiple attention heads? What does each head learn?*\n",
        "\n",
        "*Derive the attention formula: Attention(Q,K,V) = softmax(QK^T/√d_k)V*\n",
        "\n",
        "*Why do we scale by √d_k in scaled dot-product attention?*\n",
        "\n",
        "*What is the computational complexity of self-attention?*\n",
        "\n",
        "*How does causal/masked attention work in decoder models*\n",
        "\n",
        "*Explain cross-attention vs self-attention*\n",
        "\n",
        "What are some alternatives to standard attention (sparse attention, linear attention, flash atention)?\n",
        "\n",
        "\n",
        "As the model processes each word (each position in the input sequence), self attention allows it to look at other positions in the input sequence for clues that can help lead to a better encoding for this word.\n",
        "> ”The animal didn't cross the street because it was too tired”\n",
        "\n",
        "What does “it” in this sentence refer to? Is it referring to the street or to the animal? \n",
        "When the model is processing the word “it”, self-attention allows it to associate “it” with “animal”.\n",
        "\n",
        "![](images/transformer_self-attention_visualization.png)\n",
        "\n",
        "Be sure to check out the Tensor2Tensor notebook where you can load a Transformer model, and examine it using this interactive visualization.\n",
        "\n",
        "- [] Add visual of formula here\n",
        "\n",
        "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.  \n",
        "The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a\n",
        "compatibility function of the query with the corresponding key.\n",
        "\n",
        "We call our particular attention \"Scaled Dot-Product Attention\".\n",
        "The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$.  We compute the dot products of the query with all keys, divide each by $\\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values.\n",
        "\n",
        "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$.  The keys and values are also packed together into matrices $K$ and $V$.  \n",
        "Wecompute the matrix of outputs as:\n",
        "\n",
        "$$\n",
        "\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "- Query ($Q$): What we're looking for\n",
        "- Key ($K$): What we're looking at\n",
        "- Value ($V$): What we actually use\n",
        "\n",
        "1. We compute similarity scores between queries and keys using dot products\n",
        "2. Scale by $\\sqrt{d_k}$ to prevent gradients from becoming too small/prevents saturation in the softmax when d_model is large\n",
        "3.  Masking lets us ignore padded tokens or implement causal attention\n",
        "3. Apply softmax to get attention weights that sum to 1\n",
        "4. Use these weights to compute a weighted average of the values\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "dcbf7f5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask=None, dropout=None):\n",
        "    \"\"\"\n",
        "    Compute 'Scaled Dot Product Attention'\n",
        "    Attention with optional masking\n",
        "    mask shape: [batch_size, seq_len, seq_len] or broadcastable\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute scaled attention scores\n",
        "    d_k = query.size(-1)\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "\n",
        "    # Apply mask before softmax (set masked positions to large negative value)\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "    # Softmax over the last dimension\n",
        "    p_attn = scores.softmax(dim=-1)\n",
        "\n",
        "    # Apply droppout after softmax\n",
        "    if dropout is not None:\n",
        "        p_attn = dropout(p_attn)\n",
        "\n",
        "    return torch.matmul(p_attn, value), p_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43a82e35",
      "metadata": {},
      "source": [
        "Self-Attention vs Cross-Attention\n",
        "Where Q, K, V Come From\n",
        "- Self-attention: all from same sequence (Q = K = V = same_input)\n",
        "- Cross-attention: from different sequences (Q = sequence_A, K = V = sequence_B)\n",
        "\n",
        "\n",
        "Key points:\n",
        "- Supports both self-attention and cross-attention\n",
        "- Handles different sequence lengths for encoder/decoder\n",
        "\n",
        "Implementation tips:\n",
        "- Use separate Q,K,V projections\n",
        "- Handle masking through addition (not masked_fill)\n",
        "- Remember to use braodcasting and reshape for multi-head attention\n",
        "\n",
        "\n",
        "Why masking? In the decoder, we need to prevent tokens from seeing future tokens during training.\n",
        "\n",
        "The goal of reducing sequential computation also forms the\n",
        "foundation of the Extended Neural GPU, ByteNet and ConvS2S, all of\n",
        "which use convolutional neural networks as basic building block,\n",
        "computing hidden representations in parallel for all input and\n",
        "output positions. In these models, the number of operations required\n",
        "to relate signals from two arbitrary input or output positions grows\n",
        "in the distance between positions, linearly for ConvS2S and\n",
        "logarithmically for ByteNet. This makes it more difficult to learn\n",
        "dependencies between distant positions. In the Transformer this is\n",
        "reduced to a constant number of operations, albeit at the cost of\n",
        "reduced effective resolution due to averaging attention-weighted\n",
        "positions, an effect we counteract with Multi-Head Attention.\n",
        "\n",
        "Self-attention, sometimes called intra-attention is an attention\n",
        "mechanism relating different positions of a single sequence in order\n",
        "to compute a representation of the sequence. Self-attention has been\n",
        "used successfully in a variety of tasks including reading\n",
        "comprehension, abstractive summarization, textual entailment and\n",
        "learning task-independent sentence representations. End-to-end\n",
        "memory networks are based on a recurrent attention mechanism instead\n",
        "of sequencealigned recurrence and have been shown to perform well on\n",
        "simple-language question answering and language modeling tasks.\n",
        "\n",
        "To the best of our knowledge, however, the Transformer is the first\n",
        "transduction model relying entirely on self-attention to compute\n",
        "representations of its input and output without using sequence\n",
        "aligned RNNs or convolution.\n",
        "\n",
        "\n",
        "The self-attention layer helps the encoder look at other words in the input sentence as it encodes a specific word."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83caa59d",
      "metadata": {},
      "source": [
        "The two most commonly used attention functions are additive\n",
        "attention [(cite)](https://arxiv.org/abs/1409.0473), and dot-product\n",
        "(multiplicative) attention.  Dot-product attention is identical to\n",
        "our algorithm, except for the scaling factor of\n",
        "$\\frac{1}{\\sqrt{d_k}}$. Additive attention computes the\n",
        "compatibility function using a feed-forward network with a single\n",
        "hidden layer.  While the two are similar in theoretical complexity,\n",
        "dot-product attention is much faster and more space-efficient in\n",
        "practice, since it can be implemented using highly optimized matrix\n",
        "multiplication code.\n",
        "\n",
        "\n",
        "While for small values of $d_k$ the two mechanisms perform\n",
        "similarly, additive attention outperforms dot product attention\n",
        "without scaling for larger values of $d_k$\n",
        "[(cite)](https://arxiv.org/abs/1703.03906). We suspect that for\n",
        "large values of $d_k$, the dot products grow large in magnitude,\n",
        "pushing the softmax function into regions where it has extremely\n",
        "small gradients (To illustrate why the dot products get large,\n",
        "assume that the components of $q$ and $k$ are independent random\n",
        "variables with mean $0$ and variance $1$.  Then their dot product,\n",
        "$q \\cdot k = \\sum_{i=1}^{d_k} q_ik_i$, has mean $0$ and variance\n",
        "$d_k$.). To counteract this effect, we scale the dot products by\n",
        "$\\frac{1}{\\sqrt{d_k}}$.\n",
        "\n",
        "![Multi-Head Attention mechanism diagram: visualization of how multiple attention heads process input sequences in parallel, each focusing on different representation subspaces, then concatenating their outputs.](images/multihead-attention.png)\n",
        "\n",
        "Multi-head attention allows the model to jointly attend to\n",
        "information from different representation subspaces at different\n",
        "positions. With a single attention head, averaging inhibits this.\n",
        "\n",
        "$$\n",
        "\\mathrm{MultiHead}(Q, K, V) =\n",
        "    \\mathrm{Concat}(\\mathrm{head_1}, ..., \\mathrm{head_h})W^O \\\\\n",
        "    \\text{where}~\\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)\n",
        "$$\n",
        "\n",
        "Where the projections are parameter matrices $W^Q_i \\in\n",
        "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^K_i \\in\n",
        "\\mathbb{R}^{d_{\\text{model}} \\times d_k}$, $W^V_i \\in\n",
        "\\mathbb{R}^{d_{\\text{model}} \\times d_v}$ and $W^O \\in\n",
        "\\mathbb{R}^{hd_v \\times d_{\\text{model}}}$.\n",
        "\n",
        "In this work we employ $h=8$ parallel attention layers, or\n",
        "heads. For each of these we use $d_k=d_v=d_{\\text{model}}/h=64$. Due\n",
        "to the reduced dimension of each head, the total computational cost\n",
        "is similar to that of single-head attention with full\n",
        "dimensionality.\n",
        "\n",
        "\n",
        "*How would you implement multi-head attention from scratch?*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ef3af60d",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads  # dimension per head\n",
        "        \n",
        "        # Linear projections for queries, keys, and values\n",
        "        self.W_query = nn.Linear(d_model, d_model)\n",
        "        self.W_key = nn.Linear(d_model, d_model)\n",
        "        self.W_value = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        # Output projection\n",
        "        self.W_output = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        seq_len_query = query.size(1)\n",
        "        seq_len_key = key.size(1)\n",
        "        \n",
        "        # Step 1: Linear projections for all heads at once\n",
        "        # Shape: [batch_size, seq_len, d_model]\n",
        "        Q = self.W_query(query)\n",
        "        K = self.W_key(key)\n",
        "        V = self.W_value(value)\n",
        "        \n",
        "        # Step 2: Reshape to separate heads\n",
        "        # From [batch_size, seq_len, d_model] to [batch_size, seq_len, num_heads, d_k]\n",
        "        Q = Q.view(batch_size, seq_len_query, self.num_heads, self.d_k)\n",
        "        K = K.view(batch_size, seq_len_key, self.num_heads, self.d_k)\n",
        "        V = V.view(batch_size, seq_len_key, self.num_heads, self.d_k)\n",
        "        \n",
        "        # Step 3: Transpose to [batch_size, num_heads, seq_len, d_k] for efficient computation\n",
        "        Q = Q.transpose(1, 2)  # [batch_size, num_heads, seq_len_query, d_k]\n",
        "        K = K.transpose(1, 2)  # [batch_size, num_heads, seq_len_key, d_k]\n",
        "        V = V.transpose(1, 2)  # [batch_size, num_heads, seq_len_key, d_k]\n",
        "        \n",
        "        # Step 4: Apply scaled dot-product attention to each head\n",
        "        attention_output, attention_weights = scaled_dot_product_attention(\n",
        "            Q, K, V, mask\n",
        "        )\n",
        "        # attention_output: [batch_size, num_heads, seq_len_query, d_k]\n",
        "        \n",
        "        # Step 5: Concatenate heads\n",
        "        # Transpose back: [batch_size, seq_len_query, num_heads, d_k]\n",
        "        attention_output = attention_output.transpose(1, 2)\n",
        "        \n",
        "        # Reshape to concatenate heads: [batch_size, seq_len_query, d_model]\n",
        "        attention_output = attention_output.contiguous().view(\n",
        "            batch_size, seq_len_query, self.d_model\n",
        "        )\n",
        "        \n",
        "        # Step 6: Final linear projection\n",
        "        output = self.W_output(attention_output)\n",
        "        \n",
        "        return output, attention_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "14b189d8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([2, 10, 512])\n",
            "Output shape: torch.Size([2, 10, 512])\n",
            "Attention weights shape: torch.Size([2, 8, 10, 10])\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "def example_usage():\n",
        "    batch_size, seq_len, d_model = 2, 10, 512\n",
        "    num_heads = 8\n",
        "    \n",
        "    # Create sample input\n",
        "    x = torch.randn(batch_size, seq_len, d_model)\n",
        "    \n",
        "    # Initialize multi-head attention\n",
        "    mha = MultiHeadAttention(d_model, num_heads)\n",
        "    \n",
        "    # Self-attention (query, key, value are all the same)\n",
        "    output, weights = mha(x, x, x)\n",
        "    \n",
        "    print(f\"Input shape: {x.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(f\"Attention weights shape: {weights.shape}\")\n",
        "    \n",
        "    # For encoder-decoder attention, you'd use different inputs:\n",
        "    # output, weights = mha(decoder_hidden, encoder_output, encoder_output)\n",
        "\n",
        "example_usage()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ca4eea2",
      "metadata": {},
      "source": [
        "Multi-head attention runs multiple attention mechanisms in parallel, each focusing on different aspects of the relationships, then concatenates and projects the results.\n",
        "\n",
        "Applications of Attention in our Model\n",
        "\n",
        "The Transformer uses multi-head attention in three different ways:\n",
        "1) In \"encoder-decoder attention\" layers, the queries come from the\n",
        "previous decoder layer, and the memory keys and values come from the\n",
        "output of the encoder.  This allows every position in the decoder to\n",
        "attend over all positions in the input sequence.  This mimics the\n",
        "typical encoder-decoder attention mechanisms in sequence-to-sequence\n",
        "models such as [(cite)](https://arxiv.org/abs/1609.08144).\n",
        "\n",
        "\n",
        "2) The encoder contains self-attention layers.  In a self-attention\n",
        "layer all of the keys, values and queries come from the same place,\n",
        "in this case, the output of the previous layer in the encoder.  Each\n",
        "position in the encoder can attend to all positions in the previous\n",
        "layer of the encoder.\n",
        "\n",
        "\n",
        "3) Similarly, self-attention layers in the decoder allow each\n",
        "position in the decoder to attend to all positions in the decoder up\n",
        "to and including that position.  We need to prevent leftward\n",
        "information flow in the decoder to preserve the auto-regressive\n",
        "property.  We implement this inside of scaled dot-product attention\n",
        "by masking out (setting to $-\\infty$) all values in the input of the\n",
        "softmax which correspond to illegal connections."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582f83c7",
      "metadata": {},
      "source": [
        "### Position-wise Feed-Forward Networks\n",
        "\n",
        "The outputs of the self-attention layer are fed to a feed-forward neural network in the encoder.\n",
        "\n",
        "In addition to attention sub-layers, each of the layers in our\n",
        "encoder and decoder contains a fully connected feed-forward network,\n",
        "which is applied to each position separately and identically.  This\n",
        "consists of two linear transformations with a ReLU activation in\n",
        "between.\n",
        "\n",
        "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$\n",
        "\n",
        "While the linear transformations are the same across different\n",
        "positions, they use different parameters from layer to\n",
        "layer. Another way of describing this is as two convolutions with\n",
        "kernel size 1.  The dimensionality of input and output is\n",
        "$d_{\\text{model}}=512$, and the inner-layer has dimensionality\n",
        "$d_{ff}=2048$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "5627c0d0",
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    \"Implements FFN equation.\"\n",
        "\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w_2(self.dropout(self.w_1(x).relu()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c238f36",
      "metadata": {},
      "source": [
        "### Layer normalization\n",
        "\n",
        "*What is the purpose of layer normalization in transformers?*\n",
        "\n",
        "We employ a residual connection [(cite)](https://arxiv.org/abs/1512.03385) around each of the two\n",
        "sub-layers, followed by layer normalization [(cite)](https://arxiv.org/abs/1607.06450).\n",
        "\n",
        "\n",
        "When we look at the encoder and decoder blocks, we see several normalization layers called Add & Norm.\n",
        "\n",
        "The LayerNormalization class below performs layer normalization on the input data. During its forward pass, we compute the mean and standard deviation of the input data. We then normalize the input data by subtracting the mean and dividing by the standard deviation plus a small number called epsilon to avoid any divisions by zero. This process results in a normalized output with a mean 0 and a standard deviation 1.\n",
        "\n",
        "We will then scale the normalized output by a learnable parameter alpha and add a learnable parameter called bias. The training process is responsible for adjusting these parameters. The final result is a layer-normalized tensor, which ensures that the scale of the inputs to layers in the network is consistent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "eb4d6162",
      "metadata": {},
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"Construct a layernorm module (See citation for details).\"\n",
        "\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.a_2 = nn.Parameter(torch.ones(features))\n",
        "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(-1, keepdim=True)\n",
        "        std = x.std(-1, keepdim=True)\n",
        "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb9cd2bb",
      "metadata": {},
      "source": [
        "### Residual connection\n",
        "\n",
        "\n",
        "\n",
        "When we look at the architecture of the Transformer, we see that each sub-layer, including the self-attention and Feed Forward blocks, adds its output to its input before passing it to the Add & Norm layer. This approach integrates the output with the original input in the Add & Norm layer. This process is known as the skip connection, which allows the Transformer to train deep networks more effectively by providing a shortcut for the gradient to flow through during backpropagation.\n",
        "\n",
        "The ResidualConnection class below is responsible for this process.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "a1521c15",
      "metadata": {},
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual connection followed by a layer norm.\n",
        "    Note for code simplicity the norm is first as opposed to last.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, size, dropout):\n",
        "        super(ResidualConnection, self).__init__()\n",
        "        self.norm = LayerNorm(size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"Apply residual connection to any sublayer with the same size.\"\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17e4aa4d",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ef04378",
      "metadata": {
        "id": "0ef04378"
      },
      "source": [
        "## Transformer Architecture\n",
        "Now, we have all the core components to build the Transformer.\n",
        "\n",
        "- The original Transformer is an encoder-decoder \n",
        "- decoder-only\n",
        "- encoder-only"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9c1edc4",
      "metadata": {
        "id": "c9c1edc4"
      },
      "source": [
        "### Encoder\n",
        "\n",
        "The encoders are all identical in structure but don't share any weights.\n",
        "\n",
        "Each encoder has two sub-layers:\n",
        "1. Self-attention layer\n",
        "2. Feed Forward Neural Network\n",
        "\n",
        "...we employ residual connections around each of the sub-layers, followed by layer normalization.\n",
        "\n",
        "![](images/encoder-architecture.png)\n",
        "\n",
        "\n",
        "That is, the output of each sub-layer is $\\mathrm{LayerNorm}(x +\n",
        "\\mathrm{Sublayer}(x))$, where $\\mathrm{Sublayer}(x)$ is the function\n",
        "implemented by the sub-layer itself.  We apply dropout\n",
        "[(cite)](http://jmlr.org/papers/v15/srivastava14a.html) to the\n",
        "output of each sub-layer, before it is added to the sub-layer input\n",
        "and normalized.\n",
        "\n",
        "To facilitate these residual connections, all sub-layers in the\n",
        "model, as well as the embedding layers, produce outputs of dimension\n",
        "$d_{\\text{model}}=512$.\n",
        "\n",
        "Each layer has two sub-layers. The first is a multi-head\n",
        "self-attention mechanism, and the second is a simple, position-wise\n",
        "fully connected feed-forward network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3db97336",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-02T01:25:18.779893Z",
          "iopub.status.busy": "2022-05-02T01:25:18.778804Z",
          "iopub.status.idle": "2022-05-02T01:25:18.780994Z",
          "shell.execute_reply": "2022-05-02T01:25:18.781710Z"
        },
        "id": "3db97336"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5db812a90cdb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTransformerEncoderLayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_ff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# Sub-layer 1: Multi-head self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Sub-layer 1: Multi-head self-attention\n",
        "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        # Sub-layer 2: Feed-forward network\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        \n",
        "        # Layer normalization for each sub-layer\n",
        "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # Sub-layer 1: Multi-head self-attention with residual connection\n",
        "        # For self-attention: query, key, and value are all the same input\n",
        "        attention_output, attention_weights = self.self_attention(\n",
        "            query=x,    # Same input\n",
        "            key=x,      # Same input  \n",
        "            value=x,    # Same input\n",
        "            mask=mask\n",
        "        )\n",
        "        \n",
        "        # Post-norm: residual connection then normalize\n",
        "        x = self.layer_norm_1(x + self.dropout(attention_output))\n",
        "        \n",
        "        # Sub-layer 2: Feed-forward with residual connection  \n",
        "        feed_forward_output = self.feed_forward(x)\n",
        "\n",
        "        # Post-norm: residual connection then normalize\n",
        "        x = self.layer_norm_2(x + self.dropout(feed_forward_output))\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83075daf",
      "metadata": {},
      "source": [
        "TODO: add something here\n",
        "like that the encoder is a stack of multiple encoder layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d643018",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-02T01:25:18.744483Z",
          "iopub.status.busy": "2022-05-02T01:25:18.743617Z",
          "iopub.status.idle": "2022-05-02T01:25:18.745891Z",
          "shell.execute_reply": "2022-05-02T01:25:18.746578Z"
        },
        "id": "1d643018"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, source_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Input processing\n",
        "        self.embedding = Embeddings(source_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
        "        \n",
        "        # Stack of encoder layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, num_heads, d_ff, dropout) \n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, source_tokens, source_mask=None):\n",
        "        # Step 1: Convert tokens to embeddings + add positional encoding\n",
        "        embeddings = self.embedding(source_tokens)  # [batch, seq_len, d_model]\n",
        "        encoder_input = self.positional_encoding(embeddings)\n",
        "        \n",
        "        # Step 2: Pass through each encoder layer\n",
        "        for encoder_layer in self.encoder_layers:\n",
        "            encoder_input = encoder_layer(encoder_input, source_mask)\n",
        "        \n",
        "        return encoder_input  # [batch, seq_len, d_model]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c90d6ee",
      "metadata": {
        "id": "9c90d6ee"
      },
      "source": [
        "### Decoder\n",
        "\n",
        "The decode has the following layers:\n",
        "- Self-attention layer (masked multi head ) \n",
        "    - Sub-layer 1: Self-attention (same sequence)\n",
        "    - query=key=value=target_input\n",
        "- Cross-attention (multi head Ecoder-Decode attention layer): \n",
        "    - helps the decode focus on relevant parts of the input sentenc, similar to `seq2seq` models.\n",
        "    - In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.  \n",
        "    - Sub-layer 2: Cross-attention (different sequences)  \n",
        "    - query=target_input:          What we're generating\n",
        "    - key=value=encoder_output :    What we can look at\n",
        "- Feed forward layer\n",
        "\n",
        "\n",
        "Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b1df6b1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-02T01:25:18.803099Z",
          "iopub.status.busy": "2022-05-02T01:25:18.796132Z",
          "iopub.status.idle": "2022-05-02T01:25:18.806479Z",
          "shell.execute_reply": "2022-05-02T01:25:18.805667Z"
        },
        "id": "3b1df6b1"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Sub-layer 1: Masked multi-head self-attention\n",
        "        self.masked_self_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        # Sub-layer 2: Multi-head encoder-decoder attention\n",
        "        self.encoder_decoder_attention = MultiHeadAttention(d_model, num_heads)\n",
        "        \n",
        "        # Sub-layer 3: Feed-forward network\n",
        "        self.feed_forward = FeedForward(d_model, d_ff)\n",
        "        \n",
        "        # Layer normalization for each sub-layer\n",
        "        self.layer_norm_1 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm_2 = nn.LayerNorm(d_model)\n",
        "        self.layer_norm_3 = nn.LayerNorm(d_model)\n",
        "        \n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, target_input, encoder_output, source_mask=None, target_mask=None):\n",
        "        # Sub-layer 1: Masked self-attention on target sequence\n",
        "        # For self-attention: query, key, and value are all the same input (target)\n",
        "        masked_attention_output, masked_attention_weights = self.masked_self_attention(\n",
        "            query=target_input,     # Same target input\n",
        "            key=target_input,       # Same target input\n",
        "            value=target_input,     # Same target input\n",
        "            mask=target_mask        # Causal mask to prevent seeing future tokens\n",
        "        )\n",
        "        # Post-norm: residual connection then normalize\n",
        "        target_input = self.layer_norm_1(target_input + self.dropout(masked_attention_output))\n",
        "        \n",
        "        # Sub-layer 2: Encoder-decoder attention\n",
        "        # Query comes from decoder, key and value come from encoder\n",
        "        encoder_attention_output, encoder_attention_weights = self.encoder_decoder_attention(\n",
        "            query=target_input,     # What the decoder is generating\n",
        "            key=encoder_output,     # What information is available from encoder\n",
        "            value=encoder_output,   # What information to retrieve from encoder\n",
        "            mask=source_mask        # Mask for padding tokens in source\n",
        "        )\n",
        "        # Post-norm: residual connection then normalize\n",
        "        target_input = self.layer_norm_2(target_input + self.dropout(encoder_attention_output))\n",
        "        \n",
        "        # Sub-layer 3: Feed-forward network\n",
        "        feed_forward_output = self.feed_forward(target_input)\n",
        "        # Post-norm: residual connection then normalize\n",
        "        target_input = self.layer_norm_3(target_input + self.dropout(feed_forward_output))\n",
        "        \n",
        "        return target_input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "736addc1",
      "metadata": {},
      "source": [
        "TODO: add something here\n",
        "like that the encoder is a stack of multiple encoder layersljkslajd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49f8ddca",
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerDecoder(nn.Module):\n",
        "    def __init__(self, target_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_len, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Input processing\n",
        "        self.embedding = Embeddings(target_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_len, dropout)\n",
        "        \n",
        "        # Stack of decoder layers\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(d_model, num_heads, d_ff, dropout)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "    def forward(self, target_tokens, encoder_output, source_mask=None, target_mask=None):\n",
        "        # Step 1: Convert tokens to embeddings + add positional encoding\n",
        "        embeddings = self.embedding(target_tokens)  # [batch, seq_len, d_model]\n",
        "        decoder_input = self.positional_encoding(embeddings)\n",
        "        \n",
        "        # Step 2: Pass through each decoder layer\n",
        "        for decoder_layer in self.decoder_layers:\n",
        "            decoder_input = decoder_layer(\n",
        "                target_input=decoder_input,\n",
        "                encoder_output=encoder_output,\n",
        "                source_mask=source_mask,\n",
        "                target_mask=target_mask\n",
        "            )\n",
        "        \n",
        "        return decoder_input  # [batch, seq_len, d_model]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6852ba5",
      "metadata": {
        "id": "a6852ba5"
      },
      "source": [
        "\n",
        "We also modify the self-attention sub-layer in the decoder stack to\n",
        "prevent positions from attending to subsequent positions.  This\n",
        "masking, combined with fact that the output embeddings are offset by\n",
        "one position, ensures that the predictions for position $i$ can\n",
        "depend only on the known outputs at positions less than $i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "1fe1b467",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-05-02T01:25:18.813309Z",
          "iopub.status.busy": "2022-05-02T01:25:18.812387Z",
          "iopub.status.idle": "2022-05-02T01:25:18.815171Z",
          "shell.execute_reply": "2022-05-02T01:25:18.814519Z"
        },
        "id": "1fe1b467"
      },
      "outputs": [],
      "source": [
        "# \"Mask out subsequent positions.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "8cb1c9f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_causal_mask(seq_len):\n",
        "    \"\"\"Create causal mask to prevent attending to future positions\"\"\"\n",
        "    mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1)\n",
        "    return mask == 0  # Convert to boolean mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad536665",
      "metadata": {
        "id": "ad536665"
      },
      "source": [
        "\n",
        "> Below the attention mask shows the position each tgt word (row) is\n",
        "> allowed to look at (column). Words are blocked for attending to\n",
        "> future words during training.\n",
        "\n",
        "\n",
        "TODO: add visualization of mask here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4c162ba",
      "metadata": {
        "id": "e4c162ba",
        "lines_to_next_cell": 2
      },
      "source": [
        "![](images/ModalNet-20.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74aef3a8",
      "metadata": {
        "id": "74aef3a8"
      },
      "source": [
        "### Transformer Model\n",
        "\n",
        "Both the encoder and decode blocks are repeated N times. In the original paper, they defined N=6, and we will define a similar value in this notebook.\n",
        "\n",
        "![](images/transformer_encoders_decoder_stacks.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "e92c9c23",
      "metadata": {},
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, \n",
        "                 source_vocab_size,     # Source vocabulary size\n",
        "                 target_vocab_size,     # Target vocabulary size  \n",
        "                 d_model=512,           # Model dimension\n",
        "                 num_heads=8,           # Number of attention heads\n",
        "                 num_layers=6,          # Number of encoder/decoder layers\n",
        "                 d_ff=2048,             # Feed-forward dimension\n",
        "                 max_seq_len=5000,      # Maximum sequence length\n",
        "                 dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        \n",
        "        # Encoder stack\n",
        "        self.encoder = TransformerEncoder(\n",
        "            source_vocab_size=source_vocab_size,\n",
        "            d_model=d_model, \n",
        "            num_heads=num_heads, \n",
        "            num_layers=num_layers,\n",
        "            d_ff=d_ff, \n",
        "            max_seq_len=max_seq_len, \n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # Decoder stack  \n",
        "        self.decoder = TransformerDecoder(\n",
        "            target_vocab_size=target_vocab_size,\n",
        "            d_model=d_model, \n",
        "            num_heads=num_heads, \n",
        "            num_layers=num_layers,\n",
        "            d_ff=d_ff, \n",
        "            max_seq_len=max_seq_len, \n",
        "            dropout=dropout\n",
        "        )\n",
        "        \n",
        "        # Final output projection to target vocabulary\n",
        "        self.output_projection = nn.Linear(d_model, target_vocab_size)\n",
        "        \n",
        "    def forward(self, source_tokens, target_tokens, source_mask=None, target_mask=None):\n",
        "        \"\"\"\n",
        "        Forward pass for training (teacher forcing)\n",
        "        \n",
        "        source_tokens: [batch_size, source_seq_len] - source token ids\n",
        "        target_tokens: [batch_size, target_seq_len] - target token ids  \n",
        "        \"\"\"\n",
        "        # Step 1: Encode source sequence\n",
        "        encoder_output = self.encoder(source_tokens, source_mask)\n",
        "        # Shape: [batch_size, source_seq_len, d_model]\n",
        "        \n",
        "        # Step 2: Decode target sequence\n",
        "        decoder_output = self.decoder(\n",
        "            target_tokens=target_tokens,\n",
        "            encoder_output=encoder_output,\n",
        "            source_mask=source_mask,\n",
        "            target_mask=target_mask\n",
        "        )\n",
        "        # Shape: [batch_size, target_seq_len, d_model]\n",
        "        \n",
        "        # Step 3: Project to vocabulary logits\n",
        "        output_logits = self.output_projection(decoder_output)\n",
        "        # Shape: [batch_size, target_seq_len, target_vocab_size]\n",
        "        \n",
        "        return output_logits\n",
        "\n",
        "    def encode(self, source_tokens, source_mask=None):\n",
        "        \"\"\"Encode source sequence (for inference)\"\"\"\n",
        "        return self.encoder(source_tokens, source_mask)\n",
        "    \n",
        "    def decode_step(self, target_tokens, encoder_output, source_mask=None, target_mask=None):\n",
        "        \"\"\"Decode one step (for autoregressive generation)\"\"\"\n",
        "        decoder_output = self.decoder(target_tokens, encoder_output, source_mask, target_mask)\n",
        "        return self.output_projection(decoder_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "096f00ba",
      "metadata": {},
      "source": [
        "Interview Questions:\n",
        "- Walk through the forward pass of a transformer block*\n",
        "- What are the differences between encoder-only, decoder-only, and encoder-decoder transformers?*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1168efff",
      "metadata": {},
      "source": [
        "## Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48614b45",
      "metadata": {},
      "outputs": [],
      "source": [
        "#| echo: true\n",
        "#| output: false\n",
        "!pip3.7 install -q torchdata torchtext spacy altair\n",
        "#!python3.7 -m spacy download de_core_news_sm\n",
        "#!python3.7 -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "6007099b",
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "import torchtext.datasets as datasets\n",
        "import spacy\n",
        "\n",
        "from torch.nn.functional import log_softmax, pad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "7e400b58",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5, Loss: 0.2940\n",
            "Epoch 10, Loss: 0.0584\n",
            "Epoch 15, Loss: 0.0239\n",
            "Epoch 20, Loss: 0.0200\n",
            "EN: hello world -> FR: monde le monde\n",
            "EN: how are you -> FR: va ça va\n",
            "EN: good morning -> FR: bientôt bientôt bientôt\n",
            "EN: thank you -> FR: bientôt bientôt bientôt\n",
            "EN: see you soon -> FR: bientôt bientôt bientôt\n"
          ]
        }
      ],
      "source": [
        "# Example: Training and evaluating a Transformer on a simple English-to-French translation task\n",
        "\n",
        "# 1. Prepare toy data (English to French pairs)\n",
        "english_sentences = [\n",
        "    \"hello world\", \"how are you\", \"good morning\", \"thank you\", \"see you soon\"\n",
        "]\n",
        "french_sentences = [\n",
        "    \"bonjour le monde\", \"comment ça va\", \"bonjour\", \"merci\", \"à bientôt\"\n",
        "]\n",
        "\n",
        "# 2. Build vocabularies\n",
        "def build_vocab(sentences):\n",
        "    tokens = set()\n",
        "    for s in sentences:\n",
        "        tokens.update(s.split())\n",
        "    vocab = {word: idx+2 for idx, word in enumerate(sorted(tokens))}\n",
        "    vocab[\"<pad>\"] = 0\n",
        "    vocab[\"<unk>\"] = 1\n",
        "    return vocab\n",
        "\n",
        "src_vocab = build_vocab(english_sentences)\n",
        "tgt_vocab = build_vocab(french_sentences)\n",
        "inv_tgt_vocab = {v: k for k, v in tgt_vocab.items()}\n",
        "\n",
        "# 3. Tokenize and numericalize\n",
        "def encode(sentence, vocab, max_len):\n",
        "    tokens = [vocab.get(w, vocab[\"<unk>\"]) for w in sentence.split()]\n",
        "    tokens = tokens + [vocab[\"<pad>\"]] * (max_len - len(tokens))\n",
        "    return tokens[:max_len]\n",
        "\n",
        "max_src_len = max(len(s.split()) for s in english_sentences)\n",
        "max_tgt_len = max(len(s.split()) for s in french_sentences)\n",
        "\n",
        "src_data = torch.tensor([encode(s, src_vocab, max_src_len) for s in english_sentences])\n",
        "tgt_data = torch.tensor([encode(s, tgt_vocab, max_tgt_len) for s in french_sentences])\n",
        "\n",
        "# 4. Initialize transformer\n",
        "transformer = Transformer(\n",
        "    source_vocab_size=len(src_vocab),\n",
        "    target_vocab_size=len(tgt_vocab),\n",
        "    d_model=128,\n",
        "    num_heads=4,\n",
        "    num_layers=2,\n",
        "    d_ff=256,\n",
        "    max_seq_len=20,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=1e-3)\n",
        "loss_function = nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "# 5. Training loop (few epochs for demonstration)\n",
        "for epoch in range(20):\n",
        "    transformer.train()\n",
        "    optimizer.zero_grad()\n",
        "    # Shift target for teacher forcing\n",
        "    tgt_input = tgt_data[:, :-1]\n",
        "    tgt_output = tgt_data[:, 1:]\n",
        "    logits = transformer(src_data, tgt_input)\n",
        "    loss = loss_function(\n",
        "        logits.view(-1, logits.size(-1)),\n",
        "        tgt_output.contiguous().view(-1)\n",
        "    )\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch+1) % 5 == 0:\n",
        "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 6. Evaluation: Translate English sentences\n",
        "transformer.eval()\n",
        "with torch.no_grad():\n",
        "    for idx, src in enumerate(english_sentences):\n",
        "        src_tensor = torch.tensor([encode(src, src_vocab, max_src_len)])\n",
        "        tgt_tokens = [tgt_vocab[\"<pad>\"]]\n",
        "        for _ in range(max_tgt_len):\n",
        "            tgt_tensor = torch.tensor([tgt_tokens])\n",
        "            logits = transformer(src_tensor, tgt_tensor)\n",
        "            next_token = logits[0, -1].argmax().item()\n",
        "            tgt_tokens.append(next_token)\n",
        "            if next_token == tgt_vocab[\"<pad>\"]:\n",
        "                break\n",
        "        translation = [inv_tgt_vocab.get(tok, \"\") for tok in tgt_tokens[1:] if tok != tgt_vocab[\"<pad>\"]]\n",
        "        print(f\"EN: {src} -> FR: {' '.join(translation)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b7db72",
      "metadata": {},
      "source": [
        "## References\n",
        "- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)\n",
        "- [The Annotated Transformer](https://nlp.seas.harvard.edu/annotated-transformer/)\n",
        "- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90d43c31",
      "metadata": {},
      "source": [
        "TODO: ### Softmax\n",
        "We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities.  \n",
        "In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
